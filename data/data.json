{
 "K8sVersionServiceOptions": {
  "v1.10": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.11": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.12": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.13": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.14": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.14.10-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.14.9-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.10-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.11-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.11-rancher1-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.12-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.12-rancher2-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.12-rancher2-3": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.12-rancher2-5": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.6-rancher1-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.7-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.15.9-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16": {
   "etcd": null,
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.10-rancher2-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.13-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.13-rancher1-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.14-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.15-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.3-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.4-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.6-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.6-rancher1-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.7-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.8-rancher1-1": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.16.8-rancher1-2": {
   "etcd": {
    "client-cert-auth": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.17": {
   "etcd": {
    "client-cert-auth": "true",
    "enable-v2": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.18": {
   "etcd": {
    "client-cert-auth": "true",
    "enable-v2": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.19": {
   "etcd": {
    "client-cert-auth": "true",
    "enable-v2": "true",
    "peer-client-cert-auth": "true"
   },
   "kubeapi": {
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,Priority,TaintNodesByCondition,PersistentVolumeClaimResize",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  },
  "v1.9": {
   "etcd": null,
   "kubeapi": {
    "admission-control": "ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds,NodeRestriction",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "bind-address": "0.0.0.0",
    "enable-admission-plugins": "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction",
    "insecure-port": "0",
    "kubelet-preferred-address-types": "InternalIP,ExternalIP,Hostname",
    "profiling": "false",
    "requestheader-extra-headers-prefix": "X-Remote-Extra-",
    "requestheader-group-headers": "X-Remote-Group",
    "requestheader-username-headers": "X-Remote-User",
    "runtime-config": "authorization.k8s.io/v1beta1=true",
    "secure-port": "6443",
    "service-account-lookup": "true",
    "storage-backend": "etcd3",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305"
   },
   "kubelet": {
    "address": "0.0.0.0",
    "allow-privileged": "true",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cgroups-per-qos": "True",
    "cni-bin-dir": "/opt/cni/bin",
    "cni-conf-dir": "/etc/cni/net.d",
    "enforce-node-allocatable": "",
    "event-qps": "0",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "/etc/resolv.conf",
    "streaming-connection-idle-timeout": "30m",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "healthz-bind-address": "127.0.0.1",
    "v": "2"
   },
   "kubeController": {
    "address": "0.0.0.0",
    "allocate-node-cidrs": "true",
    "allow-untagged-cloud": "true",
    "configure-cloud-routes": "false",
    "enable-hostpath-provisioner": "false",
    "leader-elect": "true",
    "node-monitor-grace-period": "40s",
    "pod-eviction-timeout": "5m0s",
    "profiling": "false",
    "terminated-pod-gc-threshold": "1000",
    "v": "2"
   },
   "scheduler": {
    "address": "0.0.0.0",
    "leader-elect": "true",
    "profiling": "false",
    "v": "2"
   }
  }
 },
 "K8sVersionRKESystemImages": {
  "v1.10.0-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.4",
   "nginxProxy": "rancher/rke-tools:v0.1.4",
   "certDownloader": "rancher/rke-tools:v0.1.4",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.4",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.10.1-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.8",
   "nginxProxy": "rancher/rke-tools:v0.1.8",
   "certDownloader": "rancher/rke-tools:v0.1.8",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.8",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.1-rancher2",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.10.11-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.10.12-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.12-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.10.3-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.10",
   "nginxProxy": "rancher/rke-tools:v0.1.10",
   "certDownloader": "rancher/rke-tools:v0.1.10",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.10",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.3-rancher2",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.10.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.10",
   "nginxProxy": "rancher/rke-tools:v0.1.10",
   "certDownloader": "rancher/rke-tools:v0.1.10",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.10",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.10.5-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.8",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.8",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.8",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.10.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.2-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.2-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.6-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.15",
   "nginxProxy": "rancher/rke-tools:v0.1.15",
   "certDownloader": "rancher/rke-tools:v0.1.15",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.15",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.8-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.15",
   "nginxProxy": "rancher/rke-tools:v0.1.15",
   "certDownloader": "rancher/rke-tools:v0.1.15",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.15",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.8-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.15",
   "nginxProxy": "rancher/rke-tools:v0.1.15",
   "certDownloader": "rancher/rke-tools:v0.1.15",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.15",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.9-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.28",
   "nginxProxy": "rancher/rke-tools:v0.1.28",
   "certDownloader": "rancher/rke-tools:v0.1.28",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.28",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.11.9-rancher1-3": {
   "etcd": "rancher/coreos-etcd:v3.2.18",
   "alpine": "rancher/rke-tools:v0.1.16-2",
   "nginxProxy": "rancher/rke-tools:v0.1.16-2",
   "certDownloader": "rancher/rke-tools:v0.1.16-2",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16-2",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.11.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  },
  "v1.12.0-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.10-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.40",
   "nginxProxy": "rancher/rke-tools:v0.1.40",
   "certDownloader": "rancher/rke-tools:v0.1.40",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.40",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.12.10-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.42",
   "nginxProxy": "rancher/rke-tools:v0.1.42",
   "certDownloader": "rancher/rke-tools:v0.1.42",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.42",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.12.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.18",
   "nginxProxy": "rancher/rke-tools:v0.1.18",
   "certDownloader": "rancher/rke-tools:v0.1.18",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.18",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.4-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.5-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.6-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.6-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.12.7-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.7-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "coredns/coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.12.7-rancher1-3": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.28",
   "nginxProxy": "rancher/rke-tools:v0.1.28",
   "certDownloader": "rancher/rke-tools:v0.1.28",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.28",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.12.7-rancher1-4": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16-2",
   "nginxProxy": "rancher/rke-tools:v0.1.16-2",
   "certDownloader": "rancher/rke-tools:v0.1.16-2",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16-2",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.12.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.34",
   "nginxProxy": "rancher/rke-tools:v0.1.34",
   "certDownloader": "rancher/rke-tools:v0.1.34",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.34",
   "kubedns": "rancher/k8s-dns-kube-dns:1.14.13",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.14.13",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.14.13",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.12.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.1.3",
   "calicoCni": "rancher/calico-cni:v3.1.3",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.3",
   "canalCni": "rancher/calico-cni:v3.1.3",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.1-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "coredns": "coredns/coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.10-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16-2",
   "nginxProxy": "rancher/rke-tools:v0.1.16-2",
   "certDownloader": "rancher/rke-tools:v0.1.16-2",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16-2",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.10-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.42",
   "nginxProxy": "rancher/rke-tools:v0.1.42",
   "certDownloader": "rancher/rke-tools:v0.1.42",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.42",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.11-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.12-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.12-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.12-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.12-rancher2",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.4-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.4-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "coredns/coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.5-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.27",
   "nginxProxy": "rancher/rke-tools:v0.1.27",
   "certDownloader": "rancher/rke-tools:v0.1.27",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.27",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "coredns/coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.5-rancher1-3": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.28",
   "nginxProxy": "rancher/rke-tools:v0.1.28",
   "certDownloader": "rancher/rke-tools:v0.1.28",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.28",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.7-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.34",
   "nginxProxy": "rancher/rke-tools:v0.1.34",
   "certDownloader": "rancher/rke-tools:v0.1.34",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.34",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.13.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.2.24",
   "alpine": "rancher/rke-tools:v0.1.16",
   "nginxProxy": "rancher/rke-tools:v0.1.16",
   "certDownloader": "rancher/rke-tools:v0.1.16",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.16",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0",
   "flannelCni": "rancher/coreos-flannel-cni:v0.3.0",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause-amd64:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.3.1"
  },
  "v1.13.9-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.2.24-rancher1",
   "alpine": "rancher/rke-tools:v0.1.40",
   "nginxProxy": "rancher/rke-tools:v0.1.40",
   "certDownloader": "rancher/rke-tools:v0.1.40",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.40",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "coredns": "rancher/coredns-coredns:1.2.6",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.13.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.28",
   "nginxProxy": "rancher/rke-tools:v0.1.28",
   "certDownloader": "rancher/rke-tools:v0.1.28",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.28",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "coredns/coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.1-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.28",
   "nginxProxy": "rancher/rke-tools:v0.1.28",
   "certDownloader": "rancher/rke-tools:v0.1.28",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.28",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.10-rancher1-0": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.10-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.34",
   "nginxProxy": "rancher/rke-tools:v0.1.34",
   "certDownloader": "rancher/rke-tools:v0.1.34",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.34",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.40",
   "nginxProxy": "rancher/rke-tools:v0.1.40",
   "certDownloader": "rancher/rke-tools:v0.1.40",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.40",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.6-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.42",
   "nginxProxy": "rancher/rke-tools:v0.1.42",
   "certDownloader": "rancher/rke-tools:v0.1.42",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.42",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.7-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.8-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.8-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.8-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.8-rancher2",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.14.9-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.14.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.10.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.4.0",
   "calicoCni": "rancher/calico-cni:v3.4.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.4.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.4.0",
   "canalCni": "rancher/calico-cni:v3.4.0",
   "canalFlannel": "rancher/coreos-flannel:v0.10.0",
   "weaveNode": "weaveworks/weave-kube:2.5.0",
   "weaveCni": "weaveworks/weave-npc:2.5.0",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.1"
  },
  "v1.15.0-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.34",
   "nginxProxy": "rancher/rke-tools:v0.1.34",
   "certDownloader": "rancher/rke-tools:v0.1.34",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.34",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.10-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.10-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.11-rancher1-0": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.11-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.11-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.11-rancher1-3": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.15.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.12-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.12-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.8.9",
   "calicoCni": "rancher/calico-cni:v3.8.9",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.9",
   "calicoCtl": "rancher/calico-ctl:v3.8.9",
   "canalNode": "rancher/calico-node:v3.8.9",
   "canalCni": "rancher/calico-cni:v3.8.9",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.12-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.15.12-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.12-rancher2-3": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.58",
   "nginxProxy": "rancher/rke-tools:v0.1.58",
   "certDownloader": "rancher/rke-tools:v0.1.58",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.58",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.15.12-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.15.12-rancher2-4": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.59",
   "nginxProxy": "rancher/rke-tools:v0.1.59",
   "certDownloader": "rancher/rke-tools:v0.1.59",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.59",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.15.12-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.15.12-rancher2-5": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.15.12-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.15.2-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.40",
   "nginxProxy": "rancher/rke-tools:v0.1.40",
   "certDownloader": "rancher/rke-tools:v0.1.40",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.40",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.42",
   "nginxProxy": "rancher/rke-tools:v0.1.42",
   "certDownloader": "rancher/rke-tools:v0.1.42",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.42",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher1",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:0.21.0-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.4-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.4-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3"
  },
  "v1.15.5-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.5-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.5-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.6-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.7-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.15.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.10-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "coredns": "rancher/coredns-coredns:1.3.1",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.3.0",
   "kubernetes": "rancher/hyperkube:v1.15.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.7.4",
   "calicoCni": "rancher/calico-cni:v3.7.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.7.4",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.7.4",
   "canalCni": "rancher/calico-cni:v3.7.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.3",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.8.1",
   "calicoCni": "rancher/calico-cni:v3.8.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "canalNode": "rancher/calico-node:v3.8.1",
   "canalCni": "rancher/calico-cni:v3.8.1",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.10-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.10-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.10-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.58",
   "nginxProxy": "rancher/rke-tools:v0.1.58",
   "certDownloader": "rancher/rke-tools:v0.1.58",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.58",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.10-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.13-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.59",
   "nginxProxy": "rancher/rke-tools:v0.1.59",
   "certDownloader": "rancher/rke-tools:v0.1.59",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.59",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.13-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.13-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.13-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.14-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.14-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.15-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.15-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.15-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.15-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.35.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.16.2-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.50",
   "nginxProxy": "rancher/rke-tools:v0.1.50",
   "certDownloader": "rancher/rke-tools:v0.1.50",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.50",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.8.1",
   "calicoCni": "rancher/calico-cni:v3.8.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "canalNode": "rancher/calico-node:v3.8.1",
   "canalCni": "rancher/calico-cni:v3.8.1",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.2-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.2-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.8.1",
   "calicoCni": "rancher/calico-cni:v3.8.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "canalNode": "rancher/calico-node:v3.8.1",
   "canalCni": "rancher/calico-cni:v3.8.1",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.8.1",
   "calicoCni": "rancher/calico-cni:v3.8.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "canalNode": "rancher/calico-node:v3.8.1",
   "canalCni": "rancher/calico-cni:v3.8.1",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.4-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.6-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.6-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.7-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.7-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.8-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.8-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.8-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.16.8-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.16.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.3.15-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.2",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.16.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.4",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.0-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.51",
   "nginxProxy": "rancher/rke-tools:v0.1.51",
   "certDownloader": "rancher/rke-tools:v0.1.51",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.51",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.8.1",
   "calicoCni": "rancher/calico-cni:v3.8.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.8.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "canalNode": "rancher/calico-node:v3.8.1",
   "canalCni": "rancher/calico-cni:v3.8.1",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.8.1",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.0-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.11-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.11-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.17.12-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.12-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.35.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.17.2-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.2-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.3-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.3-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.10.2",
   "calicoCni": "rancher/calico-cni:v3.10.2",
   "calicoControllers": "rancher/calico-kube-controllers:v3.10.2",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "canalNode": "rancher/calico-node:v3.10.2",
   "canalCni": "rancher/calico-cni:v3.10.2",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.10.2",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.4-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.52",
   "nginxProxy": "rancher/rke-tools:v0.1.52",
   "certDownloader": "rancher/rke-tools:v0.1.52",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.52",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.4-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "kubernetes": "rancher/hyperkube:v1.17.4-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher5",
   "calicoNode": "rancher/calico-node:v3.13.0",
   "calicoCni": "rancher/calico-cni:v3.13.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.0",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "canalNode": "rancher/calico-node:v3.13.0",
   "canalCni": "rancher/calico-cni:v3.13.0",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.0",
   "weaveNode": "weaveworks/weave-kube:2.5.2",
   "weaveCni": "weaveworks/weave-npc:2.5.2",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.25.1-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.6-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.6-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.17.6-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.58",
   "nginxProxy": "rancher/rke-tools:v0.1.58",
   "certDownloader": "rancher/rke-tools:v0.1.58",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.58",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.6-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.17.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.59",
   "nginxProxy": "rancher/rke-tools:v0.1.59",
   "certDownloader": "rancher/rke-tools:v0.1.59",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.59",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.17.9-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.0",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.0",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.0",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.5",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.17.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.18.3-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.56",
   "nginxProxy": "rancher/rke-tools:v0.1.56",
   "certDownloader": "rancher/rke-tools:v0.1.56",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.56",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.3-rancher2",
   "flannel": "rancher/coreos-flannel:v0.11.0-rancher1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.11.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.3"
  },
  "v1.18.3-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.58",
   "nginxProxy": "rancher/rke-tools:v0.1.58",
   "certDownloader": "rancher/rke-tools:v0.1.58",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.58",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.3-rancher2",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.18.6-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.59",
   "nginxProxy": "rancher/rke-tools:v0.1.59",
   "certDownloader": "rancher/rke-tools:v0.1.59",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.59",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.18.6-rancher1-2": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.6-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.18.8-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.8-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.18.9-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.3-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.2",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.2",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.2",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "coredns": "rancher/coredns-coredns:1.6.9",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.7.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.7",
   "kubernetes": "rancher/hyperkube:v1.18.9-rancher1",
   "flannel": "rancher/coreos-flannel:v0.12.0",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.13.4",
   "calicoCni": "rancher/calico-cni:v3.13.4",
   "calicoControllers": "rancher/calico-kube-controllers:v3.13.4",
   "calicoCtl": "rancher/calico-ctl:v3.13.4",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "canalNode": "rancher/calico-node:v3.13.4",
   "canalCni": "rancher/calico-cni:v3.13.4",
   "canalFlannel": "rancher/coreos-flannel:v0.12.0",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.13.4",
   "weaveNode": "weaveworks/weave-kube:2.6.4",
   "weaveCni": "weaveworks/weave-npc:2.6.4",
   "podInfraContainer": "rancher/pause:3.1",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.35.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.19.0-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.13-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "coredns": "rancher/coredns-coredns:1.7.0",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.13",
   "kubernetes": "rancher/hyperkube:v1.19.0-rancher1",
   "flannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.16.0",
   "calicoCni": "rancher/calico-cni:v3.16.0",
   "calicoControllers": "rancher/calico-kube-controllers:v3.16.0",
   "calicoCtl": "rancher/calico-ctl:v3.16.0",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.0",
   "canalNode": "rancher/calico-node:v3.16.0",
   "canalCni": "rancher/calico-cni:v3.16.0",
   "canalControllers": "rancher/calico-kube-controllers:v3.16.0",
   "canalFlannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.0",
   "weaveNode": "weaveworks/weave-kube:2.6.5",
   "weaveCni": "weaveworks/weave-npc:2.6.5",
   "podInfraContainer": "rancher/pause:3.2",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.32.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.19.1-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.13-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "coredns": "rancher/coredns-coredns:1.7.0",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.13",
   "kubernetes": "rancher/hyperkube:v1.19.1-rancher1",
   "flannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.16.1",
   "calicoCni": "rancher/calico-cni:v3.16.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.16.1",
   "calicoCtl": "rancher/calico-ctl:v3.16.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.1",
   "canalNode": "rancher/calico-node:v3.16.1",
   "canalCni": "rancher/calico-cni:v3.16.1",
   "canalControllers": "rancher/calico-kube-controllers:v3.16.1",
   "canalFlannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.1",
   "weaveNode": "weaveworks/weave-kube:2.7.0",
   "weaveCni": "weaveworks/weave-npc:2.7.0",
   "podInfraContainer": "rancher/pause:3.2",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.35.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.19.2-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.4.13-rancher1",
   "alpine": "rancher/rke-tools:v0.1.64",
   "nginxProxy": "rancher/rke-tools:v0.1.64",
   "certDownloader": "rancher/rke-tools:v0.1.64",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.64",
   "kubedns": "rancher/k8s-dns-kube-dns:1.15.10",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny:1.15.10",
   "kubednsSidecar": "rancher/k8s-dns-sidecar:1.15.10",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "coredns": "rancher/coredns-coredns:1.7.0",
   "corednsAutoscaler": "rancher/cluster-proportional-autoscaler:1.8.1",
   "nodelocal": "rancher/k8s-dns-node-cache:1.15.13",
   "kubernetes": "rancher/hyperkube:v1.19.2-rancher1",
   "flannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "flannelCni": "rancher/flannel-cni:v0.3.0-rancher6",
   "calicoNode": "rancher/calico-node:v3.16.1",
   "calicoCni": "rancher/calico-cni:v3.16.1",
   "calicoControllers": "rancher/calico-kube-controllers:v3.16.1",
   "calicoCtl": "rancher/calico-ctl:v3.16.1",
   "calicoFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.1",
   "canalNode": "rancher/calico-node:v3.16.1",
   "canalCni": "rancher/calico-cni:v3.16.1",
   "canalControllers": "rancher/calico-kube-controllers:v3.16.1",
   "canalFlannel": "rancher/coreos-flannel:v0.13.0-rancher1-rc1",
   "canalFlexVol": "rancher/calico-pod2daemon-flexvol:v3.16.1",
   "weaveNode": "weaveworks/weave-kube:2.7.0",
   "weaveCni": "weaveworks/weave-npc:2.7.0",
   "podInfraContainer": "rancher/pause:3.2",
   "ingress": "rancher/nginx-ingress-controller:nginx-0.35.0-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.5-rancher1",
   "metricsServer": "rancher/metrics-server:v0.3.6",
   "windowsPodInfraContainer": "rancher/kubelet-pause:v0.1.4"
  },
  "v1.8.11-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.0.17",
   "alpine": "rancher/rke-tools:v0.1.8",
   "nginxProxy": "rancher/rke-tools:v0.1.8",
   "certDownloader": "rancher/rke-tools:v0.1.8",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.8",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.5",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.5",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.5",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.8.11-rancher2",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.0",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.9.5-rancher1-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.4",
   "nginxProxy": "rancher/rke-tools:v0.1.4",
   "certDownloader": "rancher/rke-tools:v0.1.4",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.4",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.7",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.7",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.7",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.9.5-rancher1",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.0",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.9.7-rancher2-1": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.8",
   "nginxProxy": "rancher/rke-tools:v0.1.8",
   "certDownloader": "rancher/rke-tools:v0.1.8",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.8",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.7",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.7",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.7",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.9.7-rancher2",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.0",
   "ingress": "rancher/nginx-ingress-controller:0.10.2-rancher3",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4"
  },
  "v1.9.7-rancher2-2": {
   "etcd": "rancher/coreos-etcd:v3.1.12",
   "alpine": "rancher/rke-tools:v0.1.13",
   "nginxProxy": "rancher/rke-tools:v0.1.13",
   "certDownloader": "rancher/rke-tools:v0.1.13",
   "kubernetesServicesSidecar": "rancher/rke-tools:v0.1.13",
   "kubedns": "rancher/k8s-dns-kube-dns-amd64:1.14.7",
   "dnsmasq": "rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.7",
   "kubednsSidecar": "rancher/k8s-dns-sidecar-amd64:1.14.7",
   "kubednsAutoscaler": "rancher/cluster-proportional-autoscaler-amd64:1.0.0",
   "kubernetes": "rancher/hyperkube:v1.9.7-rancher2",
   "flannel": "rancher/coreos-flannel:v0.9.1",
   "flannelCni": "rancher/coreos-flannel-cni:v0.2.0",
   "calicoNode": "rancher/calico-node:v3.1.1",
   "calicoCni": "rancher/calico-cni:v3.1.1",
   "calicoCtl": "rancher/calico-ctl:v2.0.0",
   "canalNode": "rancher/calico-node:v3.1.1",
   "canalCni": "rancher/calico-cni:v3.1.1",
   "canalFlannel": "rancher/coreos-flannel:v0.9.1",
   "weaveNode": "weaveworks/weave-kube:2.1.2",
   "weaveCni": "weaveworks/weave-npc:2.1.2",
   "podInfraContainer": "rancher/pause-amd64:3.0",
   "ingress": "rancher/nginx-ingress-controller:0.16.2-rancher1",
   "ingressBackend": "rancher/nginx-ingress-controller-defaultbackend:1.4",
   "metricsServer": "rancher/metrics-server-amd64:v0.2.1"
  }
 },
 "K8sVersionedTemplates": {
  "calico": {
   "\u003e=1.13.0-rancher0 \u003c1.15.0-rancher0": "calico-v1.13",
   "\u003e=1.15.0-rancher0 \u003c1.15.11-rancher1-1": "calico-v1.15",
   "\u003e=1.15.11-rancher1-1 \u003c1.15.12-rancher1-1": "calico-v1.15-privileged",
   "\u003e=1.15.12-rancher1-1 \u003c1.15.12-rancher2-2": "calico-v1.15.12",
   "\u003e=1.15.12-rancher2-2 \u003c1.16.0-alpha": "calico-v1.15-privileged",
   "\u003e=1.16.0-alpha \u003c1.16.4-rancher1": "calico-v1.16",
   "\u003e=1.16.4-rancher1 \u003c1.16.8-rancher0": "calico-v1.17",
   "\u003e=1.16.8-rancher0 \u003c1.17.0-rancher0": "calico-v1.17-privileged",
   "\u003e=1.17.0-rancher0 \u003c1.17.4-rancher0": "calico-v1.17",
   "\u003e=1.17.4-rancher0 \u003c1.19.0-rancher0": "calico-v1.17-privileged",
   "\u003e=1.19.0-rancher0": "calico-v3.16.0",
   "\u003e=1.8.0-rancher0 \u003c1.13.0-rancher0": "calico-v1.8"
  },
  "canal": {
   "\u003e=1.13.0-rancher0 \u003c1.15.0-rancher0": "canal-v1.13",
   "\u003e=1.15.0-rancher0 \u003c1.15.11-rancher1-1": "canal-v1.15",
   "\u003e=1.15.11-rancher1-1 \u003c1.15.12-rancher1-1": "canal-v1.15-privileged",
   "\u003e=1.15.12-rancher1-1 \u003c1.15.12-rancher2-2": "canal-v1.15.12",
   "\u003e=1.15.12-rancher2-2 \u003c1.16.0-alpha": "canal-v1.15-privileged-calico3134",
   "\u003e=1.16.0-alpha \u003c1.16.4-rancher1": "canal-v1.16",
   "\u003e=1.16.10-rancher2-1 \u003c1.17.0-rancher0": "canal-v1.17-privileged-calico3134",
   "\u003e=1.16.4-rancher1 \u003c1.16.8-rancher0": "canal-v1.17",
   "\u003e=1.16.8-rancher0 \u003c1.16.10-rancher2-1": "canal-v1.17-privileged",
   "\u003e=1.17.0-rancher0 \u003c1.17.4-rancher0": "canal-v1.17",
   "\u003e=1.17.4-rancher0 \u003c1.17.6-rancher2-1": "canal-v1.17-privileged",
   "\u003e=1.17.6-rancher2-1 \u003c1.19.0-rancher0": "canal-v1.17-privileged-calico3134",
   "\u003e=1.19.0-rancher0": "canal-v3.16.0",
   "\u003e=1.8.0-rancher0 \u003c1.13.0-rancher0": "canal-v1.8"
  },
  "coreDNS": {
   "\u003e=1.16.0-alpha \u003c1.17.0-alpha": "coredns-v1.16",
   "\u003e=1.17.0-alpha": "coredns-v1.17",
   "\u003e=1.8.0-rancher0 \u003c1.16.0-alpha": "coredns-v1.8"
  },
  "flannel": {
   "\u003e=1.15.0-rancher0 \u003c1.16.0-alpha": "flannel-v1.15",
   "\u003e=1.16.0-alpha": "flannel-v1.16",
   "\u003e=1.8.0-rancher0 \u003c1.15.0-rancher0": "flannel-v1.8"
  },
  "kubeDNS": {
   "\u003e=1.16.0-alpha": "kubedns-v1.16",
   "\u003e=1.8.0-rancher0 \u003c1.16.0-alpha": "kubedns-v1.8"
  },
  "metricsServer": {
   "\u003e=1.8.0-rancher0": "metricsserver-v1.8"
  },
  "nginxIngress": {
   "\u003e=1.13.10-rancher1-3 \u003c1.14.0-rancher0": "nginxingress-v1.15",
   "\u003e=1.14.0-rancher0 \u003c=1.14.6-rancher1-1": "nginxingress-v1.8",
   "\u003e=1.14.6-rancher2 \u003c1.15.0-rancher0": "nginxingress-v1.15",
   "\u003e=1.15.0-rancher0 \u003c=1.15.3-rancher1-1": "nginxingress-v1.8",
   "\u003e=1.15.12-rancher1-1 \u003c1.16.1-rancher1-1": "nginxingress-v1.15.12",
   "\u003e=1.15.3-rancher2 \u003c1.15.12-rancher1-1": "nginxingress-v1.15",
   "\u003e=1.16.1-rancher1-1 \u003c1.16.10-rancher1-1": "nginxingress-v1.15",
   "\u003e=1.16.10-rancher1-1 \u003c1.17.0-rancher1-1": "nginxingress-v1.15.12",
   "\u003e=1.17.0-rancher1-1 \u003c1.17.6-rancher1-1": "nginxingress-v1.15",
   "\u003e=1.17.6-rancher1-1": "nginxingress-v1.15.12",
   "\u003e=1.8.0-rancher0 \u003c1.13.10-rancher1-3": "nginxingress-v1.8"
  },
  "nodelocal": {
   "\u003e=1.15.11-rancher0 \u003c1.16.0-alpha": "nodelocal-v1.15",
   "\u003e=1.16.8-rancher0 \u003c1.17.0-alpha": "nodelocal-v1.15",
   "\u003e=1.17.4-rancher0": "nodelocal-v1.15"
  },
  "templateKeys": {
   "calico-v1.13": "\n{{if eq .RBACConfig \"rbac\"}}\n## start rbac here\n\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n## end rbac here\n\n---\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # To enable Typha, set this to \"calico-typha\" *and* set a non-zero value for Typha replicas\n  # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is\n  # essential.\n  typha_service_name: \"none\"\n  # Configure the Calico backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n            \"type\": \"host-local\",\n            \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName: calico-node\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Typha support: controlled by the ConfigMap.\n            - name: FELIX_TYPHAK8SSERVICENAME\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: typha_service_name\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -bird-ready\n              - -felix-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n\n\n{{if ne .CloudProvider \"none\"}}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: {{.CloudProvider}}-ippool\n  namespace: kube-system\ndata:\n  {{.CloudProvider}}-ippool: |-\n    apiVersion: projectcalico.org/v3\n    kind: IPPool\n    metadata:\n      name: ippool-ipip-1\n    spec:\n      cidr: {{.ClusterCIDR}}\n      ipipMode: Always\n      natOutgoing: true\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: calicoctl\n  namespace: kube-system\nspec:\n  hostNetwork: true\n  restartPolicy: OnFailure\n  tolerations:\n  - effect: NoExecute\n    operator: Exists\n  - effect: NoSchedule\n    operator: Exists\n  containers:\n  - name: calicoctl\n    image: {{.Calicoctl}}\n    command: [\"/bin/sh\", \"-c\", \"calicoctl apply -f {{.CloudProvider}}-ippool.yaml\"]\n    env:\n    - name: DATASTORE_TYPE\n      value: kubernetes\n    volumeMounts:\n    - name: ippool-config\n      mountPath: /root/\n  volumes:\n  - name: ippool-config\n    configMap:\n      name: {{.CloudProvider}}-ippool\n      items:\n        - key: {{.CloudProvider}}-ippool\n          path: {{.CloudProvider}}-ippool.yaml\n  # Mount in the etcd TLS secrets.\n{{end}}\n",
   "calico-v1.15": "\n{{if eq .RBACConfig \"rbac\"}}\n---\n# Source: calico/templates/rbac.yaml\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n{{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -bird-ready\n              - -felix-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\nspec:\n  # The controller can only have a single active instance.\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n",
   "calico-v1.15-privileged": "\n# CalicoTemplateV115Privileged\n{{if eq .RBACConfig \"rbac\"}}\n# Source: calico/templates/rbac.yaml\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n{{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n          securityContext:\n            privileged: true\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n              - -bird-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-ready\n              - -bird-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n",
   "calico-v1.15.12": "\n{{if eq .RBACConfig \"rbac\"}}\n---\n# Source: calico/templates/rbac.yaml\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n{{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -bird-ready\n              - -felix-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\n  annotations:\n    scheduler.alpha.kubernetes.io/critical-pod: ''\nspec:\n  # The controller can only have a single active instance.\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n",
   "calico-v1.16": "\n{{if eq .RBACConfig \"rbac\"}}\n# Source: calico/templates/rbac.yaml\n\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n---\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n          {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n          {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -bird-ready\n              - -felix-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n",
   "calico-v1.17": "\n{{if eq .RBACConfig \"rbac\"}}\n# Source: calico/templates/rbac.yaml\n\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n---\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n          {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n          {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -bird-ready\n              - -felix-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n",
   "calico-v1.17-privileged": "\n# CalicoTemplateV117Privileged\n{{if eq .RBACConfig \"rbac\"}}\n# Source: calico/templates/rbac.yaml\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n{{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n          securityContext:\n            privileged: true\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs calico-node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n              - -bird-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-ready\n              - -bird-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n{{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n{{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n",
   "calico-v1.8": "\n{{if eq .RBACConfig \"rbac\"}}\n## start rbac here\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico-node\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - namespaces\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - update\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n      - patch\n  - apiGroups: [\"\"]\n    resources:\n      - services\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - update\n      - watch\n  - apiGroups: [\"extensions\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - watch\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n## end rbac here\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n name: calico-config\n namespace: kube-system\ndata:\n # To enable Typha, set this to \"calico-typha\" *and* set a non-zero value for Typha replicas\n # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is\n # essential.\n typha_service_name: \"none\"\n # The CNI network configuration to install on each node.\n cni_network_config: |-\n    {\n     \"name\": \"k8s-pod-network\",\n     \"cniVersion\": \"0.3.0\",\n     \"plugins\": [\n       {\n         \"type\": \"calico\",\n         \"log_level\": \"WARNING\",\n         \"datastore_type\": \"kubernetes\",\n         \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n         \"mtu\": 1500,\n         \"ipam\": {\n           \"type\": \"host-local\",\n           \"subnet\": \"usePodCidr\"\n         },\n         \"policy\": {\n           \"type\": \"k8s\",\n           \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n         },\n         \"kubernetes\": {\n           \"k8s_api_root\": \"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__\",\n           \"kubeconfig\": \"{{.KubeCfg}}\"\n         }\n       },\n       {\n         \"type\": \"portmap\",\n         \"snat\": true,\n         \"capabilities\": {\"portMappings\": true}\n       }\n     ]\n    }\n\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure calico/node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n        - key: \"node-role.kubernetes.io/controlplane\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n        - key: \"node-role.kubernetes.io/etcd\"\n          operator: \"Exists\"\n          effect: \"NoExecute\"\n      serviceAccountName: calico-node\n      terminationGracePeriodSeconds: 0\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPV6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              value: \"1440\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Enable IP-in-IP within Felix.\n            - name: FELIX_IPINIPENABLED\n              value: \"true\"\n            # Typha support: controlled by the ConfigMap.\n            - name: FELIX_TYPHAK8SSERVICENAME\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: typha_service_name\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n\n\n{{if ne .CloudProvider \"none\"}}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: {{.CloudProvider}}-ippool\n  namespace: kube-system\ndata:\n  {{.CloudProvider}}-ippool: |-\n    apiVersion: projectcalico.org/v3\n    kind: IPPool\n    metadata:\n      name: ippool-ipip-1\n    spec:\n      cidr: {{.ClusterCIDR}}\n      ipipMode: Always\n      natOutgoing: true\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: calicoctl\n  namespace: kube-system\nspec:\n  hostNetwork: true\n  restartPolicy: OnFailure\n  tolerations:\n  - effect: NoExecute\n    operator: Exists\n  - effect: NoSchedule\n    operator: Exists\n  containers:\n  - name: calicoctl\n    image: {{.Calicoctl}}\n    command: [\"/bin/sh\", \"-c\", \"calicoctl apply -f {{.CloudProvider}}-ippool.yaml\"]\n    env:\n    - name: DATASTORE_TYPE\n      value: kubernetes\n    volumeMounts:\n    - name: ippool-config\n      mountPath: /root/\n  volumes:\n  - name: ippool-config\n    configMap:\n      name: {{.CloudProvider}}-ippool\n      items:\n        - key: {{.CloudProvider}}-ippool\n          path: {{.CloudProvider}}-ippool.yaml\n  # Mount in the etcd TLS secrets.\n{{end}}\n",
   "calico-v3.16.0": "\n# Calico Template based on Calico v3.16.0\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # Configure the backend to use.\n  calico_backend: \"bird\"\n  # Configure the MTU to use for workload interfaces and tunnels.\n  # - If Wireguard is enabled, set to your network MTU - 60\n  # - Otherwise, if VXLAN or BPF mode is enabled, set to your network MTU - 50\n  # - Otherwise, if IPIP is enabled, set to your network MTU - 20\n  # - Otherwise, if not using any encapsulation, set to your network MTU.\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1440\"\n{{- end}}\n  # The CNI network configuration to install on each node. The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"log_file_path\": \"/var/log/calico/cni/cni.log\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"calico-ipam\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BGPConfiguration\n    listKind: BGPConfigurationList\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: BGPConfiguration contains the configuration for any BGP routing.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BGPConfigurationSpec contains the values of the BGP configuration.\n            properties:\n              asNumber:\n                description: 'ASNumber is the default AS number used by a node. [Default:\n                  64512]'\n                format: int32\n                type: integer\n              communities:\n                description: Communities is a list of BGP community values and their\n                  arbitrary names for tagging routes.\n                items:\n                  description: Community contains standard or large community value\n                    and its name.\n                  properties:\n                    name:\n                      description: Name given to community value.\n                      type: string\n                    value:\n                      description: Value must be of format `aa:nn` or `aa:nn:mm`.\n                        For standard community use `aa:nn` format, where `aa` and\n                        `nn` are 16 bit number. For large community use `aa:nn:mm`\n                        format, where `aa`, `nn` and `mm` are 32 bit number. Where,\n                        `aa` is an AS Number, `nn` and `mm` are per-AS identifier.\n                      pattern: ^(\\d+):(\\d+)$|^(\\d+):(\\d+):(\\d+)$\n                      type: string\n                  type: object\n                type: array\n              listenPort:\n                description: ListenPort is the port where BGP protocol should listen.\n                  Defaults to 179\n                maximum: 65535\n                minimum: 1\n                type: integer\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: INFO]'\n                type: string\n              nodeToNodeMeshEnabled:\n                description: 'NodeToNodeMeshEnabled sets whether full node to node\n                  BGP mesh is enabled. [Default: true]'\n                type: boolean\n              prefixAdvertisements:\n                description: PrefixAdvertisements contains per-prefix advertisement\n                  configuration.\n                items:\n                  description: PrefixAdvertisement configures advertisement properties\n                    for the specified CIDR.\n                  properties:\n                    cidr:\n                      description: CIDR for which properties should be advertised.\n                      type: string\n                    communities:\n                      description: Communities can be list of either community names\n                        already defined in `Specs.Communities` or community value\n                        of format `aa:nn` or `aa:nn:mm`. For standard community use\n                        `aa:nn` format, where `aa` and `nn` are 16 bit number. For\n                        large community use `aa:nn:mm` format, where `aa`, `nn` and\n                        `mm` are 32 bit number. Where,`aa` is an AS Number, `nn` and\n                        `mm` are per-AS identifier.\n                      items:\n                        type: string\n                      type: array\n                  type: object\n                type: array\n              serviceClusterIPs:\n                description: ServiceClusterIPs are the CIDR blocks from which service\n                  cluster IPs are allocated. If specified, Calico will advertise these\n                  blocks, as well as any cluster IPs within them.\n                items:\n                  description: ServiceClusterIPBlock represents a single allowed ClusterIP\n                    CIDR block.\n                  properties:\n                    cidr:\n                      type: string\n                  type: object\n                type: array\n              serviceExternalIPs:\n                description: ServiceExternalIPs are the CIDR blocks for Kubernetes\n                  Service External IPs. Kubernetes Service ExternalIPs will only be\n                  advertised if they are within one of these blocks.\n                items:\n                  description: ServiceExternalIPBlock represents a single allowed\n                    External IP CIDR block.\n                  properties:\n                    cidr:\n                      type: string\n                  type: object\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: bgppeers.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BGPPeer\n    listKind: BGPPeerList\n    plural: bgppeers\n    singular: bgppeer\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BGPPeerSpec contains the specification for a BGPPeer resource.\n            properties:\n              asNumber:\n                description: The AS Number of the peer.\n                format: int32\n                type: integer\n              keepOriginalNextHop:\n                description: Option to keep the original nexthop field when routes\n                  are sent to a BGP Peer. Setting \"true\" configures the selected BGP\n                  Peers node to use the \"next hop keep;\" instead of \"next hop self;\"(default)\n                  in the specific branch of the Node on \"bird.cfg\".\n                type: boolean\n              node:\n                description: The node name identifying the Calico node instance that\n                  is peering with this peer. If this is not set, this represents a\n                  global peer, i.e. a peer that peers with every node in the deployment.\n                type: string\n              nodeSelector:\n                description: Selector for the nodes that should have this peering.  When\n                  this is set, the Node field must be empty.\n                type: string\n              peerIP:\n                description: The IP address of the peer followed by an optional port\n                  number to peer with. If port number is given, format should be `[\u003cIPv6\u003e]:port`\n                  or `\u003cIPv4\u003e:\u003cport\u003e` for IPv4. If optional port number is not set,\n                  and this peer IP and ASNumber belongs to a calico/node with ListenPort\n                  set in BGPConfiguration, then we use that port to peer.\n                type: string\n              peerSelector:\n                description: Selector for the remote nodes to peer with.  When this\n                  is set, the PeerIP and ASNumber fields must be empty.  For each\n                  peering between the local node and selected remote nodes, we configure\n                  an IPv4 peering if both ends have NodeBGPSpec.IPv4Address specified,\n                  and an IPv6 peering if both ends have NodeBGPSpec.IPv6Address specified.  The\n                  remote AS number comes from the remote nodes NodeBGPSpec.ASNumber,\n                  or the global default if that is not set.\n                type: string\n            required:\n            - asNumber\n            - peerIP\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BlockAffinity\n    listKind: BlockAffinityList\n    plural: blockaffinities\n    singular: blockaffinity\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BlockAffinitySpec contains the specification for a BlockAffinity\n              resource.\n            properties:\n              cidr:\n                type: string\n              deleted:\n                description: Deleted indicates that this block affinity is being deleted.\n                  This field is a string for compatibility with older releases that\n                  mistakenly treat this field as a string.\n                type: string\n              node:\n                type: string\n              state:\n                type: string\n            required:\n            - cidr\n            - deleted\n            - node\n            - state\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: ClusterInformation\n    listKind: ClusterInformationList\n    plural: clusterinformations\n    singular: clusterinformation\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: ClusterInformation contains the cluster specific information.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: ClusterInformationSpec contains the values of describing\n              the cluster.\n            properties:\n              calicoVersion:\n                description: CalicoVersion is the version of Calico that the cluster\n                  is running\n                type: string\n              clusterGUID:\n                description: ClusterGUID is the GUID of the cluster\n                type: string\n              clusterType:\n                description: ClusterType describes the type of the cluster\n                type: string\n              datastoreReady:\n                description: DatastoreReady is used during significant datastore migrations\n                  to signal to components such as Felix that it should wait before\n                  accessing the datastore.\n                type: boolean\n              variant:\n                description: Variant declares which variant of Calico should be active.\n                type: string\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: FelixConfiguration\n    listKind: FelixConfigurationList\n    plural: felixconfigurations\n    singular: felixconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: Felix Configuration contains the configuration for Felix.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: FelixConfigurationSpec contains the values of the Felix configuration.\n            properties:\n              awsSrcDstCheck:\n                description: 'Set source-destination-check on AWS EC2 instances. Accepted\n                  value must be one of \"DoNothing\", \"Enabled\" or \"Disabled\". [Default:\n                  DoNothing]'\n                enum:\n                - DoNothing\n                - Enable\n                - Disable\n                type: string\n              bpfConnectTimeLoadBalancingEnabled:\n                description: 'BPFConnectTimeLoadBalancingEnabled when in BPF mode,\n                  controls whether Felix installs the connection-time load balancer.  The\n                  connect-time load balancer is required for the host to be able to\n                  reach Kubernetes services and it improves the performance of pod-to-service\n                  connections.  The only reason to disable it is for debugging purposes.  [Default:\n                  true]'\n                type: boolean\n              bpfDataIfacePattern:\n                description: 'BPFDataIfacePattern is a regular expression that controls\n                  which interfaces Felix should attach BPF programs to in order to\n                  catch traffic to/from the network.  This needs to match the interfaces\n                  that Calico workload traffic flows over as well as any interfaces\n                  that handle incoming traffic to nodeports and services from outside\n                  the cluster.  It should not match the workload interfaces (usually\n                  named cali...). [Default: ^(en.*|eth.*|tunl0$)]'\n                type: string\n              bpfDisableUnprivileged:\n                description: 'BPFDisableUnprivileged, if enabled, Felix sets the kernel.unprivileged_bpf_disabled\n                  sysctl to disable unprivileged use of BPF.  This ensures that unprivileged\n                  users cannot access Calico''s BPF maps and cannot insert their own\n                  BPF programs to interfere with Calico''s. [Default: true]'\n                type: boolean\n              bpfEnabled:\n                description: 'BPFEnabled, if enabled Felix will use the BPF dataplane.\n                  [Default: false]'\n                type: boolean\n              bpfExternalServiceMode:\n                description: 'BPFExternalServiceMode in BPF mode, controls how connections\n                  from outside the cluster to services (node ports and cluster IPs)\n                  are forwarded to remote workloads.  If set to \"Tunnel\" then both\n                  request and response traffic is tunneled to the remote node.  If\n                  set to \"DSR\", the request traffic is tunneled but the response traffic\n                  is sent directly from the remote node.  In \"DSR\" mode, the remote\n                  node appears to use the IP of the ingress node; this requires a\n                  permissive L2 network.  [Default: Tunnel]'\n                type: string\n              bpfKubeProxyEndpointSlicesEnabled:\n                description: BPFKubeProxyEndpointSlicesEnabled in BPF mode, controls\n                  whether Felix's embedded kube-proxy accepts EndpointSlices or not.\n                type: boolean\n              bpfKubeProxyIptablesCleanupEnabled:\n                description: 'BPFKubeProxyIptablesCleanupEnabled, if enabled in BPF\n                  mode, Felix will proactively clean up the upstream Kubernetes kube-proxy''s\n                  iptables chains.  Should only be enabled if kube-proxy is not running.  [Default:\n                  true]'\n                type: boolean\n              bpfKubeProxyMinSyncPeriod:\n                description: 'BPFKubeProxyMinSyncPeriod, in BPF mode, controls the\n                  minimum time between updates to the dataplane for Felix''s embedded\n                  kube-proxy.  Lower values give reduced set-up latency.  Higher values\n                  reduce Felix CPU usage by batching up more work.  [Default: 1s]'\n                type: string\n              bpfLogLevel:\n                description: 'BPFLogLevel controls the log level of the BPF programs\n                  when in BPF dataplane mode.  One of \"Off\", \"Info\", or \"Debug\".  The\n                  logs are emitted to the BPF trace pipe, accessible with the command\n                  `tc exec bpf debug`. [Default: Off].'\n                type: string\n              chainInsertMode:\n                description: 'ChainInsertMode controls whether Felix hooks the kernels\n                  top-level iptables chains by inserting a rule at the top of the\n                  chain or by appending a rule at the bottom. insert is the safe default\n                  since it prevents Calicos rules from being bypassed. If you switch\n                  to append mode, be sure that the other rules in the chains signal\n                  acceptance by falling through to the Calico rules, otherwise the\n                  Calico policy will be bypassed. [Default: insert]'\n                type: string\n              dataplaneDriver:\n                type: string\n              debugDisableLogDropping:\n                type: boolean\n              debugMemoryProfilePath:\n                type: string\n              debugSimulateCalcGraphHangAfter:\n                type: string\n              debugSimulateDataplaneHangAfter:\n                type: string\n              defaultEndpointToHostAction:\n                description: 'DefaultEndpointToHostAction controls what happens to\n                  traffic that goes from a workload endpoint to the host itself (after\n                  the traffic hits the endpoint egress policy). By default Calico\n                  blocks traffic from workload endpoints to the host itself with an\n                  iptables DROP action. If you want to allow some or all traffic\n                  from endpoint to host, set this parameter to RETURN or ACCEPT. Use\n                  RETURN if you have your own rules in the iptables INPUT chain;\n                  Calico will insert its rules at the top of that chain, then RETURN\n                  packets to the INPUT chain once it has completed processing workload\n                  endpoint egress policy. Use ACCEPT to unconditionally accept packets\n                  from workloads after processing workload endpoint egress policy.\n                  [Default: Drop]'\n                type: string\n              deviceRouteProtocol:\n                description: This defines the route protocol added to programmed device\n                  routes, by default this will be RTPROT_BOOT when left blank.\n                type: integer\n              deviceRouteSourceAddress:\n                description: This is the source address to use on programmed device\n                  routes. By default the source address is left blank, leaving the\n                  kernel to choose the source address used.\n                type: string\n              disableConntrackInvalidCheck:\n                type: boolean\n              endpointReportingDelay:\n                type: string\n              endpointReportingEnabled:\n                type: boolean\n              externalNodesList:\n                description: ExternalNodesCIDRList is a list of CIDR's of external-non-calico-nodes\n                  which may source tunnel traffic and have the tunneled traffic be\n                  accepted at calico nodes.\n                items:\n                  type: string\n                type: array\n              failsafeInboundHostPorts:\n                description: 'FailsafeInboundHostPorts is a comma-delimited list of\n                  UDP/TCP ports that Felix will allow incoming traffic to host endpoints\n                  on irrespective of the security policy. This is useful to avoid\n                  accidentally cutting off a host with incorrect configuration. Each\n                  port should be specified as tcp:\u003cport-number\u003e or udp:\u003cport-number\u003e.\n                  For back-compatibility, if the protocol is not specified, it defaults\n                  to tcp. To disable all inbound host ports, use the value none.\n                  The default value allows ssh access and DHCP. [Default: tcp:22,\n                  udp:68, tcp:179, tcp:2379, tcp:2380, tcp:6443, tcp:6666, tcp:6667]'\n                items:\n                  description: ProtoPort is combination of protocol and port, both\n                    must be specified.\n                  properties:\n                    port:\n                      type: integer\n                    protocol:\n                      type: string\n                  required:\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              failsafeOutboundHostPorts:\n                description: 'FailsafeOutboundHostPorts is a comma-delimited list\n                  of UDP/TCP ports that Felix will allow outgoing traffic from host\n                  endpoints to irrespective of the security policy. This is useful\n                  to avoid accidentally cutting off a host with incorrect configuration.\n                  Each port should be specified as tcp:\u003cport-number\u003e or udp:\u003cport-number\u003e.\n                  For back-compatibility, if the protocol is not specified, it defaults\n                  to tcp. To disable all outbound host ports, use the value none.\n                  The default value opens etcds standard ports to ensure that Felix\n                  does not get cut off from etcd as well as allowing DHCP and DNS.\n                  [Default: tcp:179, tcp:2379, tcp:2380, tcp:6443, tcp:6666, tcp:6667,\n                  udp:53, udp:67]'\n                items:\n                  description: ProtoPort is combination of protocol and port, both\n                    must be specified.\n                  properties:\n                    port:\n                      type: integer\n                    protocol:\n                      type: string\n                  required:\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              featureDetectOverride:\n                description: FeatureDetectOverride is used to override the feature\n                  detection. Values are specified in a comma separated list with no\n                  spaces, example; \"SNATFullyRandom=true,MASQFullyRandom=false,RestoreSupportsLock=\".\n                  \"true\" or \"false\" will force the feature, empty or omitted values\n                  are auto-detected.\n                type: string\n              genericXDPEnabled:\n                description: 'GenericXDPEnabled enables Generic XDP so network cards\n                  that don''t support XDP offload or driver modes can use XDP. This\n                  is not recommended since it doesn''t provide better performance\n                  than iptables. [Default: false]'\n                type: boolean\n              healthEnabled:\n                type: boolean\n              healthHost:\n                type: string\n              healthPort:\n                type: integer\n              interfaceExclude:\n                description: 'InterfaceExclude is a comma-separated list of interfaces\n                  that Felix should exclude when monitoring for host endpoints. The\n                  default value ensures that Felix ignores Kubernetes'' IPVS dummy\n                  interface, which is used internally by kube-proxy. If you want to\n                  exclude multiple interface names using a single value, the list\n                  supports regular expressions. For regular expressions you must wrap\n                  the value with ''/''. For example having values ''/^kube/,veth1''\n                  will exclude all interfaces that begin with ''kube'' and also the\n                  interface ''veth1''. [Default: kube-ipvs0]'\n                type: string\n              interfacePrefix:\n                description: 'InterfacePrefix is the interface name prefix that identifies\n                  workload endpoints and so distinguishes them from host endpoint\n                  interfaces. Note: in environments other than bare metal, the orchestrators\n                  configure this appropriately. For example our Kubernetes and Docker\n                  integrations set the cali value, and our OpenStack integration\n                  sets the tap value. [Default: cali]'\n                type: string\n              interfaceRefreshInterval:\n                description: InterfaceRefreshInterval is the period at which Felix\n                  rescans local interfaces to verify their state. The rescan can be\n                  disabled by setting the interval to 0.\n                type: string\n              ipipEnabled:\n                type: boolean\n              ipipMTU:\n                description: 'IPIPMTU is the MTU to set on the tunnel device. See\n                  Configuring MTU [Default: 1440]'\n                type: integer\n              ipsetsRefreshInterval:\n                description: 'IpsetsRefreshInterval is the period at which Felix re-checks\n                  all iptables state to ensure that no other process has accidentally\n                  broken Calicos rules. Set to 0 to disable iptables refresh. [Default:\n                  90s]'\n                type: string\n              iptablesBackend:\n                description: IptablesBackend specifies which backend of iptables will\n                  be used. The default is legacy.\n                type: string\n              iptablesFilterAllowAction:\n                type: string\n              iptablesLockFilePath:\n                description: 'IptablesLockFilePath is the location of the iptables\n                  lock file. You may need to change this if the lock file is not in\n                  its standard location (for example if you have mapped it into Felixs\n                  container at a different path). [Default: /run/xtables.lock]'\n                type: string\n              iptablesLockProbeInterval:\n                description: 'IptablesLockProbeInterval is the time that Felix will\n                  wait between attempts to acquire the iptables lock if it is not\n                  available. Lower values make Felix more responsive when the lock\n                  is contended, but use more CPU. [Default: 50ms]'\n                type: string\n              iptablesLockTimeout:\n                description: 'IptablesLockTimeout is the time that Felix will wait\n                  for the iptables lock, or 0, to disable. To use this feature, Felix\n                  must share the iptables lock file with all other processes that\n                  also take the lock. When running Felix inside a container, this\n                  requires the /run directory of the host to be mounted into the calico/node\n                  or calico/felix container. [Default: 0s disabled]'\n                type: string\n              iptablesMangleAllowAction:\n                type: string\n              iptablesMarkMask:\n                description: 'IptablesMarkMask is the mask that Felix selects its\n                  IPTables Mark bits from. Should be a 32 bit hexadecimal number with\n                  at least 8 bits set, none of which clash with any other mark bits\n                  in use on the system. [Default: 0xff000000]'\n                format: int32\n                type: integer\n              iptablesNATOutgoingInterfaceFilter:\n                type: string\n              iptablesPostWriteCheckInterval:\n                description: 'IptablesPostWriteCheckInterval is the period after Felix\n                  has done a write to the dataplane that it schedules an extra read\n                  back in order to check the write was not clobbered by another process.\n                  This should only occur if another application on the system doesnt\n                  respect the iptables lock. [Default: 1s]'\n                type: string\n              iptablesRefreshInterval:\n                description: 'IptablesRefreshInterval is the period at which Felix\n                  re-checks the IP sets in the dataplane to ensure that no other process\n                  has accidentally broken Calicos rules. Set to 0 to disable IP sets\n                  refresh. Note: the default for this value is lower than the other\n                  refresh intervals as a workaround for a Linux kernel bug that was\n                  fixed in kernel version 4.11. If you are using v4.11 or greater\n                  you may want to set this to, a higher value to reduce Felix CPU\n                  usage. [Default: 10s]'\n                type: string\n              ipv6Support:\n                type: boolean\n              kubeNodePortRanges:\n                description: 'KubeNodePortRanges holds list of port ranges used for\n                  service node ports. Only used if felix detects kube-proxy running\n                  in ipvs mode. Felix uses these ranges to separate host and workload\n                  traffic. [Default: 30000:32767].'\n                items:\n                  anyOf:\n                  - type: integer\n                  - type: string\n                  pattern: ^.*\n                  x-kubernetes-int-or-string: true\n                type: array\n              logFilePath:\n                description: 'LogFilePath is the full path to the Felix log. Set to\n                  none to disable file logging. [Default: /var/log/calico/felix.log]'\n                type: string\n              logPrefix:\n                description: 'LogPrefix is the log prefix that Felix uses when rendering\n                  LOG rules. [Default: calico-packet]'\n                type: string\n              logSeverityFile:\n                description: 'LogSeverityFile is the log severity above which logs\n                  are sent to the log file. [Default: Info]'\n                type: string\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: Info]'\n                type: string\n              logSeveritySys:\n                description: 'LogSeveritySys is the log severity above which logs\n                  are sent to the syslog. Set to None for no logging to syslog. [Default:\n                  Info]'\n                type: string\n              maxIpsetSize:\n                type: integer\n              metadataAddr:\n                description: 'MetadataAddr is the IP address or domain name of the\n                  server that can answer VM queries for cloud-init metadata. In OpenStack,\n                  this corresponds to the machine running nova-api (or in Ubuntu,\n                  nova-api-metadata). A value of none (case insensitive) means that\n                  Felix should not set up any NAT rule for the metadata path. [Default:\n                  127.0.0.1]'\n                type: string\n              metadataPort:\n                description: 'MetadataPort is the port of the metadata server. This,\n                  combined with global.MetadataAddr (if not None), is used to set\n                  up a NAT rule, from 169.254.169.254:80 to MetadataAddr:MetadataPort.\n                  In most cases this should not need to be changed [Default: 8775].'\n                type: integer\n              natOutgoingAddress:\n                description: NATOutgoingAddress specifies an address to use when performing\n                  source NAT for traffic in a natOutgoing pool that is leaving the\n                  network. By default the address used is an address on the interface\n                  the traffic is leaving on (ie it uses the iptables MASQUERADE target)\n                type: string\n              natPortRange:\n                anyOf:\n                - type: integer\n                - type: string\n                description: NATPortRange specifies the range of ports that is used\n                  for port mapping when doing outgoing NAT. When unset the default\n                  behavior of the network stack is used.\n                pattern: ^.*\n                x-kubernetes-int-or-string: true\n              netlinkTimeout:\n                type: string\n              openstackRegion:\n                description: 'OpenstackRegion is the name of the region that a particular\n                  Felix belongs to. In a multi-region Calico/OpenStack deployment,\n                  this must be configured somehow for each Felix (here in the datamodel,\n                  or in felix.cfg or the environment on each compute node), and must\n                  match the [calico] openstack_region value configured in neutron.conf\n                  on each node. [Default: Empty]'\n                type: string\n              policySyncPathPrefix:\n                description: 'PolicySyncPathPrefix is used to by Felix to communicate\n                  policy changes to external services, like Application layer policy.\n                  [Default: Empty]'\n                type: string\n              prometheusGoMetricsEnabled:\n                description: 'PrometheusGoMetricsEnabled disables Go runtime metrics\n                  collection, which the Prometheus client does by default, when set\n                  to false. This reduces the number of metrics reported, reducing\n                  Prometheus load. [Default: true]'\n                type: boolean\n              prometheusMetricsEnabled:\n                description: 'PrometheusMetricsEnabled enables the Prometheus metrics\n                  server in Felix if set to true. [Default: false]'\n                type: boolean\n              prometheusMetricsHost:\n                description: 'PrometheusMetricsHost is the host that the Prometheus\n                  metrics server should bind to. [Default: empty]'\n                type: string\n              prometheusMetricsPort:\n                description: 'PrometheusMetricsPort is the TCP port that the Prometheus\n                  metrics server should bind to. [Default: 9091]'\n                type: integer\n              prometheusProcessMetricsEnabled:\n                description: 'PrometheusProcessMetricsEnabled disables process metrics\n                  collection, which the Prometheus client does by default, when set\n                  to false. This reduces the number of metrics reported, reducing\n                  Prometheus load. [Default: true]'\n                type: boolean\n              removeExternalRoutes:\n                description: Whether or not to remove device routes that have not\n                  been programmed by Felix. Disabling this will allow external applications\n                  to also add device routes. This is enabled by default which means\n                  we will remove externally added routes.\n                type: boolean\n              reportingInterval:\n                description: 'ReportingInterval is the interval at which Felix reports\n                  its status into the datastore or 0 to disable. Must be non-zero\n                  in OpenStack deployments. [Default: 30s]'\n                type: string\n              reportingTTL:\n                description: 'ReportingTTL is the time-to-live setting for process-wide\n                  status reports. [Default: 90s]'\n                type: string\n              routeRefreshInterval:\n                description: 'RouterefreshInterval is the period at which Felix re-checks\n                  the routes in the dataplane to ensure that no other process has\n                  accidentally broken Calicos rules. Set to 0 to disable route refresh.\n                  [Default: 90s]'\n                type: string\n              routeSource:\n                description: 'RouteSource configures where Felix gets its routing\n                  information. - WorkloadIPs: use workload endpoints to construct\n                  routes. - CalicoIPAM: the default - use IPAM data to construct routes.'\n                type: string\n              routeTableRange:\n                description: Calico programs additional Linux route tables for various\n                  purposes.  RouteTableRange specifies the indices of the route tables\n                  that Calico should use.\n                properties:\n                  max:\n                    type: integer\n                  min:\n                    type: integer\n                required:\n                - max\n                - min\n                type: object\n              sidecarAccelerationEnabled:\n                description: 'SidecarAccelerationEnabled enables experimental sidecar\n                  acceleration [Default: false]'\n                type: boolean\n              usageReportingEnabled:\n                description: 'UsageReportingEnabled reports anonymous Calico version\n                  number and cluster size to projectcalico.org. Logs warnings returned\n                  by the usage server. For example, if a significant security vulnerability\n                  has been discovered in the version of Calico being used. [Default:\n                  true]'\n                type: boolean\n              usageReportingInitialDelay:\n                description: 'UsageReportingInitialDelay controls the minimum delay\n                  before Felix makes a report. [Default: 300s]'\n                type: string\n              usageReportingInterval:\n                description: 'UsageReportingInterval controls the interval at which\n                  Felix makes reports. [Default: 86400s]'\n                type: string\n              useInternalDataplaneDriver:\n                type: boolean\n              vxlanEnabled:\n                type: boolean\n              vxlanMTU:\n                description: 'VXLANMTU is the MTU to set on the tunnel device. See\n                  Configuring MTU [Default: 1440]'\n                type: integer\n              vxlanPort:\n                type: integer\n              vxlanVNI:\n                type: integer\n              wireguardEnabled:\n                description: 'WireguardEnabled controls whether Wireguard is enabled.\n                  [Default: false]'\n                type: boolean\n              wireguardInterfaceName:\n                description: 'WireguardInterfaceName specifies the name to use for\n                  the Wireguard interface. [Default: wg.calico]'\n                type: string\n              wireguardListeningPort:\n                description: 'WireguardListeningPort controls the listening port used\n                  by Wireguard. [Default: 51820]'\n                type: integer\n              wireguardMTU:\n                description: 'WireguardMTU controls the MTU on the Wireguard interface.\n                  See Configuring MTU [Default: 1420]'\n                type: integer\n              wireguardRoutingRulePriority:\n                description: 'WireguardRoutingRulePriority controls the priority value\n                  to use for the Wireguard routing rule. [Default: 99]'\n                type: integer\n              xdpEnabled:\n                description: 'XDPEnabled enables XDP acceleration for suitable untracked\n                  incoming deny rules. [Default: true]'\n                type: boolean\n              xdpRefreshInterval:\n                description: 'XDPRefreshInterval is the period at which Felix re-checks\n                  all XDP state to ensure that no other process has accidentally broken\n                  Calico''s BPF maps or attached programs. Set to 0 to disable XDP\n                  refresh. [Default: 90s]'\n                type: string\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: GlobalNetworkPolicy\n    listKind: GlobalNetworkPolicyList\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            properties:\n              applyOnForward:\n                description: ApplyOnForward indicates to apply the rules in this policy\n                  on forward traffic.\n                type: boolean\n              doNotTrack:\n                description: DoNotTrack indicates whether packets matched by the rules\n                  in this policy should go through the data plane's connection tracking,\n                  such as Linux conntrack.  If True, the rules in this policy are\n                  applied before any data plane connection tracking, and packets allowed\n                  by this policy are marked as not to be tracked.\n                type: boolean\n              egress:\n                description: The ordered set of egress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              ingress:\n                description: The ordered set of ingress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              namespaceSelector:\n                description: NamespaceSelector is an optional field for an expression\n                  used to select a pod based on namespaces.\n                type: string\n              order:\n                description: Order is an optional field that specifies the order in\n                  which the policy is applied. Policies with higher \"order\" are applied\n                  after those with lower order.  If the order is omitted, it may be\n                  considered to be \"infinite\" - i.e. the policy will be applied last.  Policies\n                  with identical order will be applied in alphanumerical order based\n                  on the Policy \"Name\".\n                type: number\n              preDNAT:\n                description: PreDNAT indicates to apply the rules in this policy before\n                  any DNAT.\n                type: boolean\n              selector:\n                description: \"The selector is an expression used to pick pick out\n                  the endpoints that the policy should be applied to. \\n Selector\n                  expressions follow this syntax: \\n \\tlabel == \\\"string_literal\\\"\n                  \\ -\u003e  comparison, e.g. my_label == \\\"foo bar\\\" \\tlabel != \\\"string_literal\\\"\n                  \\  -\u003e  not equal; also matches if label is not present \\tlabel in\n                  { \\\"a\\\", \\\"b\\\", \\\"c\\\", ... }  -\u003e  true if the value of label X is\n                  one of \\\"a\\\", \\\"b\\\", \\\"c\\\" \\tlabel not in { \\\"a\\\", \\\"b\\\", \\\"c\\\",\n                  ... }  -\u003e  true if the value of label X is not one of \\\"a\\\", \\\"b\\\",\n                  \\\"c\\\" \\thas(label_name)  -\u003e True if that label is present \\t! expr\n                  -\u003e negation of expr \\texpr \u0026\u0026 expr  -\u003e Short-circuit and \\texpr\n                  || expr  -\u003e Short-circuit or \\t( expr ) -\u003e parens for grouping \\tall()\n                  or the empty selector -\u003e matches all endpoints. \\n Label names are\n                  allowed to contain alphanumerics, -, _ and /. String literals are\n                  more permissive but they do not support escape characters. \\n Examples\n                  (with made-up labels): \\n \\ttype == \\\"webserver\\\" \u0026\u0026 deployment\n                  == \\\"prod\\\" \\ttype in {\\\"frontend\\\", \\\"backend\\\"} \\tdeployment !=\n                  \\\"dev\\\" \\t! has(label_name)\"\n                type: string\n              serviceAccountSelector:\n                description: ServiceAccountSelector is an optional field for an expression\n                  used to select a pod based on service accounts.\n                type: string\n              types:\n                description: \"Types indicates whether this policy applies to ingress,\n                  or to egress, or to both.  When not explicitly specified (and so\n                  the value on creation is empty or nil), Calico defaults Types according\n                  to what Ingress and Egress rules are present in the policy.  The\n                  default is: \\n - [ PolicyTypeIngress ], if there are no Egress rules\n                  (including the case where there are   also no Ingress rules) \\n\n                  - [ PolicyTypeEgress ], if there are Egress rules but no Ingress\n                  rules \\n - [ PolicyTypeIngress, PolicyTypeEgress ], if there are\n                  both Ingress and Egress rules. \\n When the policy is read back again,\n                  Types will always be one of these values, never empty or nil.\"\n                items:\n                  description: PolicyType enumerates the possible values of the PolicySpec\n                    Types field.\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: GlobalNetworkSet\n    listKind: GlobalNetworkSetList\n    plural: globalnetworksets\n    singular: globalnetworkset\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: GlobalNetworkSet contains a set of arbitrary IP sub-networks/CIDRs\n          that share labels to allow rules to refer to them via selectors.  The labels\n          of GlobalNetworkSet are not namespaced.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: GlobalNetworkSetSpec contains the specification for a NetworkSet\n              resource.\n            properties:\n              nets:\n                description: The list of IP networks that belong to this set.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: HostEndpoint\n    listKind: HostEndpointList\n    plural: hostendpoints\n    singular: hostendpoint\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: HostEndpointSpec contains the specification for a HostEndpoint\n              resource.\n            properties:\n              expectedIPs:\n                description: \"The expected IP addresses (IPv4 and IPv6) of the endpoint.\n                  If \\\"InterfaceName\\\" is not present, Calico will look for an interface\n                  matching any of the IPs in the list and apply policy to that. Note:\n                  \\tWhen using the selector match criteria in an ingress or egress\n                  security Policy \\tor Profile, Calico converts the selector into\n                  a set of IP addresses. For host \\tendpoints, the ExpectedIPs field\n                  is used for that purpose. (If only the interface \\tname is specified,\n                  Calico does not learn the IPs of the interface for use in match\n                  \\tcriteria.)\"\n                items:\n                  type: string\n                type: array\n              interfaceName:\n                description: \"Either \\\"*\\\", or the name of a specific Linux interface\n                  to apply policy to; or empty.  \\\"*\\\" indicates that this HostEndpoint\n                  governs all traffic to, from or through the default network namespace\n                  of the host named by the \\\"Node\\\" field; entering and leaving that\n                  namespace via any interface, including those from/to non-host-networked\n                  local workloads. \\n If InterfaceName is not \\\"*\\\", this HostEndpoint\n                  only governs traffic that enters or leaves the host through the\n                  specific interface named by InterfaceName, or - when InterfaceName\n                  is empty - through the specific interface that has one of the IPs\n                  in ExpectedIPs. Therefore, when InterfaceName is empty, at least\n                  one expected IP must be specified.  Only external interfaces (such\n                  as eth0) are supported here; it isn't possible for a HostEndpoint\n                  to protect traffic through a specific local workload interface.\n                  \\n Note: Only some kinds of policy are implemented for \\\"*\\\" HostEndpoints;\n                  initially just pre-DNAT policy.  Please check Calico documentation\n                  for the latest position.\"\n                type: string\n              node:\n                description: The node name identifying the Calico node instance.\n                type: string\n              ports:\n                description: Ports contains the endpoint's named ports, which may\n                  be referenced in security policy rules.\n                items:\n                  properties:\n                    name:\n                      type: string\n                    port:\n                      type: integer\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                  required:\n                  - name\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              profiles:\n                description: A list of identifiers of security Profile objects that\n                  apply to this endpoint. Each profile is applied in the order that\n                  they appear in this list.  Profile rules are applied after the selector-based\n                  security policy.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMBlock\n    listKind: IPAMBlockList\n    plural: ipamblocks\n    singular: ipamblock\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMBlockSpec contains the specification for an IPAMBlock\n              resource.\n            properties:\n              affinity:\n                type: string\n              allocations:\n                items:\n                  type: integer\n                  # TODO: This nullable is manually added in. We should update controller-gen\n                  # to handle []*int properly itself.\n                  nullable: true\n                type: array\n              attributes:\n                items:\n                  properties:\n                    handle_id:\n                      type: string\n                    secondary:\n                      additionalProperties:\n                        type: string\n                      type: object\n                  type: object\n                type: array\n              cidr:\n                type: string\n              deleted:\n                type: boolean\n              strictAffinity:\n                type: boolean\n              unallocated:\n                items:\n                  type: integer\n                type: array\n            required:\n            - allocations\n            - attributes\n            - cidr\n            - deleted\n            - strictAffinity\n            - unallocated\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMConfig\n    listKind: IPAMConfigList\n    plural: ipamconfigs\n    singular: ipamconfig\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMConfigSpec contains the specification for an IPAMConfig\n              resource.\n            properties:\n              autoAllocateBlocks:\n                type: boolean\n              strictAffinity:\n                type: boolean\n            required:\n            - autoAllocateBlocks\n            - strictAffinity\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMHandle\n    listKind: IPAMHandleList\n    plural: ipamhandles\n    singular: ipamhandle\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMHandleSpec contains the specification for an IPAMHandle\n              resource.\n            properties:\n              block:\n                additionalProperties:\n                  type: integer\n                type: object\n              handleID:\n                type: string\n            required:\n            - block\n            - handleID\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ippools.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPPool\n    listKind: IPPoolList\n    plural: ippools\n    singular: ippool\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPPoolSpec contains the specification for an IPPool resource.\n            properties:\n              blockSize:\n                description: The block size to use for IP address assignments from\n                  this pool. Defaults to 26 for IPv4 and 112 for IPv6.\n                type: integer\n              cidr:\n                description: The pool CIDR.\n                type: string\n              disabled:\n                description: When disabled is true, Calico IPAM will not assign addresses\n                  from this pool.\n                type: boolean\n              ipip:\n                description: 'Deprecated: this field is only used for APIv1 backwards\n                  compatibility. Setting this field is not allowed, this field is\n                  for internal use only.'\n                properties:\n                  enabled:\n                    description: When enabled is true, ipip tunneling will be used\n                      to deliver packets to destinations within this pool.\n                    type: boolean\n                  mode:\n                    description: The IPIP mode.  This can be one of \"always\" or \"cross-subnet\".  A\n                      mode of \"always\" will also use IPIP tunneling for routing to\n                      destination IP addresses within this pool.  A mode of \"cross-subnet\"\n                      will only use IPIP tunneling when the destination node is on\n                      a different subnet to the originating node.  The default value\n                      (if not specified) is \"always\".\n                    type: string\n                type: object\n              ipipMode:\n                description: Contains configuration for IPIP tunneling for this pool.\n                  If not specified, then this is defaulted to \"Never\" (i.e. IPIP tunneling\n                  is disabled).\n                type: string\n              nat-outgoing:\n                description: 'Deprecated: this field is only used for APIv1 backwards\n                  compatibility. Setting this field is not allowed, this field is\n                  for internal use only.'\n                type: boolean\n              natOutgoing:\n                description: When nat-outgoing is true, packets sent from Calico networked\n                  containers in this pool to destinations outside of this pool will\n                  be masqueraded.\n                type: boolean\n              nodeSelector:\n                description: Allows IPPool to allocate for a specific node by label\n                  selector.\n                type: string\n              vxlanMode:\n                description: Contains configuration for VXLAN tunneling for this pool.\n                  If not specified, then this is defaulted to \"Never\" (i.e. VXLAN\n                  tunneling is disabled).\n                type: string\n            required:\n            - cidr\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: kubecontrollersconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: KubeControllersConfiguration\n    listKind: KubeControllersConfigurationList\n    plural: kubecontrollersconfigurations\n    singular: kubecontrollersconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: KubeControllersConfigurationSpec contains the values of the\n              Kubernetes controllers configuration.\n            properties:\n              controllers:\n                description: Controllers enables and configures individual Kubernetes\n                  controllers\n                properties:\n                  namespace:\n                    description: Namespace enables and configures the namespace controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  node:\n                    description: Node enables and configures the node controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      hostEndpoint:\n                        description: HostEndpoint controls syncing nodes to host endpoints.\n                          Disabled by default, set to nil to disable.\n                        properties:\n                          autoCreate:\n                            description: 'AutoCreate enables automatic creation of\n                              host endpoints for every node. [Default: Disabled]'\n                            type: string\n                        type: object\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                      syncLabels:\n                        description: 'SyncLabels controls whether to copy Kubernetes\n                          node labels to Calico nodes. [Default: Enabled]'\n                        type: string\n                    type: object\n                  policy:\n                    description: Policy enables and configures the policy controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  serviceAccount:\n                    description: ServiceAccount enables and configures the service\n                      account controller. Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  workloadEndpoint:\n                    description: WorkloadEndpoint enables and configures the workload\n                      endpoint controller. Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                type: object\n              etcdV3CompactionPeriod:\n                description: 'EtcdV3CompactionPeriod is the period between etcdv3\n                  compaction requests. Set to 0 to disable. [Default: 10m]'\n                type: string\n              healthChecks:\n                description: 'HealthChecks enables or disables support for health\n                  checks [Default: Enabled]'\n                type: string\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: Info]'\n                type: string\n            required:\n            - controllers\n            type: object\n          status:\n            description: KubeControllersConfigurationStatus represents the status\n              of the configuration. It's useful for admins to be able to see the actual\n              config that was applied, which can be modified by environment variables\n              on the kube-controllers process.\n            properties:\n              environmentVars:\n                additionalProperties:\n                  type: string\n                description: EnvironmentVars contains the environment variables on\n                  the kube-controllers that influenced the RunningConfig.\n                type: object\n              runningConfig:\n                description: RunningConfig contains the effective config that is running\n                  in the kube-controllers pod, after merging the API resource with\n                  any environment variables.\n                properties:\n                  controllers:\n                    description: Controllers enables and configures individual Kubernetes\n                      controllers\n                    properties:\n                      namespace:\n                        description: Namespace enables and configures the namespace\n                          controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      node:\n                        description: Node enables and configures the node controller.\n                          Enabled by default, set to nil to disable.\n                        properties:\n                          hostEndpoint:\n                            description: HostEndpoint controls syncing nodes to host\n                              endpoints. Disabled by default, set to nil to disable.\n                            properties:\n                              autoCreate:\n                                description: 'AutoCreate enables automatic creation\n                                  of host endpoints for every node. [Default: Disabled]'\n                                type: string\n                            type: object\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                          syncLabels:\n                            description: 'SyncLabels controls whether to copy Kubernetes\n                              node labels to Calico nodes. [Default: Enabled]'\n                            type: string\n                        type: object\n                      policy:\n                        description: Policy enables and configures the policy controller.\n                          Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      serviceAccount:\n                        description: ServiceAccount enables and configures the service\n                          account controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      workloadEndpoint:\n                        description: WorkloadEndpoint enables and configures the workload\n                          endpoint controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                    type: object\n                  etcdV3CompactionPeriod:\n                    description: 'EtcdV3CompactionPeriod is the period between etcdv3\n                      compaction requests. Set to 0 to disable. [Default: 10m]'\n                    type: string\n                  healthChecks:\n                    description: 'HealthChecks enables or disables support for health\n                      checks [Default: Enabled]'\n                    type: string\n                  logSeverityScreen:\n                    description: 'LogSeverityScreen is the log severity above which\n                      logs are sent to the stdout. [Default: Info]'\n                    type: string\n                required:\n                - controllers\n                type: object\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: NetworkPolicy\n    listKind: NetworkPolicyList\n    plural: networkpolicies\n    singular: networkpolicy\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            properties:\n              egress:\n                description: The ordered set of egress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              ingress:\n                description: The ordered set of ingress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              order:\n                description: Order is an optional field that specifies the order in\n                  which the policy is applied. Policies with higher \"order\" are applied\n                  after those with lower order.  If the order is omitted, it may be\n                  considered to be \"infinite\" - i.e. the policy will be applied last.  Policies\n                  with identical order will be applied in alphanumerical order based\n                  on the Policy \"Name\".\n                type: number\n              selector:\n                description: \"The selector is an expression used to pick pick out\n                  the endpoints that the policy should be applied to. \\n Selector\n                  expressions follow this syntax: \\n \\tlabel == \\\"string_literal\\\"\n                  \\ -\u003e  comparison, e.g. my_label == \\\"foo bar\\\" \\tlabel != \\\"string_literal\\\"\n                  \\  -\u003e  not equal; also matches if label is not present \\tlabel in\n                  { \\\"a\\\", \\\"b\\\", \\\"c\\\", ... }  -\u003e  true if the value of label X is\n                  one of \\\"a\\\", \\\"b\\\", \\\"c\\\" \\tlabel not in { \\\"a\\\", \\\"b\\\", \\\"c\\\",\n                  ... }  -\u003e  true if the value of label X is not one of \\\"a\\\", \\\"b\\\",\n                  \\\"c\\\" \\thas(label_name)  -\u003e True if that label is present \\t! expr\n                  -\u003e negation of expr \\texpr \u0026\u0026 expr  -\u003e Short-circuit and \\texpr\n                  || expr  -\u003e Short-circuit or \\t( expr ) -\u003e parens for grouping \\tall()\n                  or the empty selector -\u003e matches all endpoints. \\n Label names are\n                  allowed to contain alphanumerics, -, _ and /. String literals are\n                  more permissive but they do not support escape characters. \\n Examples\n                  (with made-up labels): \\n \\ttype == \\\"webserver\\\" \u0026\u0026 deployment\n                  == \\\"prod\\\" \\ttype in {\\\"frontend\\\", \\\"backend\\\"} \\tdeployment !=\n                  \\\"dev\\\" \\t! has(label_name)\"\n                type: string\n              serviceAccountSelector:\n                description: ServiceAccountSelector is an optional field for an expression\n                  used to select a pod based on service accounts.\n                type: string\n              types:\n                description: \"Types indicates whether this policy applies to ingress,\n                  or to egress, or to both.  When not explicitly specified (and so\n                  the value on creation is empty or nil), Calico defaults Types according\n                  to what Ingress and Egress are present in the policy.  The default\n                  is: \\n - [ PolicyTypeIngress ], if there are no Egress rules (including\n                  the case where there are   also no Ingress rules) \\n - [ PolicyTypeEgress\n                  ], if there are Egress rules but no Ingress rules \\n - [ PolicyTypeIngress,\n                  PolicyTypeEgress ], if there are both Ingress and Egress rules.\n                  \\n When the policy is read back again, Types will always be one\n                  of these values, never empty or nil.\"\n                items:\n                  description: PolicyType enumerates the possible values of the PolicySpec\n                    Types field.\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: networksets.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: NetworkSet\n    listKind: NetworkSetList\n    plural: networksets\n    singular: networkset\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: NetworkSet is the Namespaced-equivalent of the GlobalNetworkSet.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: NetworkSetSpec contains the specification for a NetworkSet\n              resource.\n            properties:\n              nets:\n                description: The list of IP networks that belong to this set.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\n{{if eq .RBACConfig \"rbac\"}}\n# Source: calico/templates/calico-kube-controllers-rbac.yaml\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # kube-controllers manages hostendpoints.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n  # KubeControllersConfiguration is where it gets its config\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - kubecontrollersconfigurations\n    verbs:\n      # read its own config\n      - get\n      # create a default if none exists\n      - create\n      # update status\n      - update\n      # watch for changes\n      - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n---\n---\n# Source: calico/templates/calico-node-rbac.yaml\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only required for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n  # These permissions are required for Calico CNI to perform IPAM allocations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ipamconfigs\n    verbs:\n      - get\n  # Block affinities must also be watchable by confd for route aggregation.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n    verbs:\n      - watch\n  # The Calico IPAM migration needs to get daemonsets. These permissions can be\n  # removed if not upgrading from an installation using host-local IPAM.\n  - apiGroups: [\"apps\"]\n    resources:\n      - daemonsets\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n      # Rancher-specific: The annotation for scheduler.alpha.kubernetes.io/critical-pod originated from the v3.13.4 base \n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-node\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container performs upgrade from host-local IPAM to calico-ipam.\n        # It can be deleted if this is a fresh installation, or if you have already\n        # upgraded to use calico-ipam.\n        - name: upgrade-ipam\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/calico-ipam\", \"-upgrade\"]\n          envFrom:\n          - configMapRef:\n              # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.\n              name: kubernetes-services-endpoint\n              optional: true\n          env:\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n          volumeMounts:\n            - mountPath: /var/lib/cni/networks\n              name: host-local-net-dir\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n          securityContext:\n            privileged: true\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/install\"]\n          envFrom:\n          - configMapRef:\n              # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.\n              name: kubernetes-services-endpoint\n              optional: true\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs calico-node container on each Kubernetes node. This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          envFrom:\n          - configMapRef:\n              # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.\n              name: kubernetes-services-endpoint\n              optional: true\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,bgp\"\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: \"autodetect\"\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: \"Always\"\n            # Enable or Disable VXLAN on the default IP pool.\n            - name: CALICO_IPV4POOL_VXLAN\n              value: \"Never\"\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Set MTU for the VXLAN tunnel device.\n            - name: FELIX_VXLANMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Set MTU for the Wireguard tunnel device.\n            - name: FELIX_WIREGUARDMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            # Rancher-specific: Explicitly set CALICO_IPV4POOL_CIDR\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"{{.ClusterCIDR}}\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n            # Rancher-specific: Set FELIX_IPTABLESBACKEND to auto for autodetection of nftables\n            - name: FELIX_IPTABLESBACKEND\n              value: \"auto\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n              - -bird-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-ready\n              - -bird-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n            # For eBPF mode, we need to be able to mount the BPF filesystem at /sys/fs/bpf so we mount in the\n            # parent directory.\n            - name: sysfs\n              mountPath: /sys/fs/\n              # Bidirectional means that, if we mount the BPF filesystem at /sys/fs/bpf it will propagate to the host.\n              # If the host is known to mount that filesystem already then Bidirectional can be omitted.\n              mountPropagation: Bidirectional\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        - name: sysfs\n          hostPath:\n            path: /sys/fs/\n            type: DirectoryOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Mount in the directory for host-local IPAM allocations. This is\n        # used when upgrading from host-local to calico-ipam, and can be removed\n        # if not using the upgrade-ipam init container.\n        - name: host-local-net-dir\n          hostPath:\n            path: /var/lib/cni/networks\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n{{if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n      # Added by Rancher to mark as a critical pod.\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        # Rancher-specific: Set tolerations on the calico-kube-controllers so as to let it run on all nodes.\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n      {{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n---\n{{if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-etcd-secrets.yaml\n---\n# Source: calico/templates/calico-typha.yaml\n---\n# Source: calico/templates/configure-canal.yaml\n",
   "canal-v1.13": "\n{{if eq .RBACConfig \"rbac\"}}\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\n# Bind the Calico ClusterRole to the canal ServiceAccount.\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n\n# Canal Version v3.1.1\n# https://docs.projectcalico.org/v3.1/releases#v3.1.1\n# This manifest includes the following component versions:\n#   calico/node:v3.1.1\n#   calico/cni:v3.1.1\n#   coreos/flannel:v0.9.1\n\n---\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n          \"mtu\": {{.MTU}},\n{{- end}}\n{{- end}}\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n            \"type\": \"host-local\",\n            \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName: canal\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n\n---\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n",
   "canal-v1.15": "\n{{if eq .RBACConfig \"rbac\"}}\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\n# Bind the Calico ClusterRole to the canal ServiceAccount.\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n\n# Canal Version v3.1.1\n# https://docs.projectcalico.org/v3.1/releases#v3.1.1\n# This manifest includes the following component versions:\n#   calico/node:v3.1.1\n#   calico/cni:v3.1.1\n#   coreos/flannel:v0.9.1\n\n---\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n          \"mtu\": {{.MTU}},\n{{- end}}\n{{- end}}\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n            \"type\": \"host-local\",\n            \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n\n---\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n",
   "canal-v1.15-privileged": "\n# CanalTemplateV115Privileged\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n---\n# Source: calico/templates/rbac.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1450\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n",
   "canal-v1.15-privileged-calico3134": "\n# CanalTemplateV115PrivilegedCalico3134\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n---\n# Source: calico/templates/rbac.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1450\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n---\n",
   "canal-v1.15.12": "\n{{if eq .RBACConfig \"rbac\"}}\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\n# Bind the Calico ClusterRole to the canal ServiceAccount.\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n\n# Canal Version v3.1.1\n# https://docs.projectcalico.org/v3.1/releases#v3.1.1\n# This manifest includes the following component versions:\n#   calico/node:v3.1.1\n#   calico/cni:v3.1.1\n#   coreos/flannel:v0.9.1\n\n---\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n          \"mtu\": {{.MTU}},\n{{- end}}\n{{- end}}\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n            \"type\": \"host-local\",\n            \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      initContainers:\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n\n---\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy and networking mode.\n\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n",
   "canal-v1.16": "\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n          \"mtu\": {{.MTU}},\n{{- end}}\n{{- end}}\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\",\n              \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n          },\n          \"kubernetes\": {\n            \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n{{if eq .RBACConfig \"rbac\"}}\n---\n# Source: calico/templates/rbac.yaml\n\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Tolerate this effect so the pods will be schedulable at all times\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n        - key: \"node-role.kubernetes.io/controlplane\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n        - key: \"node-role.kubernetes.io/etcd\"\n          operator: \"Exists\"\n          effect: \"NoExecute\"\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"192.168.0.0/16\"\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n",
   "canal-v1.17": "\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n          \"mtu\": {{.MTU}},\n{{- end}}\n{{- end}}\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\",\n              \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n          },\n          \"kubernetes\": {\n            \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n{{if eq .RBACConfig \"rbac\"}}\n---\n# Source: calico/templates/rbac.yaml\n\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Tolerate this effect so the pods will be schedulable at all times\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n        - key: \"node-role.kubernetes.io/controlplane\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n        - key: \"node-role.kubernetes.io/etcd\"\n          operator: \"Exists\"\n          effect: \"NoExecute\"\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            - name: CALICO_IPV4POOL_CIDR\n              value: \"192.168.0.0/16\"\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n",
   "canal-v1.17-privileged": "\n# CanalTemplateV117Privileged\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n---\n# Source: calico/templates/rbac.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1450\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n",
   "canal-v1.17-privileged-calico3134": "\n# CanalTemplateV117PrivilegedCalico3134\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n---\n# Source: calico/templates/rbac.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only requried for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1450\"\n{{- end}}\n\n  # The CNI network configuration to install on each node.  The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        # This, along with the CriticalAddonsOnly toleration below,\n        # marks the pod as a critical add-on, ensuring it gets\n        # priority scheduling and that its resources are reserved\n        # if it ever gets evicted.\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs canal container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within --cluster-cidr.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: \"192.168.0.0/16\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n# Source: calico/templates/kdd-crds.yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgppeers.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPPeer\n    plural: bgppeers\n    singular: bgppeer\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BlockAffinity\n    plural: blockaffinities\n    singular: blockaffinity\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMBlock\n    plural: ipamblocks\n    singular: ipamblock\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMConfig\n    plural: ipamconfigs\n    singular: ipamconfig\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPAMHandle\n    plural: ipamhandles\n    singular: ipamhandle\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n---\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networksets.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkSet\n    plural: networksets\n    singular: networkset\n",
   "canal-v1.8": "\n{{if eq .RBACConfig \"rbac\"}}\n# Calico Roles\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: calico\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - namespaces\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - update\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n      - list\n      - watch\n      - patch\n  - apiGroups: [\"\"]\n    resources:\n      - services\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - update\n      - watch\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - globalnetworkpolicies\n      - networkpolicies\n      - clusterinformations\n      - hostendpoints\n      - globalnetworksets\n    verbs:\n      - create\n      - get\n      - list\n      - update\n      - watch\n\n---\n\n# Flannel roles\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n\n---\n\n# Bind the calico ClusterRole to the canal ServiceAccount.\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n{{end}}\n\n# Canal Version v3.1.1\n# https://docs.projectcalico.org/v3.1/releases#v3.1.1\n# This manifest includes the following component versions:\n#   calico/node:v3.1.1\n#   calico/cni:v3.1.1\n#   coreos/flannel:v0.9.1\n\n---\n# This ConfigMap can be used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n\n  # The CNI network configuration to install on each node.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.0\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"WARNING\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"ipam\": {\n            \"type\": \"host-local\",\n            \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n            \"type\": \"k8s\",\n            \"k8s_auth_token\": \"__SERVICEACCOUNT_TOKEN__\"\n          },\n          \"kubernetes\": {\n            \"k8s_api_root\": \"https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__\",\n            \"kubeconfig\": \"{{.KubeCfg}}\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        }\n      ]\n    }\n\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\",\n        \"VNI\": {{.FlannelBackend.VNI}},\n        \"Port\": {{.FlannelBackend.Port}}\n      }\n    }\n\n---\n\n# This manifest installs the calico/node container, as well\n# as the Calico CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: extensions/v1beta1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      serviceAccountName: canal\n      tolerations:\n        # Tolerate this effect so the pods will be schedulable at all times\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n        - key: \"node-role.kubernetes.io/controlplane\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n        - key: \"node-role.kubernetes.io/etcd\"\n          operator: \"Exists\"\n          effect: \"NoExecute\"\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      containers:\n        # Runs calico/node container on each Kubernetes node.  This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Enable felix logging to stdout\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"Warning\"\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Disable file logging so kubectl logs works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # Disable IPV6 support in Felix.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            httpGet:\n              path: /liveness\n              port: 9099\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n        # This container installs the Calico CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/install-cni.sh\"]\n          env:\n            - name: CNI_CONF_NAME\n              value: \"10-calico.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - name: run\n            mountPath: /run\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n          - name: xtables-lock\n            mountPath: /run/xtables.lock\n            readOnly: false\n      volumes:\n        # Used by calico/node.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used by flannel.\n        - name: run\n          hostPath:\n            path: /run\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n\n# Create all the CustomResourceDefinitions needed for\n# Calico policy-only mode.\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n   name: felixconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: FelixConfiguration\n    plural: felixconfigurations\n    singular: felixconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: BGPConfiguration\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: ippools.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: IPPool\n    plural: ippools\n    singular: ippool\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: ClusterInformation\n    plural: clusterinformations\n    singular: clusterinformation\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkPolicy\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  scope: Namespaced\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: NetworkPolicy\n    plural: networkpolicies\n    singular: networkpolicy\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: GlobalNetworkSet\n    plural: globalnetworksets\n    singular: globalnetworkset\n\n---\n\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  scope: Cluster\n  group: crd.projectcalico.org\n  version: v1\n  names:\n    kind: HostEndpoint\n    plural: hostendpoints\n    singular: hostendpoint\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n",
   "canal-v3.16.0": "\n# Canal Template based on Canal v3.16.0 \n---\n# Source: calico/templates/calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Canal installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: canal-config\n  namespace: kube-system\ndata:\n  # Typha is disabled.\n  typha_service_name: \"none\"\n  # The interface used by canal for host \u003c-\u003e host communication.\n  # If left blank, then the interface is chosen using the node's\n  # default route.\n  canal_iface: \"{{.CanalInterface}}\"\n  # Whether or not to masquerade traffic to destinations not within\n  # the pod network.\n  masquerade: \"true\"\n  # Configure the MTU to use\n{{- if .MTU }}\n{{- if ne .MTU 0 }}\n  veth_mtu: \"{{.MTU}}\"\n{{- end}}\n{{- else }}\n  veth_mtu: \"1450\"\n{{- end}}\n  # The CNI network configuration to install on each node. The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    {\n      \"name\": \"k8s-pod-network\",\n      \"cniVersion\": \"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"calico\",\n          \"log_level\": \"info\",\n          \"log_file_path\": \"/var/log/calico/cni/cni.log\",\n          \"datastore_type\": \"kubernetes\",\n          \"nodename\": \"__KUBERNETES_NODE_NAME__\",\n          \"mtu\": __CNI_MTU__,\n          \"ipam\": {\n              \"type\": \"host-local\",\n              \"subnet\": \"usePodCidr\"\n          },\n          \"policy\": {\n              \"type\": \"k8s\"\n          },\n          \"kubernetes\": {\n              \"kubeconfig\": \"__KUBECONFIG_FILEPATH__\"\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"snat\": true,\n          \"capabilities\": {\"portMappings\": true}\n        },\n        {\n          \"type\": \"bandwidth\",\n          \"capabilities\": {\"bandwidth\": true}\n        }\n      ]\n    }\n  # Flannel network configuration. Mounted into the flannel container.\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\"\n      }\n    }\n---\n# Source: calico/templates/kdd-crds.yaml\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: bgpconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BGPConfiguration\n    listKind: BGPConfigurationList\n    plural: bgpconfigurations\n    singular: bgpconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: BGPConfiguration contains the configuration for any BGP routing.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BGPConfigurationSpec contains the values of the BGP configuration.\n            properties:\n              asNumber:\n                description: 'ASNumber is the default AS number used by a node. [Default:\n                  64512]'\n                format: int32\n                type: integer\n              communities:\n                description: Communities is a list of BGP community values and their\n                  arbitrary names for tagging routes.\n                items:\n                  description: Community contains standard or large community value\n                    and its name.\n                  properties:\n                    name:\n                      description: Name given to community value.\n                      type: string\n                    value:\n                      description: Value must be of format `aa:nn` or `aa:nn:mm`.\n                        For standard community use `aa:nn` format, where `aa` and\n                        `nn` are 16 bit number. For large community use `aa:nn:mm`\n                        format, where `aa`, `nn` and `mm` are 32 bit number. Where,\n                        `aa` is an AS Number, `nn` and `mm` are per-AS identifier.\n                      pattern: ^(\\d+):(\\d+)$|^(\\d+):(\\d+):(\\d+)$\n                      type: string\n                  type: object\n                type: array\n              listenPort:\n                description: ListenPort is the port where BGP protocol should listen.\n                  Defaults to 179\n                maximum: 65535\n                minimum: 1\n                type: integer\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: INFO]'\n                type: string\n              nodeToNodeMeshEnabled:\n                description: 'NodeToNodeMeshEnabled sets whether full node to node\n                  BGP mesh is enabled. [Default: true]'\n                type: boolean\n              prefixAdvertisements:\n                description: PrefixAdvertisements contains per-prefix advertisement\n                  configuration.\n                items:\n                  description: PrefixAdvertisement configures advertisement properties\n                    for the specified CIDR.\n                  properties:\n                    cidr:\n                      description: CIDR for which properties should be advertised.\n                      type: string\n                    communities:\n                      description: Communities can be list of either community names\n                        already defined in `Specs.Communities` or community value\n                        of format `aa:nn` or `aa:nn:mm`. For standard community use\n                        `aa:nn` format, where `aa` and `nn` are 16 bit number. For\n                        large community use `aa:nn:mm` format, where `aa`, `nn` and\n                        `mm` are 32 bit number. Where,`aa` is an AS Number, `nn` and\n                        `mm` are per-AS identifier.\n                      items:\n                        type: string\n                      type: array\n                  type: object\n                type: array\n              serviceClusterIPs:\n                description: ServiceClusterIPs are the CIDR blocks from which service\n                  cluster IPs are allocated. If specified, Calico will advertise these\n                  blocks, as well as any cluster IPs within them.\n                items:\n                  description: ServiceClusterIPBlock represents a single allowed ClusterIP\n                    CIDR block.\n                  properties:\n                    cidr:\n                      type: string\n                  type: object\n                type: array\n              serviceExternalIPs:\n                description: ServiceExternalIPs are the CIDR blocks for Kubernetes\n                  Service External IPs. Kubernetes Service ExternalIPs will only be\n                  advertised if they are within one of these blocks.\n                items:\n                  description: ServiceExternalIPBlock represents a single allowed\n                    External IP CIDR block.\n                  properties:\n                    cidr:\n                      type: string\n                  type: object\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: bgppeers.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BGPPeer\n    listKind: BGPPeerList\n    plural: bgppeers\n    singular: bgppeer\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BGPPeerSpec contains the specification for a BGPPeer resource.\n            properties:\n              asNumber:\n                description: The AS Number of the peer.\n                format: int32\n                type: integer\n              keepOriginalNextHop:\n                description: Option to keep the original nexthop field when routes\n                  are sent to a BGP Peer. Setting \"true\" configures the selected BGP\n                  Peers node to use the \"next hop keep;\" instead of \"next hop self;\"(default)\n                  in the specific branch of the Node on \"bird.cfg\".\n                type: boolean\n              node:\n                description: The node name identifying the Calico node instance that\n                  is peering with this peer. If this is not set, this represents a\n                  global peer, i.e. a peer that peers with every node in the deployment.\n                type: string\n              nodeSelector:\n                description: Selector for the nodes that should have this peering.  When\n                  this is set, the Node field must be empty.\n                type: string\n              peerIP:\n                description: The IP address of the peer followed by an optional port\n                  number to peer with. If port number is given, format should be `[\u003cIPv6\u003e]:port`\n                  or `\u003cIPv4\u003e:\u003cport\u003e` for IPv4. If optional port number is not set,\n                  and this peer IP and ASNumber belongs to a calico/node with ListenPort\n                  set in BGPConfiguration, then we use that port to peer.\n                type: string\n              peerSelector:\n                description: Selector for the remote nodes to peer with.  When this\n                  is set, the PeerIP and ASNumber fields must be empty.  For each\n                  peering between the local node and selected remote nodes, we configure\n                  an IPv4 peering if both ends have NodeBGPSpec.IPv4Address specified,\n                  and an IPv6 peering if both ends have NodeBGPSpec.IPv6Address specified.  The\n                  remote AS number comes from the remote nodes NodeBGPSpec.ASNumber,\n                  or the global default if that is not set.\n                type: string\n            required:\n            - asNumber\n            - peerIP\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: blockaffinities.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: BlockAffinity\n    listKind: BlockAffinityList\n    plural: blockaffinities\n    singular: blockaffinity\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: BlockAffinitySpec contains the specification for a BlockAffinity\n              resource.\n            properties:\n              cidr:\n                type: string\n              deleted:\n                description: Deleted indicates that this block affinity is being deleted.\n                  This field is a string for compatibility with older releases that\n                  mistakenly treat this field as a string.\n                type: string\n              node:\n                type: string\n              state:\n                type: string\n            required:\n            - cidr\n            - deleted\n            - node\n            - state\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: clusterinformations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: ClusterInformation\n    listKind: ClusterInformationList\n    plural: clusterinformations\n    singular: clusterinformation\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: ClusterInformation contains the cluster specific information.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: ClusterInformationSpec contains the values of describing\n              the cluster.\n            properties:\n              calicoVersion:\n                description: CalicoVersion is the version of Calico that the cluster\n                  is running\n                type: string\n              clusterGUID:\n                description: ClusterGUID is the GUID of the cluster\n                type: string\n              clusterType:\n                description: ClusterType describes the type of the cluster\n                type: string\n              datastoreReady:\n                description: DatastoreReady is used during significant datastore migrations\n                  to signal to components such as Felix that it should wait before\n                  accessing the datastore.\n                type: boolean\n              variant:\n                description: Variant declares which variant of Calico should be active.\n                type: string\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: felixconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: FelixConfiguration\n    listKind: FelixConfigurationList\n    plural: felixconfigurations\n    singular: felixconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: Felix Configuration contains the configuration for Felix.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: FelixConfigurationSpec contains the values of the Felix configuration.\n            properties:\n              awsSrcDstCheck:\n                description: 'Set source-destination-check on AWS EC2 instances. Accepted\n                  value must be one of \"DoNothing\", \"Enabled\" or \"Disabled\". [Default:\n                  DoNothing]'\n                enum:\n                - DoNothing\n                - Enable\n                - Disable\n                type: string\n              bpfConnectTimeLoadBalancingEnabled:\n                description: 'BPFConnectTimeLoadBalancingEnabled when in BPF mode,\n                  controls whether Felix installs the connection-time load balancer.  The\n                  connect-time load balancer is required for the host to be able to\n                  reach Kubernetes services and it improves the performance of pod-to-service\n                  connections.  The only reason to disable it is for debugging purposes.  [Default:\n                  true]'\n                type: boolean\n              bpfDataIfacePattern:\n                description: 'BPFDataIfacePattern is a regular expression that controls\n                  which interfaces Felix should attach BPF programs to in order to\n                  catch traffic to/from the network.  This needs to match the interfaces\n                  that Calico workload traffic flows over as well as any interfaces\n                  that handle incoming traffic to nodeports and services from outside\n                  the cluster.  It should not match the workload interfaces (usually\n                  named cali...). [Default: ^(en.*|eth.*|tunl0$)]'\n                type: string\n              bpfDisableUnprivileged:\n                description: 'BPFDisableUnprivileged, if enabled, Felix sets the kernel.unprivileged_bpf_disabled\n                  sysctl to disable unprivileged use of BPF.  This ensures that unprivileged\n                  users cannot access Calico''s BPF maps and cannot insert their own\n                  BPF programs to interfere with Calico''s. [Default: true]'\n                type: boolean\n              bpfEnabled:\n                description: 'BPFEnabled, if enabled Felix will use the BPF dataplane.\n                  [Default: false]'\n                type: boolean\n              bpfExternalServiceMode:\n                description: 'BPFExternalServiceMode in BPF mode, controls how connections\n                  from outside the cluster to services (node ports and cluster IPs)\n                  are forwarded to remote workloads.  If set to \"Tunnel\" then both\n                  request and response traffic is tunneled to the remote node.  If\n                  set to \"DSR\", the request traffic is tunneled but the response traffic\n                  is sent directly from the remote node.  In \"DSR\" mode, the remote\n                  node appears to use the IP of the ingress node; this requires a\n                  permissive L2 network.  [Default: Tunnel]'\n                type: string\n              bpfKubeProxyEndpointSlicesEnabled:\n                description: BPFKubeProxyEndpointSlicesEnabled in BPF mode, controls\n                  whether Felix's embedded kube-proxy accepts EndpointSlices or not.\n                type: boolean\n              bpfKubeProxyIptablesCleanupEnabled:\n                description: 'BPFKubeProxyIptablesCleanupEnabled, if enabled in BPF\n                  mode, Felix will proactively clean up the upstream Kubernetes kube-proxy''s\n                  iptables chains.  Should only be enabled if kube-proxy is not running.  [Default:\n                  true]'\n                type: boolean\n              bpfKubeProxyMinSyncPeriod:\n                description: 'BPFKubeProxyMinSyncPeriod, in BPF mode, controls the\n                  minimum time between updates to the dataplane for Felix''s embedded\n                  kube-proxy.  Lower values give reduced set-up latency.  Higher values\n                  reduce Felix CPU usage by batching up more work.  [Default: 1s]'\n                type: string\n              bpfLogLevel:\n                description: 'BPFLogLevel controls the log level of the BPF programs\n                  when in BPF dataplane mode.  One of \"Off\", \"Info\", or \"Debug\".  The\n                  logs are emitted to the BPF trace pipe, accessible with the command\n                  `tc exec bpf debug`. [Default: Off].'\n                type: string\n              chainInsertMode:\n                description: 'ChainInsertMode controls whether Felix hooks the kernels\n                  top-level iptables chains by inserting a rule at the top of the\n                  chain or by appending a rule at the bottom. insert is the safe default\n                  since it prevents Calicos rules from being bypassed. If you switch\n                  to append mode, be sure that the other rules in the chains signal\n                  acceptance by falling through to the Calico rules, otherwise the\n                  Calico policy will be bypassed. [Default: insert]'\n                type: string\n              dataplaneDriver:\n                type: string\n              debugDisableLogDropping:\n                type: boolean\n              debugMemoryProfilePath:\n                type: string\n              debugSimulateCalcGraphHangAfter:\n                type: string\n              debugSimulateDataplaneHangAfter:\n                type: string\n              defaultEndpointToHostAction:\n                description: 'DefaultEndpointToHostAction controls what happens to\n                  traffic that goes from a workload endpoint to the host itself (after\n                  the traffic hits the endpoint egress policy). By default Calico\n                  blocks traffic from workload endpoints to the host itself with an\n                  iptables DROP action. If you want to allow some or all traffic\n                  from endpoint to host, set this parameter to RETURN or ACCEPT. Use\n                  RETURN if you have your own rules in the iptables INPUT chain;\n                  Calico will insert its rules at the top of that chain, then RETURN\n                  packets to the INPUT chain once it has completed processing workload\n                  endpoint egress policy. Use ACCEPT to unconditionally accept packets\n                  from workloads after processing workload endpoint egress policy.\n                  [Default: Drop]'\n                type: string\n              deviceRouteProtocol:\n                description: This defines the route protocol added to programmed device\n                  routes, by default this will be RTPROT_BOOT when left blank.\n                type: integer\n              deviceRouteSourceAddress:\n                description: This is the source address to use on programmed device\n                  routes. By default the source address is left blank, leaving the\n                  kernel to choose the source address used.\n                type: string\n              disableConntrackInvalidCheck:\n                type: boolean\n              endpointReportingDelay:\n                type: string\n              endpointReportingEnabled:\n                type: boolean\n              externalNodesList:\n                description: ExternalNodesCIDRList is a list of CIDR's of external-non-calico-nodes\n                  which may source tunnel traffic and have the tunneled traffic be\n                  accepted at calico nodes.\n                items:\n                  type: string\n                type: array\n              failsafeInboundHostPorts:\n                description: 'FailsafeInboundHostPorts is a comma-delimited list of\n                  UDP/TCP ports that Felix will allow incoming traffic to host endpoints\n                  on irrespective of the security policy. This is useful to avoid\n                  accidentally cutting off a host with incorrect configuration. Each\n                  port should be specified as tcp:\u003cport-number\u003e or udp:\u003cport-number\u003e.\n                  For back-compatibility, if the protocol is not specified, it defaults\n                  to tcp. To disable all inbound host ports, use the value none.\n                  The default value allows ssh access and DHCP. [Default: tcp:22,\n                  udp:68, tcp:179, tcp:2379, tcp:2380, tcp:6443, tcp:6666, tcp:6667]'\n                items:\n                  description: ProtoPort is combination of protocol and port, both\n                    must be specified.\n                  properties:\n                    port:\n                      type: integer\n                    protocol:\n                      type: string\n                  required:\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              failsafeOutboundHostPorts:\n                description: 'FailsafeOutboundHostPorts is a comma-delimited list\n                  of UDP/TCP ports that Felix will allow outgoing traffic from host\n                  endpoints to irrespective of the security policy. This is useful\n                  to avoid accidentally cutting off a host with incorrect configuration.\n                  Each port should be specified as tcp:\u003cport-number\u003e or udp:\u003cport-number\u003e.\n                  For back-compatibility, if the protocol is not specified, it defaults\n                  to tcp. To disable all outbound host ports, use the value none.\n                  The default value opens etcds standard ports to ensure that Felix\n                  does not get cut off from etcd as well as allowing DHCP and DNS.\n                  [Default: tcp:179, tcp:2379, tcp:2380, tcp:6443, tcp:6666, tcp:6667,\n                  udp:53, udp:67]'\n                items:\n                  description: ProtoPort is combination of protocol and port, both\n                    must be specified.\n                  properties:\n                    port:\n                      type: integer\n                    protocol:\n                      type: string\n                  required:\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              featureDetectOverride:\n                description: FeatureDetectOverride is used to override the feature\n                  detection. Values are specified in a comma separated list with no\n                  spaces, example; \"SNATFullyRandom=true,MASQFullyRandom=false,RestoreSupportsLock=\".\n                  \"true\" or \"false\" will force the feature, empty or omitted values\n                  are auto-detected.\n                type: string\n              genericXDPEnabled:\n                description: 'GenericXDPEnabled enables Generic XDP so network cards\n                  that don''t support XDP offload or driver modes can use XDP. This\n                  is not recommended since it doesn''t provide better performance\n                  than iptables. [Default: false]'\n                type: boolean\n              healthEnabled:\n                type: boolean\n              healthHost:\n                type: string\n              healthPort:\n                type: integer\n              interfaceExclude:\n                description: 'InterfaceExclude is a comma-separated list of interfaces\n                  that Felix should exclude when monitoring for host endpoints. The\n                  default value ensures that Felix ignores Kubernetes'' IPVS dummy\n                  interface, which is used internally by kube-proxy. If you want to\n                  exclude multiple interface names using a single value, the list\n                  supports regular expressions. For regular expressions you must wrap\n                  the value with ''/''. For example having values ''/^kube/,veth1''\n                  will exclude all interfaces that begin with ''kube'' and also the\n                  interface ''veth1''. [Default: kube-ipvs0]'\n                type: string\n              interfacePrefix:\n                description: 'InterfacePrefix is the interface name prefix that identifies\n                  workload endpoints and so distinguishes them from host endpoint\n                  interfaces. Note: in environments other than bare metal, the orchestrators\n                  configure this appropriately. For example our Kubernetes and Docker\n                  integrations set the cali value, and our OpenStack integration\n                  sets the tap value. [Default: cali]'\n                type: string\n              interfaceRefreshInterval:\n                description: InterfaceRefreshInterval is the period at which Felix\n                  rescans local interfaces to verify their state. The rescan can be\n                  disabled by setting the interval to 0.\n                type: string\n              ipipEnabled:\n                type: boolean\n              ipipMTU:\n                description: 'IPIPMTU is the MTU to set on the tunnel device. See\n                  Configuring MTU [Default: 1440]'\n                type: integer\n              ipsetsRefreshInterval:\n                description: 'IpsetsRefreshInterval is the period at which Felix re-checks\n                  all iptables state to ensure that no other process has accidentally\n                  broken Calicos rules. Set to 0 to disable iptables refresh. [Default:\n                  90s]'\n                type: string\n              iptablesBackend:\n                description: IptablesBackend specifies which backend of iptables will\n                  be used. The default is legacy.\n                type: string\n              iptablesFilterAllowAction:\n                type: string\n              iptablesLockFilePath:\n                description: 'IptablesLockFilePath is the location of the iptables\n                  lock file. You may need to change this if the lock file is not in\n                  its standard location (for example if you have mapped it into Felixs\n                  container at a different path). [Default: /run/xtables.lock]'\n                type: string\n              iptablesLockProbeInterval:\n                description: 'IptablesLockProbeInterval is the time that Felix will\n                  wait between attempts to acquire the iptables lock if it is not\n                  available. Lower values make Felix more responsive when the lock\n                  is contended, but use more CPU. [Default: 50ms]'\n                type: string\n              iptablesLockTimeout:\n                description: 'IptablesLockTimeout is the time that Felix will wait\n                  for the iptables lock, or 0, to disable. To use this feature, Felix\n                  must share the iptables lock file with all other processes that\n                  also take the lock. When running Felix inside a container, this\n                  requires the /run directory of the host to be mounted into the calico/node\n                  or calico/felix container. [Default: 0s disabled]'\n                type: string\n              iptablesMangleAllowAction:\n                type: string\n              iptablesMarkMask:\n                description: 'IptablesMarkMask is the mask that Felix selects its\n                  IPTables Mark bits from. Should be a 32 bit hexadecimal number with\n                  at least 8 bits set, none of which clash with any other mark bits\n                  in use on the system. [Default: 0xff000000]'\n                format: int32\n                type: integer\n              iptablesNATOutgoingInterfaceFilter:\n                type: string\n              iptablesPostWriteCheckInterval:\n                description: 'IptablesPostWriteCheckInterval is the period after Felix\n                  has done a write to the dataplane that it schedules an extra read\n                  back in order to check the write was not clobbered by another process.\n                  This should only occur if another application on the system doesnt\n                  respect the iptables lock. [Default: 1s]'\n                type: string\n              iptablesRefreshInterval:\n                description: 'IptablesRefreshInterval is the period at which Felix\n                  re-checks the IP sets in the dataplane to ensure that no other process\n                  has accidentally broken Calicos rules. Set to 0 to disable IP sets\n                  refresh. Note: the default for this value is lower than the other\n                  refresh intervals as a workaround for a Linux kernel bug that was\n                  fixed in kernel version 4.11. If you are using v4.11 or greater\n                  you may want to set this to, a higher value to reduce Felix CPU\n                  usage. [Default: 10s]'\n                type: string\n              ipv6Support:\n                type: boolean\n              kubeNodePortRanges:\n                description: 'KubeNodePortRanges holds list of port ranges used for\n                  service node ports. Only used if felix detects kube-proxy running\n                  in ipvs mode. Felix uses these ranges to separate host and workload\n                  traffic. [Default: 30000:32767].'\n                items:\n                  anyOf:\n                  - type: integer\n                  - type: string\n                  pattern: ^.*\n                  x-kubernetes-int-or-string: true\n                type: array\n              logFilePath:\n                description: 'LogFilePath is the full path to the Felix log. Set to\n                  none to disable file logging. [Default: /var/log/calico/felix.log]'\n                type: string\n              logPrefix:\n                description: 'LogPrefix is the log prefix that Felix uses when rendering\n                  LOG rules. [Default: calico-packet]'\n                type: string\n              logSeverityFile:\n                description: 'LogSeverityFile is the log severity above which logs\n                  are sent to the log file. [Default: Info]'\n                type: string\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: Info]'\n                type: string\n              logSeveritySys:\n                description: 'LogSeveritySys is the log severity above which logs\n                  are sent to the syslog. Set to None for no logging to syslog. [Default:\n                  Info]'\n                type: string\n              maxIpsetSize:\n                type: integer\n              metadataAddr:\n                description: 'MetadataAddr is the IP address or domain name of the\n                  server that can answer VM queries for cloud-init metadata. In OpenStack,\n                  this corresponds to the machine running nova-api (or in Ubuntu,\n                  nova-api-metadata). A value of none (case insensitive) means that\n                  Felix should not set up any NAT rule for the metadata path. [Default:\n                  127.0.0.1]'\n                type: string\n              metadataPort:\n                description: 'MetadataPort is the port of the metadata server. This,\n                  combined with global.MetadataAddr (if not None), is used to set\n                  up a NAT rule, from 169.254.169.254:80 to MetadataAddr:MetadataPort.\n                  In most cases this should not need to be changed [Default: 8775].'\n                type: integer\n              natOutgoingAddress:\n                description: NATOutgoingAddress specifies an address to use when performing\n                  source NAT for traffic in a natOutgoing pool that is leaving the\n                  network. By default the address used is an address on the interface\n                  the traffic is leaving on (ie it uses the iptables MASQUERADE target)\n                type: string\n              natPortRange:\n                anyOf:\n                - type: integer\n                - type: string\n                description: NATPortRange specifies the range of ports that is used\n                  for port mapping when doing outgoing NAT. When unset the default\n                  behavior of the network stack is used.\n                pattern: ^.*\n                x-kubernetes-int-or-string: true\n              netlinkTimeout:\n                type: string\n              openstackRegion:\n                description: 'OpenstackRegion is the name of the region that a particular\n                  Felix belongs to. In a multi-region Calico/OpenStack deployment,\n                  this must be configured somehow for each Felix (here in the datamodel,\n                  or in felix.cfg or the environment on each compute node), and must\n                  match the [calico] openstack_region value configured in neutron.conf\n                  on each node. [Default: Empty]'\n                type: string\n              policySyncPathPrefix:\n                description: 'PolicySyncPathPrefix is used to by Felix to communicate\n                  policy changes to external services, like Application layer policy.\n                  [Default: Empty]'\n                type: string\n              prometheusGoMetricsEnabled:\n                description: 'PrometheusGoMetricsEnabled disables Go runtime metrics\n                  collection, which the Prometheus client does by default, when set\n                  to false. This reduces the number of metrics reported, reducing\n                  Prometheus load. [Default: true]'\n                type: boolean\n              prometheusMetricsEnabled:\n                description: 'PrometheusMetricsEnabled enables the Prometheus metrics\n                  server in Felix if set to true. [Default: false]'\n                type: boolean\n              prometheusMetricsHost:\n                description: 'PrometheusMetricsHost is the host that the Prometheus\n                  metrics server should bind to. [Default: empty]'\n                type: string\n              prometheusMetricsPort:\n                description: 'PrometheusMetricsPort is the TCP port that the Prometheus\n                  metrics server should bind to. [Default: 9091]'\n                type: integer\n              prometheusProcessMetricsEnabled:\n                description: 'PrometheusProcessMetricsEnabled disables process metrics\n                  collection, which the Prometheus client does by default, when set\n                  to false. This reduces the number of metrics reported, reducing\n                  Prometheus load. [Default: true]'\n                type: boolean\n              removeExternalRoutes:\n                description: Whether or not to remove device routes that have not\n                  been programmed by Felix. Disabling this will allow external applications\n                  to also add device routes. This is enabled by default which means\n                  we will remove externally added routes.\n                type: boolean\n              reportingInterval:\n                description: 'ReportingInterval is the interval at which Felix reports\n                  its status into the datastore or 0 to disable. Must be non-zero\n                  in OpenStack deployments. [Default: 30s]'\n                type: string\n              reportingTTL:\n                description: 'ReportingTTL is the time-to-live setting for process-wide\n                  status reports. [Default: 90s]'\n                type: string\n              routeRefreshInterval:\n                description: 'RouterefreshInterval is the period at which Felix re-checks\n                  the routes in the dataplane to ensure that no other process has\n                  accidentally broken Calicos rules. Set to 0 to disable route refresh.\n                  [Default: 90s]'\n                type: string\n              routeSource:\n                description: 'RouteSource configures where Felix gets its routing\n                  information. - WorkloadIPs: use workload endpoints to construct\n                  routes. - CalicoIPAM: the default - use IPAM data to construct routes.'\n                type: string\n              routeTableRange:\n                description: Calico programs additional Linux route tables for various\n                  purposes.  RouteTableRange specifies the indices of the route tables\n                  that Calico should use.\n                properties:\n                  max:\n                    type: integer\n                  min:\n                    type: integer\n                required:\n                - max\n                - min\n                type: object\n              sidecarAccelerationEnabled:\n                description: 'SidecarAccelerationEnabled enables experimental sidecar\n                  acceleration [Default: false]'\n                type: boolean\n              usageReportingEnabled:\n                description: 'UsageReportingEnabled reports anonymous Calico version\n                  number and cluster size to projectcalico.org. Logs warnings returned\n                  by the usage server. For example, if a significant security vulnerability\n                  has been discovered in the version of Calico being used. [Default:\n                  true]'\n                type: boolean\n              usageReportingInitialDelay:\n                description: 'UsageReportingInitialDelay controls the minimum delay\n                  before Felix makes a report. [Default: 300s]'\n                type: string\n              usageReportingInterval:\n                description: 'UsageReportingInterval controls the interval at which\n                  Felix makes reports. [Default: 86400s]'\n                type: string\n              useInternalDataplaneDriver:\n                type: boolean\n              vxlanEnabled:\n                type: boolean\n              vxlanMTU:\n                description: 'VXLANMTU is the MTU to set on the tunnel device. See\n                  Configuring MTU [Default: 1440]'\n                type: integer\n              vxlanPort:\n                type: integer\n              vxlanVNI:\n                type: integer\n              wireguardEnabled:\n                description: 'WireguardEnabled controls whether Wireguard is enabled.\n                  [Default: false]'\n                type: boolean\n              wireguardInterfaceName:\n                description: 'WireguardInterfaceName specifies the name to use for\n                  the Wireguard interface. [Default: wg.calico]'\n                type: string\n              wireguardListeningPort:\n                description: 'WireguardListeningPort controls the listening port used\n                  by Wireguard. [Default: 51820]'\n                type: integer\n              wireguardMTU:\n                description: 'WireguardMTU controls the MTU on the Wireguard interface.\n                  See Configuring MTU [Default: 1420]'\n                type: integer\n              wireguardRoutingRulePriority:\n                description: 'WireguardRoutingRulePriority controls the priority value\n                  to use for the Wireguard routing rule. [Default: 99]'\n                type: integer\n              xdpEnabled:\n                description: 'XDPEnabled enables XDP acceleration for suitable untracked\n                  incoming deny rules. [Default: true]'\n                type: boolean\n              xdpRefreshInterval:\n                description: 'XDPRefreshInterval is the period at which Felix re-checks\n                  all XDP state to ensure that no other process has accidentally broken\n                  Calico''s BPF maps or attached programs. Set to 0 to disable XDP\n                  refresh. [Default: 90s]'\n                type: string\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: globalnetworkpolicies.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: GlobalNetworkPolicy\n    listKind: GlobalNetworkPolicyList\n    plural: globalnetworkpolicies\n    singular: globalnetworkpolicy\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            properties:\n              applyOnForward:\n                description: ApplyOnForward indicates to apply the rules in this policy\n                  on forward traffic.\n                type: boolean\n              doNotTrack:\n                description: DoNotTrack indicates whether packets matched by the rules\n                  in this policy should go through the data plane's connection tracking,\n                  such as Linux conntrack.  If True, the rules in this policy are\n                  applied before any data plane connection tracking, and packets allowed\n                  by this policy are marked as not to be tracked.\n                type: boolean\n              egress:\n                description: The ordered set of egress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              ingress:\n                description: The ordered set of ingress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              namespaceSelector:\n                description: NamespaceSelector is an optional field for an expression\n                  used to select a pod based on namespaces.\n                type: string\n              order:\n                description: Order is an optional field that specifies the order in\n                  which the policy is applied. Policies with higher \"order\" are applied\n                  after those with lower order.  If the order is omitted, it may be\n                  considered to be \"infinite\" - i.e. the policy will be applied last.  Policies\n                  with identical order will be applied in alphanumerical order based\n                  on the Policy \"Name\".\n                type: number\n              preDNAT:\n                description: PreDNAT indicates to apply the rules in this policy before\n                  any DNAT.\n                type: boolean\n              selector:\n                description: \"The selector is an expression used to pick pick out\n                  the endpoints that the policy should be applied to. \\n Selector\n                  expressions follow this syntax: \\n \\tlabel == \\\"string_literal\\\"\n                  \\ -\u003e  comparison, e.g. my_label == \\\"foo bar\\\" \\tlabel != \\\"string_literal\\\"\n                  \\  -\u003e  not equal; also matches if label is not present \\tlabel in\n                  { \\\"a\\\", \\\"b\\\", \\\"c\\\", ... }  -\u003e  true if the value of label X is\n                  one of \\\"a\\\", \\\"b\\\", \\\"c\\\" \\tlabel not in { \\\"a\\\", \\\"b\\\", \\\"c\\\",\n                  ... }  -\u003e  true if the value of label X is not one of \\\"a\\\", \\\"b\\\",\n                  \\\"c\\\" \\thas(label_name)  -\u003e True if that label is present \\t! expr\n                  -\u003e negation of expr \\texpr \u0026\u0026 expr  -\u003e Short-circuit and \\texpr\n                  || expr  -\u003e Short-circuit or \\t( expr ) -\u003e parens for grouping \\tall()\n                  or the empty selector -\u003e matches all endpoints. \\n Label names are\n                  allowed to contain alphanumerics, -, _ and /. String literals are\n                  more permissive but they do not support escape characters. \\n Examples\n                  (with made-up labels): \\n \\ttype == \\\"webserver\\\" \u0026\u0026 deployment\n                  == \\\"prod\\\" \\ttype in {\\\"frontend\\\", \\\"backend\\\"} \\tdeployment !=\n                  \\\"dev\\\" \\t! has(label_name)\"\n                type: string\n              serviceAccountSelector:\n                description: ServiceAccountSelector is an optional field for an expression\n                  used to select a pod based on service accounts.\n                type: string\n              types:\n                description: \"Types indicates whether this policy applies to ingress,\n                  or to egress, or to both.  When not explicitly specified (and so\n                  the value on creation is empty or nil), Calico defaults Types according\n                  to what Ingress and Egress rules are present in the policy.  The\n                  default is: \\n - [ PolicyTypeIngress ], if there are no Egress rules\n                  (including the case where there are   also no Ingress rules) \\n\n                  - [ PolicyTypeEgress ], if there are Egress rules but no Ingress\n                  rules \\n - [ PolicyTypeIngress, PolicyTypeEgress ], if there are\n                  both Ingress and Egress rules. \\n When the policy is read back again,\n                  Types will always be one of these values, never empty or nil.\"\n                items:\n                  description: PolicyType enumerates the possible values of the PolicySpec\n                    Types field.\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: globalnetworksets.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: GlobalNetworkSet\n    listKind: GlobalNetworkSetList\n    plural: globalnetworksets\n    singular: globalnetworkset\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: GlobalNetworkSet contains a set of arbitrary IP sub-networks/CIDRs\n          that share labels to allow rules to refer to them via selectors.  The labels\n          of GlobalNetworkSet are not namespaced.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: GlobalNetworkSetSpec contains the specification for a NetworkSet\n              resource.\n            properties:\n              nets:\n                description: The list of IP networks that belong to this set.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: hostendpoints.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: HostEndpoint\n    listKind: HostEndpointList\n    plural: hostendpoints\n    singular: hostendpoint\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: HostEndpointSpec contains the specification for a HostEndpoint\n              resource.\n            properties:\n              expectedIPs:\n                description: \"The expected IP addresses (IPv4 and IPv6) of the endpoint.\n                  If \\\"InterfaceName\\\" is not present, Calico will look for an interface\n                  matching any of the IPs in the list and apply policy to that. Note:\n                  \\tWhen using the selector match criteria in an ingress or egress\n                  security Policy \\tor Profile, Calico converts the selector into\n                  a set of IP addresses. For host \\tendpoints, the ExpectedIPs field\n                  is used for that purpose. (If only the interface \\tname is specified,\n                  Calico does not learn the IPs of the interface for use in match\n                  \\tcriteria.)\"\n                items:\n                  type: string\n                type: array\n              interfaceName:\n                description: \"Either \\\"*\\\", or the name of a specific Linux interface\n                  to apply policy to; or empty.  \\\"*\\\" indicates that this HostEndpoint\n                  governs all traffic to, from or through the default network namespace\n                  of the host named by the \\\"Node\\\" field; entering and leaving that\n                  namespace via any interface, including those from/to non-host-networked\n                  local workloads. \\n If InterfaceName is not \\\"*\\\", this HostEndpoint\n                  only governs traffic that enters or leaves the host through the\n                  specific interface named by InterfaceName, or - when InterfaceName\n                  is empty - through the specific interface that has one of the IPs\n                  in ExpectedIPs. Therefore, when InterfaceName is empty, at least\n                  one expected IP must be specified.  Only external interfaces (such\n                  as eth0) are supported here; it isn't possible for a HostEndpoint\n                  to protect traffic through a specific local workload interface.\n                  \\n Note: Only some kinds of policy are implemented for \\\"*\\\" HostEndpoints;\n                  initially just pre-DNAT policy.  Please check Calico documentation\n                  for the latest position.\"\n                type: string\n              node:\n                description: The node name identifying the Calico node instance.\n                type: string\n              ports:\n                description: Ports contains the endpoint's named ports, which may\n                  be referenced in security policy rules.\n                items:\n                  properties:\n                    name:\n                      type: string\n                    port:\n                      type: integer\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                  required:\n                  - name\n                  - port\n                  - protocol\n                  type: object\n                type: array\n              profiles:\n                description: A list of identifiers of security Profile objects that\n                  apply to this endpoint. Each profile is applied in the order that\n                  they appear in this list.  Profile rules are applied after the selector-based\n                  security policy.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamblocks.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMBlock\n    listKind: IPAMBlockList\n    plural: ipamblocks\n    singular: ipamblock\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMBlockSpec contains the specification for an IPAMBlock\n              resource.\n            properties:\n              affinity:\n                type: string\n              allocations:\n                items:\n                  type: integer\n                  # TODO: This nullable is manually added in. We should update controller-gen\n                  # to handle []*int properly itself.\n                  nullable: true\n                type: array\n              attributes:\n                items:\n                  properties:\n                    handle_id:\n                      type: string\n                    secondary:\n                      additionalProperties:\n                        type: string\n                      type: object\n                  type: object\n                type: array\n              cidr:\n                type: string\n              deleted:\n                type: boolean\n              strictAffinity:\n                type: boolean\n              unallocated:\n                items:\n                  type: integer\n                type: array\n            required:\n            - allocations\n            - attributes\n            - cidr\n            - deleted\n            - strictAffinity\n            - unallocated\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamconfigs.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMConfig\n    listKind: IPAMConfigList\n    plural: ipamconfigs\n    singular: ipamconfig\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMConfigSpec contains the specification for an IPAMConfig\n              resource.\n            properties:\n              autoAllocateBlocks:\n                type: boolean\n              strictAffinity:\n                type: boolean\n            required:\n            - autoAllocateBlocks\n            - strictAffinity\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ipamhandles.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPAMHandle\n    listKind: IPAMHandleList\n    plural: ipamhandles\n    singular: ipamhandle\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPAMHandleSpec contains the specification for an IPAMHandle\n              resource.\n            properties:\n              block:\n                additionalProperties:\n                  type: integer\n                type: object\n              handleID:\n                type: string\n            required:\n            - block\n            - handleID\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: ippools.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: IPPool\n    listKind: IPPoolList\n    plural: ippools\n    singular: ippool\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: IPPoolSpec contains the specification for an IPPool resource.\n            properties:\n              blockSize:\n                description: The block size to use for IP address assignments from\n                  this pool. Defaults to 26 for IPv4 and 112 for IPv6.\n                type: integer\n              cidr:\n                description: The pool CIDR.\n                type: string\n              disabled:\n                description: When disabled is true, Calico IPAM will not assign addresses\n                  from this pool.\n                type: boolean\n              ipip:\n                description: 'Deprecated: this field is only used for APIv1 backwards\n                  compatibility. Setting this field is not allowed, this field is\n                  for internal use only.'\n                properties:\n                  enabled:\n                    description: When enabled is true, ipip tunneling will be used\n                      to deliver packets to destinations within this pool.\n                    type: boolean\n                  mode:\n                    description: The IPIP mode.  This can be one of \"always\" or \"cross-subnet\".  A\n                      mode of \"always\" will also use IPIP tunneling for routing to\n                      destination IP addresses within this pool.  A mode of \"cross-subnet\"\n                      will only use IPIP tunneling when the destination node is on\n                      a different subnet to the originating node.  The default value\n                      (if not specified) is \"always\".\n                    type: string\n                type: object\n              ipipMode:\n                description: Contains configuration for IPIP tunneling for this pool.\n                  If not specified, then this is defaulted to \"Never\" (i.e. IPIP tunneling\n                  is disabled).\n                type: string\n              nat-outgoing:\n                description: 'Deprecated: this field is only used for APIv1 backwards\n                  compatibility. Setting this field is not allowed, this field is\n                  for internal use only.'\n                type: boolean\n              natOutgoing:\n                description: When nat-outgoing is true, packets sent from Calico networked\n                  containers in this pool to destinations outside of this pool will\n                  be masqueraded.\n                type: boolean\n              nodeSelector:\n                description: Allows IPPool to allocate for a specific node by label\n                  selector.\n                type: string\n              vxlanMode:\n                description: Contains configuration for VXLAN tunneling for this pool.\n                  If not specified, then this is defaulted to \"Never\" (i.e. VXLAN\n                  tunneling is disabled).\n                type: string\n            required:\n            - cidr\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: kubecontrollersconfigurations.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: KubeControllersConfiguration\n    listKind: KubeControllersConfigurationList\n    plural: kubecontrollersconfigurations\n    singular: kubecontrollersconfiguration\n  scope: Cluster\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: KubeControllersConfigurationSpec contains the values of the\n              Kubernetes controllers configuration.\n            properties:\n              controllers:\n                description: Controllers enables and configures individual Kubernetes\n                  controllers\n                properties:\n                  namespace:\n                    description: Namespace enables and configures the namespace controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  node:\n                    description: Node enables and configures the node controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      hostEndpoint:\n                        description: HostEndpoint controls syncing nodes to host endpoints.\n                          Disabled by default, set to nil to disable.\n                        properties:\n                          autoCreate:\n                            description: 'AutoCreate enables automatic creation of\n                              host endpoints for every node. [Default: Disabled]'\n                            type: string\n                        type: object\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                      syncLabels:\n                        description: 'SyncLabels controls whether to copy Kubernetes\n                          node labels to Calico nodes. [Default: Enabled]'\n                        type: string\n                    type: object\n                  policy:\n                    description: Policy enables and configures the policy controller.\n                      Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  serviceAccount:\n                    description: ServiceAccount enables and configures the service\n                      account controller. Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                  workloadEndpoint:\n                    description: WorkloadEndpoint enables and configures the workload\n                      endpoint controller. Enabled by default, set to nil to disable.\n                    properties:\n                      reconcilerPeriod:\n                        description: 'ReconcilerPeriod is the period to perform reconciliation\n                          with the Calico datastore. [Default: 5m]'\n                        type: string\n                    type: object\n                type: object\n              etcdV3CompactionPeriod:\n                description: 'EtcdV3CompactionPeriod is the period between etcdv3\n                  compaction requests. Set to 0 to disable. [Default: 10m]'\n                type: string\n              healthChecks:\n                description: 'HealthChecks enables or disables support for health\n                  checks [Default: Enabled]'\n                type: string\n              logSeverityScreen:\n                description: 'LogSeverityScreen is the log severity above which logs\n                  are sent to the stdout. [Default: Info]'\n                type: string\n            required:\n            - controllers\n            type: object\n          status:\n            description: KubeControllersConfigurationStatus represents the status\n              of the configuration. It's useful for admins to be able to see the actual\n              config that was applied, which can be modified by environment variables\n              on the kube-controllers process.\n            properties:\n              environmentVars:\n                additionalProperties:\n                  type: string\n                description: EnvironmentVars contains the environment variables on\n                  the kube-controllers that influenced the RunningConfig.\n                type: object\n              runningConfig:\n                description: RunningConfig contains the effective config that is running\n                  in the kube-controllers pod, after merging the API resource with\n                  any environment variables.\n                properties:\n                  controllers:\n                    description: Controllers enables and configures individual Kubernetes\n                      controllers\n                    properties:\n                      namespace:\n                        description: Namespace enables and configures the namespace\n                          controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      node:\n                        description: Node enables and configures the node controller.\n                          Enabled by default, set to nil to disable.\n                        properties:\n                          hostEndpoint:\n                            description: HostEndpoint controls syncing nodes to host\n                              endpoints. Disabled by default, set to nil to disable.\n                            properties:\n                              autoCreate:\n                                description: 'AutoCreate enables automatic creation\n                                  of host endpoints for every node. [Default: Disabled]'\n                                type: string\n                            type: object\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                          syncLabels:\n                            description: 'SyncLabels controls whether to copy Kubernetes\n                              node labels to Calico nodes. [Default: Enabled]'\n                            type: string\n                        type: object\n                      policy:\n                        description: Policy enables and configures the policy controller.\n                          Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      serviceAccount:\n                        description: ServiceAccount enables and configures the service\n                          account controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                      workloadEndpoint:\n                        description: WorkloadEndpoint enables and configures the workload\n                          endpoint controller. Enabled by default, set to nil to disable.\n                        properties:\n                          reconcilerPeriod:\n                            description: 'ReconcilerPeriod is the period to perform\n                              reconciliation with the Calico datastore. [Default:\n                              5m]'\n                            type: string\n                        type: object\n                    type: object\n                  etcdV3CompactionPeriod:\n                    description: 'EtcdV3CompactionPeriod is the period between etcdv3\n                      compaction requests. Set to 0 to disable. [Default: 10m]'\n                    type: string\n                  healthChecks:\n                    description: 'HealthChecks enables or disables support for health\n                      checks [Default: Enabled]'\n                    type: string\n                  logSeverityScreen:\n                    description: 'LogSeverityScreen is the log severity above which\n                      logs are sent to the stdout. [Default: Info]'\n                    type: string\n                required:\n                - controllers\n                type: object\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: networkpolicies.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: NetworkPolicy\n    listKind: NetworkPolicyList\n    plural: networkpolicies\n    singular: networkpolicy\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            properties:\n              egress:\n                description: The ordered set of egress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              ingress:\n                description: The ordered set of ingress rules.  Each rule contains\n                  a set of packet match criteria and a corresponding action to apply.\n                items:\n                  description: \"A Rule encapsulates a set of match criteria and an\n                    action.  Both selector-based security Policy and security Profiles\n                    reference rules - separated out as a list of rules for both ingress\n                    and egress packet matching. \\n Each positive match criteria has\n                    a negated version, prefixed with Not. All the match criteria\n                    within a rule must be satisfied for a packet to match. A single\n                    rule can contain the positive and negative version of a match\n                    and both must be satisfied for the rule to match.\"\n                  properties:\n                    action:\n                      type: string\n                    destination:\n                      description: Destination contains the match criteria that apply\n                        to destination entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                    http:\n                      description: HTTP contains match criteria that apply to HTTP\n                        requests.\n                      properties:\n                        methods:\n                          description: Methods is an optional field that restricts\n                            the rule to apply only to HTTP requests that use one of\n                            the listed HTTP Methods (e.g. GET, PUT, etc.) Multiple\n                            methods are OR'd together.\n                          items:\n                            type: string\n                          type: array\n                        paths:\n                          description: 'Paths is an optional field that restricts\n                            the rule to apply to HTTP requests that use one of the\n                            listed HTTP Paths. Multiple paths are OR''d together.\n                            e.g: - exact: /foo - prefix: /bar NOTE: Each entry may\n                            ONLY specify either a `exact` or a `prefix` match. The\n                            validator will check for it.'\n                          items:\n                            description: 'HTTPPath specifies an HTTP path to match.\n                              It may be either of the form: exact: \u003cpath\u003e: which matches\n                              the path exactly or prefix: \u003cpath-prefix\u003e: which matches\n                              the path prefix'\n                            properties:\n                              exact:\n                                type: string\n                              prefix:\n                                type: string\n                            type: object\n                          type: array\n                      type: object\n                    icmp:\n                      description: ICMP is an optional field that restricts the rule\n                        to apply to a specific type and code of ICMP traffic.  This\n                        should only be specified if the Protocol field is set to \"ICMP\"\n                        or \"ICMPv6\".\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    ipVersion:\n                      description: IPVersion is an optional field that restricts the\n                        rule to only match a specific IP version.\n                      type: integer\n                    metadata:\n                      description: Metadata contains additional information for this\n                        rule\n                      properties:\n                        annotations:\n                          additionalProperties:\n                            type: string\n                          description: Annotations is a set of key value pairs that\n                            give extra information about the rule\n                          type: object\n                      type: object\n                    notICMP:\n                      description: NotICMP is the negated version of the ICMP field.\n                      properties:\n                        code:\n                          description: Match on a specific ICMP code.  If specified,\n                            the Type value must also be specified. This is a technical\n                            limitation imposed by the kernels iptables firewall,\n                            which Calico uses to enforce the rule.\n                          type: integer\n                        type:\n                          description: Match on a specific ICMP type.  For example\n                            a value of 8 refers to ICMP Echo Request (i.e. pings).\n                          type: integer\n                      type: object\n                    notProtocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: NotProtocol is the negated version of the Protocol\n                        field.\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    protocol:\n                      anyOf:\n                      - type: integer\n                      - type: string\n                      description: \"Protocol is an optional field that restricts the\n                        rule to only apply to traffic of a specific IP protocol. Required\n                        if any of the EntityRules contain Ports (because ports only\n                        apply to certain protocols). \\n Must be one of these string\n                        values: \\\"TCP\\\", \\\"UDP\\\", \\\"ICMP\\\", \\\"ICMPv6\\\", \\\"SCTP\\\",\n                        \\\"UDPLite\\\" or an integer in the range 1-255.\"\n                      pattern: ^.*\n                      x-kubernetes-int-or-string: true\n                    source:\n                      description: Source contains the match criteria that apply to\n                        source entity.\n                      properties:\n                        namespaceSelector:\n                          description: \"NamespaceSelector is an optional field that\n                            contains a selector expression. Only traffic that originates\n                            from (or terminates at) endpoints within the selected\n                            namespaces will be matched. When both NamespaceSelector\n                            and Selector are defined on the same rule, then only workload\n                            endpoints that are matched by both selectors will be selected\n                            by the rule. \\n For NetworkPolicy, an empty NamespaceSelector\n                            implies that the Selector is limited to selecting only\n                            workload endpoints in the same namespace as the NetworkPolicy.\n                            \\n For NetworkPolicy, `global()` NamespaceSelector implies\n                            that the Selector is limited to selecting only GlobalNetworkSet\n                            or HostEndpoint. \\n For GlobalNetworkPolicy, an empty\n                            NamespaceSelector implies the Selector applies to workload\n                            endpoints across all namespaces.\"\n                          type: string\n                        nets:\n                          description: Nets is an optional field that restricts the\n                            rule to only apply to traffic that originates from (or\n                            terminates at) IP addresses in any of the given subnets.\n                          items:\n                            type: string\n                          type: array\n                        notNets:\n                          description: NotNets is the negated version of the Nets\n                            field.\n                          items:\n                            type: string\n                          type: array\n                        notPorts:\n                          description: NotPorts is the negated version of the Ports\n                            field. Since only some protocols have ports, if any ports\n                            are specified it requires the Protocol match in the Rule\n                            to be set to \"TCP\" or \"UDP\".\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        notSelector:\n                          description: NotSelector is the negated version of the Selector\n                            field.  See Selector field for subtleties with negated\n                            selectors.\n                          type: string\n                        ports:\n                          description: \"Ports is an optional field that restricts\n                            the rule to only apply to traffic that has a source (destination)\n                            port that matches one of these ranges/values. This value\n                            is a list of integers or strings that represent ranges\n                            of ports. \\n Since only some protocols have ports, if\n                            any ports are specified it requires the Protocol match\n                            in the Rule to be set to \\\"TCP\\\" or \\\"UDP\\\".\"\n                          items:\n                            anyOf:\n                            - type: integer\n                            - type: string\n                            pattern: ^.*\n                            x-kubernetes-int-or-string: true\n                          type: array\n                        selector:\n                          description: \"Selector is an optional field that contains\n                            a selector expression (see Policy for sample syntax).\n                            \\ Only traffic that originates from (terminates at) endpoints\n                            matching the selector will be matched. \\n Note that: in\n                            addition to the negated version of the Selector (see NotSelector\n                            below), the selector expression syntax itself supports\n                            negation.  The two types of negation are subtly different.\n                            One negates the set of matched endpoints, the other negates\n                            the whole match: \\n \\tSelector = \\\"!has(my_label)\\\" matches\n                            packets that are from other Calico-controlled \\tendpoints\n                            that do not have the label my_label. \\n \\tNotSelector\n                            = \\\"has(my_label)\\\" matches packets that are not from\n                            Calico-controlled \\tendpoints that do have the label my_label.\n                            \\n The effect is that the latter will accept packets from\n                            non-Calico sources whereas the former is limited to packets\n                            from Calico-controlled endpoints.\"\n                          type: string\n                        serviceAccounts:\n                          description: ServiceAccounts is an optional field that restricts\n                            the rule to only apply to traffic that originates from\n                            (or terminates at) a pod running as a matching service\n                            account.\n                          properties:\n                            names:\n                              description: Names is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account whose name is in the list.\n                              items:\n                                type: string\n                              type: array\n                            selector:\n                              description: Selector is an optional field that restricts\n                                the rule to only apply to traffic that originates\n                                from (or terminates at) a pod running as a service\n                                account that matches the given label selector. If\n                                both Names and Selector are specified then they are\n                                AND'ed.\n                              type: string\n                          type: object\n                      type: object\n                  required:\n                  - action\n                  type: object\n                type: array\n              order:\n                description: Order is an optional field that specifies the order in\n                  which the policy is applied. Policies with higher \"order\" are applied\n                  after those with lower order.  If the order is omitted, it may be\n                  considered to be \"infinite\" - i.e. the policy will be applied last.  Policies\n                  with identical order will be applied in alphanumerical order based\n                  on the Policy \"Name\".\n                type: number\n              selector:\n                description: \"The selector is an expression used to pick pick out\n                  the endpoints that the policy should be applied to. \\n Selector\n                  expressions follow this syntax: \\n \\tlabel == \\\"string_literal\\\"\n                  \\ -\u003e  comparison, e.g. my_label == \\\"foo bar\\\" \\tlabel != \\\"string_literal\\\"\n                  \\  -\u003e  not equal; also matches if label is not present \\tlabel in\n                  { \\\"a\\\", \\\"b\\\", \\\"c\\\", ... }  -\u003e  true if the value of label X is\n                  one of \\\"a\\\", \\\"b\\\", \\\"c\\\" \\tlabel not in { \\\"a\\\", \\\"b\\\", \\\"c\\\",\n                  ... }  -\u003e  true if the value of label X is not one of \\\"a\\\", \\\"b\\\",\n                  \\\"c\\\" \\thas(label_name)  -\u003e True if that label is present \\t! expr\n                  -\u003e negation of expr \\texpr \u0026\u0026 expr  -\u003e Short-circuit and \\texpr\n                  || expr  -\u003e Short-circuit or \\t( expr ) -\u003e parens for grouping \\tall()\n                  or the empty selector -\u003e matches all endpoints. \\n Label names are\n                  allowed to contain alphanumerics, -, _ and /. String literals are\n                  more permissive but they do not support escape characters. \\n Examples\n                  (with made-up labels): \\n \\ttype == \\\"webserver\\\" \u0026\u0026 deployment\n                  == \\\"prod\\\" \\ttype in {\\\"frontend\\\", \\\"backend\\\"} \\tdeployment !=\n                  \\\"dev\\\" \\t! has(label_name)\"\n                type: string\n              serviceAccountSelector:\n                description: ServiceAccountSelector is an optional field for an expression\n                  used to select a pod based on service accounts.\n                type: string\n              types:\n                description: \"Types indicates whether this policy applies to ingress,\n                  or to egress, or to both.  When not explicitly specified (and so\n                  the value on creation is empty or nil), Calico defaults Types according\n                  to what Ingress and Egress are present in the policy.  The default\n                  is: \\n - [ PolicyTypeIngress ], if there are no Egress rules (including\n                  the case where there are   also no Ingress rules) \\n - [ PolicyTypeEgress\n                  ], if there are Egress rules but no Ingress rules \\n - [ PolicyTypeIngress,\n                  PolicyTypeEgress ], if there are both Ingress and Egress rules.\n                  \\n When the policy is read back again, Types will always be one\n                  of these values, never empty or nil.\"\n                items:\n                  description: PolicyType enumerates the possible values of the PolicySpec\n                    Types field.\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  annotations:\n    controller-gen.kubebuilder.io/version: (devel)\n  creationTimestamp: null\n  name: networksets.crd.projectcalico.org\nspec:\n  group: crd.projectcalico.org\n  names:\n    kind: NetworkSet\n    listKind: NetworkSetList\n    plural: networksets\n    singular: networkset\n  scope: Namespaced\n  versions:\n  - name: v1\n    schema:\n      openAPIV3Schema:\n        description: NetworkSet is the Namespaced-equivalent of the GlobalNetworkSet.\n        properties:\n          apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n              of an object. Servers should convert recognized schemas to the latest\n              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'\n            type: string\n          kind:\n            description: 'Kind is a string value representing the REST resource this\n              object represents. Servers may infer this from the endpoint the client\n              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'\n            type: string\n          metadata:\n            type: object\n          spec:\n            description: NetworkSetSpec contains the specification for a NetworkSet\n              resource.\n            properties:\n              nets:\n                description: The list of IP networks that belong to this set.\n                items:\n                  type: string\n                type: array\n            type: object\n        type: object\n    served: true\n    storage: true\nstatus:\n  acceptedNames:\n    kind: \"\"\n    plural: \"\"\n  conditions: []\n  storedVersions: []\n---\n---\n# Source: calico/templates/calico-kube-controllers-rbac.yaml\n{{if eq .RBACConfig \"rbac\"}}\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Nodes are watched to monitor for deletions.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - watch\n      - list\n      - get\n  # Pods are queried to check for existence.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  # IPAM resources are manipulated when nodes are deleted.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n    verbs:\n      - list\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - blockaffinities\n      - ipamblocks\n      - ipamhandles\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # kube-controllers manages hostendpoints.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - hostendpoints\n    verbs:\n      - get\n      - list\n      - create\n      - update\n      - delete\n  # Needs access to update clusterinformations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - clusterinformations\n    verbs:\n      - get\n      - create\n      - update\n  # KubeControllersConfiguration is where it gets its config\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - kubecontrollersconfigurations\n    verbs:\n      # read its own config\n      - get\n      # create a default if none exists\n      - create\n      # update status\n      - update\n      # watch for changes\n      - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n---\n---\n# Source: calico/templates/calico-node-rbac.yaml\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n      # Used to discover Typhas.\n      - get\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [\"\"]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n      # Calico stores some configuration information in node annotations.\n      - update\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [\"networking.k8s.io\"]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n  # Used by Calico for policy information.\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - list\n      - watch\n  # The CNI plugin patches pods/status.\n  - apiGroups: [\"\"]\n    resources:\n      - pods/status\n    verbs:\n      - patch\n  # Calico monitors various CRDs for config.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - globalfelixconfigs\n      - felixconfigurations\n      - bgppeers\n      - globalbgpconfigs\n      - bgpconfigurations\n      - ippools\n      - ipamblocks\n      - globalnetworkpolicies\n      - globalnetworksets\n      - networkpolicies\n      - networksets\n      - clusterinformations\n      - hostendpoints\n      - blockaffinities\n    verbs:\n      - get\n      - list\n      - watch\n  # Calico must create and update some CRDs on startup.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - ippools\n      - felixconfigurations\n      - clusterinformations\n    verbs:\n      - create\n      - update\n  # Calico stores some configuration information on the node.\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  # These permissions are only required for upgrade from v2.6, and can\n  # be removed after upgrade or on fresh installations.\n  - apiGroups: [\"crd.projectcalico.org\"]\n    resources:\n      - bgpconfigurations\n      - bgppeers\n    verbs:\n      - create\n      - update\n---\n# Flannel ClusterRole\n# Pulled from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: [\"\"]\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups: [\"\"]\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups: [\"\"]\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\n# Bind the flannel ClusterRole to the canal ServiceAccount.\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: canal-flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: canal-calico\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: canal\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-node.yaml\n# This manifest installs the canal container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: canal\n  namespace: kube-system\n  labels:\n    k8s-app: canal\nspec:\n  selector:\n    matchLabels:\n      k8s-app: canal\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  template:\n    metadata:\n      labels:\n        k8s-app: canal\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n        # Make sure canal gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: canal\n      {{end}}\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: {{.CNIImage}}\n          command: [\"/opt/cni/bin/install\"]\n          envFrom:\n          - configMapRef:\n              # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.\n              name: kubernetes-services-endpoint\n              optional: true\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: \"10-canal.conflist\"\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: cni_network_config\n            # Set the hostname based on the k8s node name.\n            - name: KUBERNETES_NODE_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: \"false\"\n          volumeMounts:\n            - mountPath: /host/opt/cni/bin\n              name: cni-bin-dir\n            - mountPath: /host/etc/cni/net.d\n              name: cni-net-dir\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: {{.FlexVolImg}}\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: /host/driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs canal container on each Kubernetes node. This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: {{.NodeImage}}\n          envFrom:\n          - configMapRef:\n              # Allow KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT to be overridden for eBPF mode.\n              name: kubernetes-services-endpoint\n              optional: true\n          env:\n            # Use Kubernetes API as the backing datastore.\n            - name: DATASTORE_TYPE\n              value: \"kubernetes\"\n            # Configure route aggregation based on pod CIDR.\n            - name: USE_POD_CIDR\n              value: \"true\"\n            # Wait for the datastore.\n            - name: WAIT_FOR_DATASTORE\n              value: \"true\"\n            # Set based on the k8s node name.\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Don't enable BGP.\n            - name: CALICO_NETWORKING_BACKEND\n              value: \"none\"\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: \"k8s,canal\"\n            # Period, in seconds, at which felix re-applies all iptables state\n            - name: FELIX_IPTABLESREFRESHINTERVAL\n              value: \"60\"\n            # No IP address needed.\n            - name: IP\n              value: \"\"\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within `--cluster-cidr`.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: \"192.168.0.0/16\"\n            # Disable file logging so `kubectl logs` works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: \"true\"\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: \"ACCEPT\"\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: \"false\"\n            # Disable felix logging to file\n            - name: FELIX_LOGFILEPATH\n              value: \"none\"\n            # Disable felix logging for syslog\n            - name: FELIX_LOGSEVERITYSYS\n              value: \"\"\n            # Set Felix logging to \"info\"\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: \"info\"\n            - name: FELIX_HEALTHENABLED\n              value: \"true\"\n            # Rancher-specific: Set FELIX_IPTABLESBACKEND to auto for autodetection of nftables\n            - name: FELIX_IPTABLESBACKEND\n              value: \"auto\"\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - /bin/calico-node\n              - -felix-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            httpGet:\n              path: /readiness\n              port: 9099\n              host: localhost\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: /lib/modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: /run/xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: /var/run/calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: /var/lib/calico\n              name: var-lib-calico\n              readOnly: false\n            - name: policysync\n              mountPath: /var/run/nodeagent\n            # For eBPF mode, we need to be able to mount the BPF filesystem at /sys/fs/bpf so we mount in the\n            # parent directory.\n            - name: sysfs\n              mountPath: /sys/fs/\n              # Bidirectional means that, if we mount the BPF filesystem at /sys/fs/bpf it will propagate to the host.\n              # If the host is known to mount that filesystem already then Bidirectional can be omitted.\n              mountPropagation: Bidirectional\n        # This container runs flannel using the kube-subnet-mgr backend\n        # for allocating subnets.\n        - name: kube-flannel\n          image: {{.CanalFlannelImg}}\n          command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n          securityContext:\n            privileged: true\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: FLANNELD_IFACE\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: canal_iface\n            - name: FLANNELD_IP_MASQ\n              valueFrom:\n                configMapKeyRef:\n                  name: canal-config\n                  key: masquerade\n          volumeMounts:\n          - mountPath: /run/xtables.lock\n            name: xtables-lock\n            readOnly: false\n          - name: flannel-cfg\n            mountPath: /etc/kube-flannel/\n      volumes:\n        # Used by canal.\n        - name: lib-modules\n          hostPath:\n            path: /lib/modules\n        - name: var-run-calico\n          hostPath:\n            path: /var/run/calico\n        - name: var-lib-calico\n          hostPath:\n            path: /var/lib/calico\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n        - name: sysfs\n          hostPath:\n            path: /sys/fs/\n            type: DirectoryOrCreate\n        # Used by flannel.\n        - name: flannel-cfg\n          configMap:\n            name: canal-config\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: /opt/cni/bin\n        - name: cni-net-dir\n          hostPath:\n            path: /etc/cni/net.d\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: /var/run/nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n{{- if .FlexVolPluginDir }}\n            path: {{.FlexVolPluginDir}}\n{{- else }}\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds\n{{- end }}\n---\n{{if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: canal\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-kube-controllers.yaml\n# See https://github.com/projectcalico/kube-controllers\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n    spec:\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n        # Rancher-specific: Set tolerations on the calico-kube-controllers so as to let it run on all nodes.\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: calico-kube-controllers\n      {{end}}\n      priorityClassName: system-cluster-critical\n      containers:\n        - name: calico-kube-controllers\n          image: {{.ControllersImage}}\n          env:\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: node\n            - name: DATASTORE_TYPE\n              value: kubernetes\n          readinessProbe:\n            exec:\n              command:\n              - /usr/bin/check-status\n              - -r\n---\n{{if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n{{end}}\n---\n# Source: calico/templates/calico-etcd-secrets.yaml\n---\n# Source: calico/templates/calico-typha.yaml\n---\n# Source: calico/templates/configure-canal.yaml\n",
   "coredns-v1.16": "\n---\n{{- if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n{{- end }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health\n        ready\n        kubernetes {{.ClusterDomain}} {{ if .ReverseCIDRs }}{{ .ReverseCIDRs }}{{ else }}{{ \"in-addr.arpa ip6.arpa\" }}{{ end }} {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n\t{{- if .UpstreamNameservers }}\n        forward . {{range $i, $v := .UpstreamNameservers}}{{if $i}} {{end}}{{.}}{{end}}\n\t{{- else }}\n        forward . \"/etc/resolv.conf\"\n\t{{- end }}\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  strategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'\n    spec:\n      priorityClassName: system-cluster-critical\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns\n{{- end }}\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      containers:\n      - name: coredns\n        image: {{.CoreDNSImage}}\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8181\n            scheme: HTTP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  annotations:\n    prometheus.io/port: \"9153\"\n    prometheus.io/scrape: \"true\"\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: {{.ClusterDNSServer}}\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n  - name: metrics\n    port: 9153\n    protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: coredns-autoscaler\nspec:\n  selector:\n    matchLabels:\n      k8s-app: coredns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: coredns-autoscaler\n    spec:\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns-autoscaler\n{{- end }}\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: autoscaler\n        image: {{.CoreDNSAutoScalerImage}}\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=coredns-autoscaler\n          - --target=Deployment/coredns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n{{if .LinearAutoscalerParams}}\n          - --default-params={\"linear\":{{.LinearAutoscalerParams}}}\n{{else}}\n          - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1,\"preventSinglePointFailure\":true}}\n{{end}}\n          - --logtostderr=true\n          - --v=2\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\",\"apps\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: coredns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:coredns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}",
   "coredns-v1.17": "\n---\n{{- if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n{{- end }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n          lameduck 5s\n        }\n        ready\n        kubernetes {{.ClusterDomain}} {{ if .ReverseCIDRs }}{{ .ReverseCIDRs }}{{ else }}{{ \"in-addr.arpa ip6.arpa\" }}{{ end }} {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n\t{{- if .UpstreamNameservers }}\n        forward . {{range $i, $v := .UpstreamNameservers}}{{if $i}} {{end}}{{.}}{{end}}\n\t{{- else }}\n        forward . \"/etc/resolv.conf\"\n\t{{- end }}\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  replicas: 1\n  strategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'\n    spec:\n      priorityClassName: system-cluster-critical\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns\n{{- end }}\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            - labelSelector:\n                matchExpressions:\n                - key: k8s-app\n                  operator: In\n                  values: [\"kube-dns\"]\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: coredns\n        image: {{.CoreDNSImage}}\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8181\n            scheme: HTTP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  annotations:\n    prometheus.io/port: \"9153\"\n    prometheus.io/scrape: \"true\"\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: {{.ClusterDNSServer}}\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n  - name: metrics\n    port: 9153\n    protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: coredns-autoscaler\nspec:\n  selector:\n    matchLabels:\n      k8s-app: coredns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: coredns-autoscaler\n    spec:\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns-autoscaler\n{{- end }}\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: autoscaler\n        image: {{.CoreDNSAutoScalerImage}}\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=coredns-autoscaler\n          - --target=Deployment/coredns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n{{if .LinearAutoscalerParams}}\n          - --default-params={\"linear\":{{.LinearAutoscalerParams}}}\n{{else}}\n          - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1,\"preventSinglePointFailure\":true}}\n{{end}}\n          - --nodelabels=node-role.kubernetes.io/worker=true,beta.kubernetes.io/os=linux\n          - --logtostderr=true\n          - --v=2\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\",\"apps\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: coredns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:coredns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}",
   "coredns-v1.8": "\n---\n{{- if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      kubernetes.io/cluster-service: \"true\"\n      addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: system:coredns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - endpoints\n  - services\n  - pods\n  - namespaces\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  verbs:\n  - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: EnsureExists\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n- kind: ServiceAccount\n  name: coredns\n  namespace: kube-system\n{{- end }}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health\n        kubernetes {{.ClusterDomain}} {{ if .ReverseCIDRs }}{{ .ReverseCIDRs }}{{ else }}{{ \"in-addr.arpa ip6.arpa\" }}{{ end }} {\n          pods insecure\n          upstream\n          fallthrough in-addr.arpa ip6.arpa\n          ttl 30\n        }\n        prometheus :9153\n\t{{- if .UpstreamNameservers }}\n        forward . {{range $i, $v := .UpstreamNameservers}}{{if $i}} {{end}}{{.}}{{end}}\n\t{{- else }}\n        forward . \"/etc/resolv.conf\"\n\t{{- end }}\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  strategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'\n    spec:\n      priorityClassName: system-cluster-critical\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns\n{{- end }}\n      tolerations:\n        - key: \"CriticalAddonsOnly\"\n          operator: \"Exists\"\n        - effect: NoExecute\n          operator: Exists\n        - effect: NoSchedule\n          operator: Exists\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      containers:\n      - name: coredns\n        image: {{.CoreDNSImage}}\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n          readOnly: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  annotations:\n    prometheus.io/port: \"9153\"\n    prometheus.io/scrape: \"true\"\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"CoreDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: {{.ClusterDNSServer}}\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n  - name: metrics\n    port: 9153\n    protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: coredns-autoscaler\nspec:\n  selector:\n    matchLabels:\n      k8s-app: coredns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: coredns-autoscaler\n    spec:\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: coredns-autoscaler\n{{- end }}\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: autoscaler\n        image: {{.CoreDNSAutoScalerImage}}\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=coredns-autoscaler\n          - --target=Deployment/coredns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n{{if .LinearAutoscalerParams}}\n          - --default-params={\"linear\":{{.LinearAutoscalerParams}}}\n{{else}}\n          - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1}}\n{{end}}\n          - --logtostderr=true\n          - --v=2\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns-autoscaler\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:coredns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: coredns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:coredns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}",
   "flannel-v1.15": "\n{{- if eq .RBACConfig \"rbac\"}}\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: ['extensions']\n    resources: ['podsecuritypolicies']\n    verbs: ['use']\n    resourceNames: ['psp.flannel.unprivileged']\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n{{end}}\n---\napiVersion: extensions/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configMap\n    - secret\n    - emptyDir\n    - hostPath\n  allowedHostPaths:\n    - pathPrefix: \"/etc/cni/net.d\"\n    - pathPrefix: \"/etc/kube-flannel\"\n    - pathPrefix: \"/run/flannel\"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: ['NET_ADMIN']\n  defaultAddCapabilities: []\n  requiredDropCapabilities: []\n  # Host namespaces\n  hostPID: false\n  hostIPC: false\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  # SELinux\n  seLinux:\n    # SELinux is unsed in CaaSP\n    rule: 'RunAsAny'\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"cniVersion\":\"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\",\n        \"VNI\": {{.FlannelBackend.VNI}},\n        \"Port\": {{.FlannelBackend.Port}}\n      }\n    }\n---\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel\n  namespace: kube-system\n  labels:\n    tier: node\n    k8s-app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        k8s-app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n      - operator: Exists\n      {{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: flannel\n      {{end}}\n      containers:\n      - name: kube-flannel\n        image: {{.Image}}\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        {{- if .FlannelInterface}}\n        - --iface={{.FlannelInterface}}\n        {{end}}\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: [\"NET_ADMIN\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      - name: install-cni\n        image: {{.CNIImage}}\n        command: [\"/install-cni.sh\"]\n        env:\n        # The CNI network config to install on each node.\n        - name: CNI_NETWORK_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: kube-flannel-cfg\n              key: cni-conf.json\n        - name: CNI_CONF_NAME\n          value: \"10-flannel.conflist\"\n        volumeMounts:\n        - name: cni\n          mountPath: /host/etc/cni/net.d\n        - name: host-cni-bin\n          mountPath: /host/opt/cni/bin/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n        - name: host-cni-bin\n          hostPath:\n            path: /opt/cni/bin\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 20%\n{{end}}\n",
   "flannel-v1.16": "\n{{- if eq .RBACConfig \"rbac\"}}\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups: ['extensions']\n    resources: ['podsecuritypolicies']\n    verbs: ['use']\n    resourceNames: ['psp.flannel.unprivileged']\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n{{end}}\n---\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: psp.flannel.unprivileged\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default\n    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default\nspec:\n  privileged: false\n  volumes:\n    - configMap\n    - secret\n    - emptyDir\n    - hostPath\n  allowedHostPaths:\n    - pathPrefix: \"/etc/cni/net.d\"\n    - pathPrefix: \"/etc/kube-flannel\"\n    - pathPrefix: \"/run/flannel\"\n  readOnlyRootFilesystem: false\n  # Users and groups\n  runAsUser:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  fsGroup:\n    rule: RunAsAny\n  # Privilege Escalation\n  allowPrivilegeEscalation: false\n  defaultAllowPrivilegeEscalation: false\n  # Capabilities\n  allowedCapabilities: ['NET_ADMIN']\n  defaultAddCapabilities: []\n  requiredDropCapabilities: []\n  # Host namespaces\n  hostPID: false\n  hostIPC: false\n  hostNetwork: true\n  hostPorts:\n  - min: 0\n    max: 65535\n  # SELinux\n  seLinux:\n    # SELinux is unsed in CaaSP\n    rule: 'RunAsAny'\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"cniVersion\":\"0.3.1\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\",\n        \"VNI\": {{.FlannelBackend.VNI}},\n        \"Port\": {{.FlannelBackend.Port}}\n      }\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel\n  namespace: kube-system\n  labels:\n    tier: node\n    k8s-app: flannel\nspec:\n  selector:\n    matchLabels:\n      k8s-app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        k8s-app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      tolerations:\n      - operator: Exists\n      {{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: flannel\n      {{end}}\n      containers:\n      - name: kube-flannel\n        image: {{.Image}}\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        {{- if .FlannelInterface}}\n        - --iface={{.FlannelInterface}}\n        {{end}}\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: false\n          capabilities:\n             add: [\"NET_ADMIN\"]\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      - name: install-cni\n        image: {{.CNIImage}}\n        command: [\"/install-cni.sh\"]\n        env:\n        # The CNI network config to install on each node.\n        - name: CNI_NETWORK_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: kube-flannel-cfg\n              key: cni-conf.json\n        - name: CNI_CONF_NAME\n          value: \"10-flannel.conflist\"\n        volumeMounts:\n        - name: cni\n          mountPath: /host/etc/cni/net.d\n        - name: host-cni-bin\n          mountPath: /host/opt/cni/bin/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n        - name: host-cni-bin\n          hostPath:\n            path: /opt/cni/bin\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 20%\n{{end}}\n",
   "flannel-v1.8": "\n{{- if eq .RBACConfig \"rbac\"}}\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n{{- end}}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: \"kube-system\"\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\":\"cbr0\",\n      \"cniVersion\":\"0.3.1\",\n      \"plugins\":[\n        {\n          \"type\":\"flannel\",\n          \"delegate\":{\n            \"forceAddress\":true,\n            \"isDefaultGateway\":true\n          }\n        },\n        {\n          \"type\":\"portmap\",\n          \"capabilities\":{\n            \"portMappings\":true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"{{.ClusterCIDR}}\",\n      \"Backend\": {\n        \"Type\": \"{{.FlannelBackend.Type}}\",\n        \"VNI\": {{.FlannelBackend.VNI}},\n        \"Port\": {{.FlannelBackend.Port}}\n      }\n    }\n---\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel\n  namespace: \"kube-system\"\n  labels:\n    tier: node\n    k8s-app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        k8s-app: flannel\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      serviceAccountName: flannel\n      containers:\n      - name: kube-flannel\n        image: {{.Image}}\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500M\n          requests:\n            cpu: 150m\n            memory: 64M\n        {{- if .FlannelInterface}}\n        command: [\"/opt/bin/flanneld\",\"--ip-masq\",\"--kube-subnet-mgr\",\"--iface={{.FlannelInterface}}\"]\n        {{- else}}\n        command: [\"/opt/bin/flanneld\",\"--ip-masq\",\"--kube-subnet-mgr\"]\n        {{- end}}\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      - name: install-cni\n        image: {{.CNIImage}}\n        command: [\"/install-cni.sh\"]\n        env:\n        # The CNI network config to install on each node.\n        - name: CNI_NETWORK_CONFIG\n          valueFrom:\n            configMapKeyRef:\n              name: kube-flannel-cfg\n              key: cni-conf.json\n        - name: CNI_CONF_NAME\n          value: \"10-flannel.conflist\"\n        volumeMounts:\n        - name: cni\n          mountPath: /host/etc/cni/net.d\n        - name: host-cni-bin\n          mountPath: /host/opt/cni/bin/\n      hostNetwork: true\n      tolerations:\n      {{- if ge .ClusterVersion \"v1.12\" }}\n      - operator: Exists\n        effect: NoSchedule\n      - operator: Exists\n        effect: NoExecute\n      {{- else }}\n      - key: node-role.kubernetes.io/controlplane\n        operator: Exists\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/etcd\n        operator: Exists\n        effect: NoExecute\n      {{- end }}\n      - key: node.kubernetes.io/not-ready\n        effect: NoSchedule\n        operator: Exists\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n        - name: host-cni-bin\n          hostPath:\n            path: /opt/cni/bin\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 20%\n{{end}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n",
   "kubedns-v1.16": "\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns-autoscaler\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-dns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns-autoscaler\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      serviceAccountName: kube-dns-autoscaler\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: autoscaler\n        image: {{.KubeDNSAutoScalerImage}}\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=kube-dns-autoscaler\n          - --target=Deployment/kube-dns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n{{if .LinearAutoscalerParams}}\n          - --default-params={\"linear\":{{.LinearAutoscalerParams}}}\n{{else}}\n          - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1,\"preventSinglePointFailure\":true}}\n{{end}}\n          - --logtostderr=true\n          - --v=2\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n{{- if eq .RBACConfig \"rbac\"}}\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\",\"apps\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: kube-dns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-dns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  # replicas: not specified here:\n  # 1. In order to make Addon Manager do not reconcile this replicas parameter.\n  # 2. Default is 1.\n  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.\n  strategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    rollingUpdate:\n      maxSurge: 10%\n      maxUnavailable: 0\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: k8s-app\n                    operator: In\n                    values: [\"kube-dns\"]\n              topologyKey: kubernetes.io/hostname\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: kube-dns-config\n        configMap:\n          name: kube-dns\n          optional: true\n      containers:\n      - name: kubedns\n        image: {{.KubeDNSImage}}\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        livenessProbe:\n          httpGet:\n            path: /healthcheck/kubedns\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8081\n            scheme: HTTP\n          # we poll on pod startup for the Kubernetes master service and\n          # only setup the /readiness HTTP server once that's available.\n          initialDelaySeconds: 3\n          timeoutSeconds: 5\n        args:\n        - --domain={{.ClusterDomain}}.\n        - --dns-port=10053\n        - --config-dir=/kube-dns-config\n        - --v=2\n        env:\n        - name: PROMETHEUS_PORT\n          value: \"10055\"\n        ports:\n        - containerPort: 10053\n          name: dns-local\n          protocol: UDP\n        - containerPort: 10053\n          name: dns-tcp-local\n          protocol: TCP\n        - containerPort: 10055\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: /kube-dns-config\n      - name: dnsmasq\n        image: {{.DNSMasqImage}}\n        livenessProbe:\n          httpGet:\n            path: /healthcheck/dnsmasq\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - -v=2\n        - -logtostderr\n        - -configDir=/etc/k8s/dns/dnsmasq-nanny\n        - -restartDnsmasq=true\n        - --\n        - -k\n        - --cache-size=1000\n        - --log-facility=-\n        - --server=/{{.ClusterDomain}}/127.0.0.1#10053\n\t{{- if .ReverseCIDRs }}\n\t{{- range .ReverseCIDRs }}\n        - --server=/{{.}}/127.0.0.1#10053\n\t{{- end }}\n\t{{- else }}\n        - --server=/in-addr.arpa/127.0.0.1#10053\n        - --server=/ip6.arpa/127.0.0.1#10053\n\t{{- end }}\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details\n        resources:\n          requests:\n            cpu: 150m\n            memory: 20Mi\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: /etc/k8s/dns/dnsmasq-nanny\n      - name: sidecar\n        image: {{.KubeDNSSidecarImage}}\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --v=2\n        - --logtostderr\n        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.{{.ClusterDomain}},5,A\n        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.{{.ClusterDomain}},5,A\n        ports:\n        - containerPort: 10054\n          name: metrics\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n      dnsPolicy: Default  # Don't use cluster DNS.\n      serviceAccountName: kube-dns\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"KubeDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: {{.ClusterDNSServer}}\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dns\n  namespace: kube-system\ndata:\n{{- if .UpstreamNameservers }}\n  upstreamNameservers: |\n    [{{range $i, $v := .UpstreamNameservers}}{{if $i}}, {{end}}{{printf \"%q\" .}}{{end}}]\n{{- end }}\n{{- if .StubDomains }}\n  stubDomains: |\n    {{ GetKubednsStubDomains .StubDomains }}\n{{- end }}",
   "kubedns-v1.8": "\n---\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns-autoscaler\nspec:\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns-autoscaler\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      serviceAccountName: kube-dns-autoscaler\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: autoscaler\n        image: {{.KubeDNSAutoScalerImage}}\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - /cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=kube-dns-autoscaler\n          - --target=Deployment/kube-dns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n{{if .LinearAutoscalerParams}}\n          - --default-params={\"linear\":{{.LinearAutoscalerParams}}}\n{{else}}\n          - --default-params={\"linear\":{\"coresPerReplica\":128,\"nodesPerReplica\":4,\"min\":1}}\n{{end}}\n          - --logtostderr=true\n          - --v=2\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n{{- if eq .RBACConfig \"rbac\"}}\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"deployments/scale\", \"replicasets/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: system:kube-dns-autoscaler\nsubjects:\n  - kind: ServiceAccount\n    name: kube-dns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-dns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n{{- end }}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  # replicas: not specified here:\n  # 1. In order to make Addon Manager do not reconcile this replicas parameter.\n  # 2. Default is 1.\n  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.\n  strategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    rollingUpdate:\n      maxSurge: 10%\n      maxUnavailable: 0\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n    spec:\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                  - key: k8s-app\n                    operator: In\n                    values: [\"kube-dns\"]\n              topologyKey: kubernetes.io/hostname\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: kube-dns-config\n        configMap:\n          name: kube-dns\n          optional: true\n      containers:\n      - name: kubedns\n        image: {{.KubeDNSImage}}\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        livenessProbe:\n          httpGet:\n            path: /healthcheck/kubedns\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: 8081\n            scheme: HTTP\n          # we poll on pod startup for the Kubernetes master service and\n          # only setup the /readiness HTTP server once that's available.\n          initialDelaySeconds: 3\n          timeoutSeconds: 5\n        args:\n        - --domain={{.ClusterDomain}}.\n        - --dns-port=10053\n        - --config-dir=/kube-dns-config\n        - --v=2\n        env:\n        - name: PROMETHEUS_PORT\n          value: \"10055\"\n        ports:\n        - containerPort: 10053\n          name: dns-local\n          protocol: UDP\n        - containerPort: 10053\n          name: dns-tcp-local\n          protocol: TCP\n        - containerPort: 10055\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: /kube-dns-config\n      - name: dnsmasq\n        image: {{.DNSMasqImage}}\n        livenessProbe:\n          httpGet:\n            path: /healthcheck/dnsmasq\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - -v=2\n        - -logtostderr\n        - -configDir=/etc/k8s/dns/dnsmasq-nanny\n        - -restartDnsmasq=true\n        - --\n        - -k\n        - --cache-size=1000\n        - --log-facility=-\n        - --server=/{{.ClusterDomain}}/127.0.0.1#10053\n\t{{- if .ReverseCIDRs }}\n\t{{- range .ReverseCIDRs }}\n        - --server=/{{.}}/127.0.0.1#10053\n\t{{- end }}\n\t{{- else }}\n        - --server=/in-addr.arpa/127.0.0.1#10053\n        - --server=/ip6.arpa/127.0.0.1#10053\n\t{{- end }}\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        # see: https://github.com/kubernetes/kubernetes/issues/29055 for details\n        resources:\n          requests:\n            cpu: 150m\n            memory: 20Mi\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: /etc/k8s/dns/dnsmasq-nanny\n      - name: sidecar\n        image: {{.KubeDNSSidecarImage}}\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --v=2\n        - --logtostderr\n        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.{{.ClusterDomain}},5,A\n        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.{{.ClusterDomain}},5,A\n        ports:\n        - containerPort: 10054\n          name: metrics\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n      dnsPolicy: Default  # Don't use cluster DNS.\n      serviceAccountName: kube-dns\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"KubeDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: {{.ClusterDNSServer}}\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dns\n  namespace: kube-system\ndata:\n{{- if .UpstreamNameservers }}\n  upstreamNameservers: |\n    [{{range $i, $v := .UpstreamNameservers}}{{if $i}}, {{end}}{{printf \"%q\" .}}{{end}}]\n{{- end }}\n{{- if .StubDomains }}\n  stubDomains: |\n    {{ GetKubednsStubDomains .StubDomains }}\n{{- end }}",
   "metricsserver-v1.8": "\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: metrics-server:system:auth-delegator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: metrics-server-auth-reader\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: system:metrics-server\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - nodes\n  - nodes/stats\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"extensions\"\n  resources:\n  - deployments\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:metrics-server\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n- kind: ServiceAccount\n  name: metrics-server\n  namespace: kube-system\n{{- end }}\n---\napiVersion: apiregistration.k8s.io/v1beta1\nkind: APIService\nmetadata:\n  name: v1beta1.metrics.k8s.io\nspec:\n  service:\n    name: metrics-server\n    namespace: kube-system\n  group: metrics.k8s.io\n  version: v1beta1\n  insecureSkipTLSVerify: true\n  groupPriorityMinimum: 100\n  versionPriority: 100\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    k8s-app: metrics-server\nspec:\n{{if .Replicas}}\n  replicas: {{.Replicas}}\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n{{if .UpdateStrategy}}\n  strategy:\n{{ toYaml .UpdateStrategy | indent 4}}\n{{end}}\n  template:\n    metadata:\n      name: metrics-server\n      labels:\n        k8s-app: metrics-server\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      serviceAccountName: metrics-server\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: metrics-server\n        image: {{ .MetricsServerImage }}\n        imagePullPolicy: Always\n        command:\n        - /metrics-server\n        {{- if eq .Version \"v0.3\" }}\n        - --kubelet-insecure-tls\n        - --kubelet-preferred-address-types=InternalIP\n        - --logtostderr\n        {{- else }}\n        - --source=kubernetes.summary_api:https://kubernetes.default.svc?kubeletHttps=true\u0026kubeletPort=10250\u0026useServiceAccount=true\u0026insecure=true\n        {{- end }}\n        {{ range $k,$v := .Options }}\n        -  --{{ $k }}={{ $v }}\n        {{ end }}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: metrics-server\n  namespace: kube-system\n  labels:\n    kubernetes.io/name: \"Metrics-server\"\nspec:\n  selector:\n    k8s-app: metrics-server\n  ports:\n  - port: 443\n    protocol: TCP\n    targetPort: 443\n",
   "nginxingress-v1.15": "\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: nginx-configuration\n  namespace: ingress-nginx\n  labels:\n    app: ingress-nginx\ndata:\n{{ range $k,$v := .Options }}\n  {{ $k }}: \"{{ $v }}\"\n{{ end }}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: udp-services\n  namespace: ingress-nginx\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: nginx-ingress-clusterrole\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - endpoints\n      - nodes\n      - pods\n      - secrets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"extensions\"\n      - \"networking.k8s.io\"\n    resources:\n      - ingresses\n      - daemonsets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n        - events\n    verbs:\n        - create\n        - patch\n  - apiGroups:\n      - \"extensions\"\n      - \"networking.k8s.io\"\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: nginx-ingress-role\n  namespace: ingress-nginx\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - pods\n      - secrets\n      - namespaces\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      # Defaults to \"\u003celection-id\u003e-\u003cingress-class\u003e\"\n      # Here: \"\u003cingress-controller-leader\u003e-\u003cnginx\u003e\"\n      # This has to be adapted if you change either parameter\n      # when launching the nginx-ingress-controller.\n      - \"ingress-controller-leader-nginx\"\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: nginx-ingress-role-nisa-binding\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: nginx-ingress-role\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: nginx-ingress-clusterrole-nisa-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: nginx-ingress-clusterrole\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n{{ end }}\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  selector:\n    matchLabels:\n      app: ingress-nginx\n{{if .UpdateStrategy}}\n  updateStrategy:\n{{ toYaml .UpdateStrategy | indent 4}}\n{{end}}\n  template:\n    metadata:\n      labels:\n        app: ingress-nginx\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      hostNetwork: true\n      {{if .DNSPolicy}}\n      dnsPolicy: {{.DNSPolicy}}\n      {{end}}\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: nginx-ingress-serviceaccount\n      {{ end }}\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      {{- if ne .AlpineImage \"\"}}\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - sysctl -w net.core.somaxconn=32768; sysctl -w net.ipv4.ip_local_port_range=\"1024 65535\"\n        image: {{.AlpineImage}}\n        imagePullPolicy: IfNotPresent\n        name: sysctl\n        securityContext:\n          privileged: true\n      {{- end }}\n      containers:\n        - name: nginx-ingress-controller\n          image: {{.IngressImage}}\n          args:\n            - /nginx-ingress-controller\n            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n            - --configmap=$(POD_NAMESPACE)/nginx-configuration\n            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n            - --annotations-prefix=nginx.ingress.kubernetes.io\n          {{ range $k, $v := .ExtraArgs }}\n            - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}}\n          {{ end }}\n          {{- if eq .AlpineImage \"\"}}\n          securityContext:\n            capabilities:\n                drop:\n                - ALL\n                add:\n                - NET_BIND_SERVICE\n            runAsUser: 33\n          {{- end }}\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n{{if .ExtraEnvs}}\n{{ toYaml .ExtraEnvs | indent 12}}\n{{end}}\n          ports:\n          - name: http\n            containerPort: 80\n          - name: https\n            containerPort: 443\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n{{if .ExtraVolumeMounts}}\n          volumeMounts:\n{{ toYaml .ExtraVolumeMounts | indent 12}}\n{{end}}\n{{if .ExtraVolumes}}\n      volumes:\n{{ toYaml .ExtraVolumes | indent 8}}\n{{end}}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app: default-http-backend\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: default-http-backend\n  template:\n    metadata:\n      labels:\n        app: default-http-backend\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      terminationGracePeriodSeconds: 60\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: default-http-backend\n        # Any image is permissable as long as:\n        # 1. It serves a 404 page at /\n        # 2. It serves 200 on a /healthz endpoint\n        image: {{.IngressBackend}}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: ingress-nginx\n  labels:\n    app: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: default-http-backend\n",
   "nginxingress-v1.15.12": "\n# this template is intended for use by \u003e= nginx-0.32.0\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: nginx-configuration\n  namespace: ingress-nginx\n  labels:\n    app: ingress-nginx\ndata:\n{{ range $k,$v := .Options }}\n  {{ $k }}: \"{{ $v }}\"\n{{ end }}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: udp-services\n  namespace: ingress-nginx\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: nginx-ingress-clusterrole\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - endpoints\n      - nodes\n      - pods\n      - secrets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - update\n      - watch\n  - apiGroups:\n      - extensions\n      - \"networking.k8s.io\" # k8s 1.14+\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n    verbs:\n      - create\n      - patch\n  - apiGroups:\n      - extensions\n      - \"networking.k8s.io\" # k8s 1.14+\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: nginx-ingress-role\n  namespace: ingress-nginx\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - namespaces\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - pods\n      - secrets\n      - endpoints\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - update\n      - watch\n  - apiGroups:\n      - extensions\n      - \"networking.k8s.io\" # k8s 1.14+\n    resources:\n      - ingresses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n      - \"networking.k8s.io\" # k8s 1.14+\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      # Defaults to \"\u003celection-id\u003e-\u003cingress-class\u003e\"\n      # Here: \"\u003cingress-controller-leader\u003e-\u003cnginx\u003e\"\n      # This has to be adapted if you change either parameter\n      # when launching the nginx-ingress-controller.\n      - ingress-controller-leader-nginx\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n    verbs:\n      - create\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - events\n    verbs:\n      - create\n      - patch\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: nginx-ingress-role-nisa-binding\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: nginx-ingress-role\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: nginx-ingress-clusterrole-nisa-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: nginx-ingress-clusterrole\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n{{ end }}\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  selector:\n    matchLabels:\n      app: ingress-nginx\n{{if .UpdateStrategy}}\n  updateStrategy:\n{{ toYaml .UpdateStrategy | indent 4}}\n{{end}}\n  template:\n    metadata:\n      labels:\n        app: ingress-nginx\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      hostNetwork: true\n      {{if .DNSPolicy}}\n      dnsPolicy: {{.DNSPolicy}}\n      {{end}}\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: nginx-ingress-serviceaccount\n      {{ end }}\n      terminationGracePeriodSeconds: 60\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n        - name: nginx-ingress-controller\n          image: {{.IngressImage}}\n          args:\n            - /nginx-ingress-controller\n            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n            - --configmap=$(POD_NAMESPACE)/nginx-configuration\n            - --election-id=ingress-controller-leader\n            - --ingress-class=nginx\n            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n            - --annotations-prefix=nginx.ingress.kubernetes.io\n          {{ range $k, $v := .ExtraArgs }}\n            - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}}\n          {{ end }}\n          securityContext:\n            capabilities:\n                drop:\n                - ALL\n                add:\n                - NET_BIND_SERVICE\n            runAsUser: 101\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n{{if .ExtraEnvs}}\n{{ toYaml .ExtraEnvs | indent 12}}\n{{end}}\n          ports:\n          - name: http\n            containerPort: 80\n          - name: https\n            containerPort: 443\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n{{if .ExtraVolumeMounts}}\n          volumeMounts:\n{{ toYaml .ExtraVolumeMounts | indent 12}}\n{{end}}\n{{if .ExtraVolumes}}\n      volumes:\n{{ toYaml .ExtraVolumes | indent 8}}\n{{end}}\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app: default-http-backend\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: default-http-backend\n  template:\n    metadata:\n      labels:\n        app: default-http-backend\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      terminationGracePeriodSeconds: 60\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: default-http-backend\n        # Any image is permissable as long as:\n        # 1. It serves a 404 page at /\n        # 2. It serves 200 on a /healthz endpoint\n        image: {{.IngressBackend}}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: ingress-nginx\n  labels:\n    app: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: default-http-backend\n",
   "nginxingress-v1.8": "\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: nginx-configuration\n  namespace: ingress-nginx\n  labels:\n    app: ingress-nginx\ndata:\n{{ range $k,$v := .Options }}\n  {{ $k }}: \"{{ $v }}\"\n{{ end }}\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: udp-services\n  namespace: ingress-nginx\n{{if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nginx-ingress-serviceaccount\n  namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: nginx-ingress-clusterrole\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - endpoints\n      - nodes\n      - pods\n      - secrets\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - services\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"extensions\"\n    resources:\n      - ingresses\n      - daemonsets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n        - events\n    verbs:\n        - create\n        - patch\n  - apiGroups:\n      - \"extensions\"\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: nginx-ingress-role\n  namespace: ingress-nginx\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n      - pods\n      - secrets\n      - namespaces\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    resourceNames:\n      # Defaults to \"\u003celection-id\u003e-\u003cingress-class\u003e\"\n      # Here: \"\u003cingress-controller-leader\u003e-\u003cnginx\u003e\"\n      # This has to be adapted if you change either parameter\n      # when launching the nginx-ingress-controller.\n      - \"ingress-controller-leader-nginx\"\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - \"\"\n    resources:\n      - configmaps\n    verbs:\n      - create\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: nginx-ingress-role-nisa-binding\n  namespace: ingress-nginx\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: nginx-ingress-role\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: nginx-ingress-clusterrole-nisa-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: nginx-ingress-clusterrole\nsubjects:\n  - kind: ServiceAccount\n    name: nginx-ingress-serviceaccount\n    namespace: ingress-nginx\n{{ end }}\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ingress-controller\n  namespace: ingress-nginx\nspec:\n  selector:\n    matchLabels:\n      app: ingress-nginx\n{{if .UpdateStrategy}}\n  updateStrategy:\n{{ toYaml .UpdateStrategy | indent 4}}\n{{end}}\n  template:\n    metadata:\n      labels:\n        app: ingress-nginx\n      annotations:\n        prometheus.io/port: '10254'\n        prometheus.io/scrape: 'true'\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      hostNetwork: true\n      {{if .DNSPolicy}}\n      dnsPolicy: {{.DNSPolicy}}\n      {{end}}\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      {{if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: nginx-ingress-serviceaccount\n      {{ end }}\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      {{- if ne .AlpineImage \"\"}}\n      initContainers:\n      - command:\n        - sh\n        - -c\n        - sysctl -w net.core.somaxconn=32768; sysctl -w net.ipv4.ip_local_port_range=\"1024 65535\"\n        image: {{.AlpineImage}}\n        imagePullPolicy: IfNotPresent\n        name: sysctl\n        securityContext:\n          privileged: true\n      {{- end }}\n      containers:\n        - name: nginx-ingress-controller\n          image: {{.IngressImage}}\n          args:\n            - /nginx-ingress-controller\n            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n            - --configmap=$(POD_NAMESPACE)/nginx-configuration\n            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services\n            - --annotations-prefix=nginx.ingress.kubernetes.io\n          {{ range $k, $v := .ExtraArgs }}\n            - --{{ $k }}{{if ne $v \"\" }}={{ $v }}{{end}}\n          {{ end }}\n          {{- if eq .AlpineImage \"\"}}\n          securityContext:\n            capabilities:\n                drop:\n                - ALL\n                add:\n                - NET_BIND_SERVICE\n            runAsUser: 33\n          {{- end }}\n          env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n{{if .ExtraEnvs}}\n{{ toYaml .ExtraEnvs | indent 12}}\n{{end}}\n          ports:\n          - name: http\n            containerPort: 80\n          - name: https\n            containerPort: 443\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /healthz\n              port: 10254\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n{{if .ExtraVolumeMounts}}\n          volumeMounts:\n{{ toYaml .ExtraVolumeMounts | indent 12}}\n{{end}}\n{{if .ExtraVolumes}}\n      volumes:\n{{ toYaml .ExtraVolumes | indent 8}}\n{{end}}\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app: default-http-backend\n  namespace: ingress-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: default-http-backend\n  template:\n    metadata:\n      labels:\n        app: default-http-backend\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n                - key: node-role.kubernetes.io/worker\n                  operator: Exists\n      terminationGracePeriodSeconds: 60\n      tolerations:\n      - effect: NoExecute\n        operator: Exists\n      - effect: NoSchedule\n        operator: Exists\n      containers:\n      - name: default-http-backend\n        # Any image is permissable as long as:\n        # 1. It serves a 404 page at /\n        # 2. It serves 200 on a /healthz endpoint\n        image: {{.IngressBackend}}\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: ingress-nginx\n  labels:\n    app: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: default-http-backend\n",
   "nodelocal-v1.15": "\n{{- if eq .RBACConfig \"rbac\"}}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: node-local-dns\n  namespace: kube-system\n  labels:\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n{{- end }}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns-upstream\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"KubeDNSUpstream\"\nspec:\n  ports:\n  - name: dns\n    port: 53\n    protocol: UDP\n    targetPort: 53\n  - name: dns-tcp\n    port: 53\n    protocol: TCP\n    targetPort: 53\n  selector:\n    k8s-app: kube-dns\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: node-local-dns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\ndata:\n  Corefile: |\n    {{.ClusterDomain}}:53 {\n        errors\n        cache {\n                success 9984 30\n                denial 9984 5\n        }\n        reload\n        loop\n        bind {{.IPAddress}} {{.ClusterDNSServer}}\n        forward . __PILLAR__CLUSTER__DNS__ {\n                force_tcp\n        }\n        prometheus :9253\n        health {{.IPAddress}}:8080\n        }\n    in-addr.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind {{.IPAddress}} {{.ClusterDNSServer}}\n        forward . __PILLAR__CLUSTER__DNS__ {\n                force_tcp\n        }\n        prometheus :9253\n        }\n    ip6.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind {{.IPAddress}} {{.ClusterDNSServer}}\n        forward . __PILLAR__CLUSTER__DNS__ {\n                force_tcp\n        }\n        prometheus :9253\n        }\n    .:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind {{.IPAddress}} {{.ClusterDNSServer}}\n        forward . __PILLAR__UPSTREAM__SERVERS__ {\n                force_tcp\n        }\n        prometheus :9253\n        }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-local-dns\n  namespace: kube-system\n  labels:\n    k8s-app: node-local-dns\n    kubernetes.io/cluster-service: \"true\"\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 4}}\n{{else}}\n    rollingUpdate:\n      maxUnavailable: 1\n{{end}}\n  selector:\n    matchLabels:\n      k8s-app: node-local-dns\n  template:\n    metadata:\n       labels:\n          k8s-app: node-local-dns\n    spec:\n      priorityClassName: system-node-critical\n{{- if eq .RBACConfig \"rbac\"}}\n      serviceAccountName: node-local-dns\n{{- end }}\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n              - matchExpressions:\n                - key: beta.kubernetes.io/os\n                  operator: NotIn\n                  values:\n                    - windows\n      hostNetwork: true\n{{if .NodeSelector}}\n      nodeSelector:\n      {{ range $k, $v := .NodeSelector }}\n        {{ $k }}: \"{{ $v }}\"\n      {{ end }}\n{{end}}\n      dnsPolicy: Default  # Don't use cluster DNS.\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-cache\n        image: {{.NodelocalImage}}\n        resources:\n          requests:\n            cpu: 25m\n            memory: 5Mi\n        args: [ \"-localip\", \"{{.IPAddress}},{{.ClusterDNSServer}}\", \"-conf\", \"/etc/Corefile\", \"-upstreamsvc\", \"kube-dns-upstream\" ]\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9253\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: {{.IPAddress}}\n            path: /health\n            port: 8080\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /run/xtables.lock\n          name: xtables-lock\n          readOnly: false\n        - name: config-volume\n          mountPath: /etc/coredns\n        - name: kube-dns-config\n          mountPath: /etc/kube-dns\n      volumes:\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: kube-dns-config\n        configMap:\n          name: kube-dns\n          optional: true\n      - name: config-volume\n        configMap:\n          name: node-local-dns\n          items:\n            - key: Corefile\n              path: Corefile.base\n",
   "weave-v1.16": "\n---\n# This ConfigMap can be used to configure a self-hosted Weave Net installation.\napiVersion: v1\nkind: List\nitems:\n  - apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: weave-net\n      namespace: kube-system\n  - apiVersion: apps/v1\n    kind: DaemonSet\n    metadata:\n      name: weave-net\n      labels:\n        name: weave-net\n      namespace: kube-system\n    spec:\n      selector:\n        matchLabels:\n          name: weave-net\n      template:\n        metadata:\n          annotations:\n            scheduler.alpha.kubernetes.io/critical-pod: ''\n            scheduler.alpha.kubernetes.io/tolerations: \u003e-\n              [{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]\n          labels:\n            name: weave-net\n        spec:\n          affinity:\n            nodeAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                  - matchExpressions:\n                    - key: beta.kubernetes.io/os\n                      operator: NotIn\n                      values:\n                        - windows\n{{if .NodeSelector}}\n          nodeSelector:\n            {{ range $k, $v := .NodeSelector }}\n              {{ $k }}: \"{{ $v }}\"\n            {{ end }}\n{{end}}\n          containers:\n            - name: weave\n              command:\n                - /home/weave/launch.sh\n              env:\n                - name: HOSTNAME\n                  valueFrom:\n                    fieldRef:\n                      apiVersion: v1\n                      fieldPath: spec.nodeName\n                - name: IPALLOC_RANGE\n                  value: \"{{.ClusterCIDR}}\"\n                {{- if .WeavePassword}}\n                - name: WEAVE_PASSWORD\n                  value: \"{{.WeavePassword}}\"\n                {{- end}}\n                {{- if .MTU }}\n                {{- if ne .MTU 0 }}\n                - name: WEAVE_MTU\n                  value: \"{{.MTU}}\"\n                {{- end }}\n                {{- end }}\n              image: {{.Image}}\n              readinessProbe:\n                httpGet:\n                  host: 127.0.0.1\n                  path: /status\n                  port: 6784\n                initialDelaySeconds: 30\n              resources:\n                requests:\n                  cpu: 10m\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: weavedb\n                  mountPath: /weavedb\n                - name: cni-bin\n                  mountPath: /host/opt\n                - name: cni-bin2\n                  mountPath: /host/home\n                - name: cni-conf\n                  mountPath: /host/etc\n                - name: dbus\n                  mountPath: /host/var/lib/dbus\n                - name: lib-modules\n                  mountPath: /lib/modules\n                - name: xtables-lock\n                  mountPath: /run/xtables.lock\n            - name: weave-npc\n              env:\n                - name: HOSTNAME\n                  valueFrom:\n                    fieldRef:\n                      apiVersion: v1\n                      fieldPath: spec.nodeName\n              image: {{.CNIImage}}\n              resources:\n                requests:\n                  cpu: 10m\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: xtables-lock\n                  mountPath: /run/xtables.lock\n            - name: weave-plugins\n              command:\n                - /opt/rke-tools/weave-plugins-cni.sh\n              image: {{.WeaveLoopbackImage}}\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: cni-bin\n                  mountPath: /opt\n          hostNetwork: true\n          hostPID: true\n          restartPolicy: Always\n          securityContext:\n            seLinuxOptions: {}\n          serviceAccountName: weave-net\n          tolerations:\n          - operator: Exists\n            effect: NoSchedule\n          - operator: Exists\n            effect: NoExecute\n          volumes:\n            - name: weavedb\n              hostPath:\n                path: /var/lib/weave\n            - name: cni-bin\n              hostPath:\n                path: /opt\n            - name: cni-bin2\n              hostPath:\n                path: /home\n            - name: cni-conf\n              hostPath:\n                path: /etc\n            - name: dbus\n              hostPath:\n                path: /var/lib/dbus\n            - name: lib-modules\n              hostPath:\n                path: /lib/modules\n            - name: xtables-lock\n              hostPath:\n                path: /run/xtables.lock\n      updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 8}}\n{{end}}\n        type: RollingUpdate\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\nrules:\n  - apiGroups:\n      - ''\n    resources:\n      - pods\n      - namespaces\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - networkpolicies\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - ''\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\nroleRef:\n  kind: ClusterRole\n  name: weave-net\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n  - kind: ServiceAccount\n    name: weave-net\n    namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\nrules:\n  - apiGroups:\n      - ''\n    resourceNames:\n      - weave-net\n    resources:\n      - configmaps\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - ''\n    resources:\n      - configmaps\n    verbs:\n      - create\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\nroleRef:\n  kind: Role\n  name: weave-net\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n  - kind: ServiceAccount\n    name: weave-net\n    namespace: kube-system\n{{- end}}\n",
   "weave-v1.8": "\n---\n# This ConfigMap can be used to configure a self-hosted Weave Net installation.\napiVersion: v1\nkind: List\nitems:\n  - apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: weave-net\n      namespace: kube-system\n  - apiVersion: extensions/v1beta1\n    kind: DaemonSet\n    metadata:\n      name: weave-net\n      labels:\n        name: weave-net\n      namespace: kube-system\n    spec:\n      template:\n        metadata:\n          annotations:\n            scheduler.alpha.kubernetes.io/critical-pod: ''\n            scheduler.alpha.kubernetes.io/tolerations: \u003e-\n              [{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]\n          labels:\n            name: weave-net\n        spec:\n          affinity:\n            nodeAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                nodeSelectorTerms:\n                  - matchExpressions:\n                    - key: beta.kubernetes.io/os\n                      operator: NotIn\n                      values:\n                        - windows\n{{if .NodeSelector}}\n          nodeSelector:\n            {{ range $k, $v := .NodeSelector }}\n              {{ $k }}: \"{{ $v }}\"\n            {{ end }}\n{{end}}\n          containers:\n            - name: weave\n              command:\n                - /home/weave/launch.sh\n              env:\n                - name: HOSTNAME\n                  valueFrom:\n                    fieldRef:\n                      apiVersion: v1\n                      fieldPath: spec.nodeName\n                - name: IPALLOC_RANGE\n                  value: \"{{.ClusterCIDR}}\"\n                {{- if .WeavePassword}}\n                - name: WEAVE_PASSWORD\n                  value: \"{{.WeavePassword}}\"\n                {{- end}}\n                {{- if .MTU }}\n                {{- if ne .MTU 0 }}\n                - name: WEAVE_MTU\n                  value: \"{{.MTU}}\"\n                {{- end }}\n                {{- end }}\n              image: {{.Image}}\n              readinessProbe:\n                httpGet:\n                  host: 127.0.0.1\n                  path: /status\n                  port: 6784\n                initialDelaySeconds: 30\n              resources:\n                requests:\n                  cpu: 10m\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: weavedb\n                  mountPath: /weavedb\n                - name: cni-bin\n                  mountPath: /host/opt\n                - name: cni-bin2\n                  mountPath: /host/home\n                - name: cni-conf\n                  mountPath: /host/etc\n                - name: dbus\n                  mountPath: /host/var/lib/dbus\n                - name: lib-modules\n                  mountPath: /lib/modules\n                - name: xtables-lock\n                  mountPath: /run/xtables.lock\n            - name: weave-npc\n              env:\n                - name: HOSTNAME\n                  valueFrom:\n                    fieldRef:\n                      apiVersion: v1\n                      fieldPath: spec.nodeName\n              image: {{.CNIImage}}\n              resources:\n                requests:\n                  cpu: 10m\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: xtables-lock\n                  mountPath: /run/xtables.lock\n            - name: weave-plugins\n              command:\n                - /opt/rke-tools/weave-plugins-cni.sh\n              image: {{.WeaveLoopbackImage}}\n              securityContext:\n                privileged: true\n              volumeMounts:\n                - name: cni-bin\n                  mountPath: /opt\n          hostNetwork: true\n          hostPID: true\n          restartPolicy: Always\n          securityContext:\n            seLinuxOptions: {}\n          serviceAccountName: weave-net\n          tolerations:\n          - operator: Exists\n            effect: NoSchedule\n          - operator: Exists\n            effect: NoExecute\n          volumes:\n            - name: weavedb\n              hostPath:\n                path: /var/lib/weave\n            - name: cni-bin\n              hostPath:\n                path: /opt\n            - name: cni-bin2\n              hostPath:\n                path: /home\n            - name: cni-conf\n              hostPath:\n                path: /etc\n            - name: dbus\n              hostPath:\n                path: /var/lib/dbus\n            - name: lib-modules\n              hostPath:\n                path: /lib/modules\n            - name: xtables-lock\n              hostPath:\n                path: /run/xtables.lock\n      updateStrategy:\n{{if .UpdateStrategy}}\n{{ toYaml .UpdateStrategy | indent 8}}\n{{end}}\n        type: RollingUpdate\n{{- if eq .RBACConfig \"rbac\"}}\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\nrules:\n  - apiGroups:\n      - ''\n    resources:\n      - pods\n      - namespaces\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - networking.k8s.io\n    resources:\n      - networkpolicies\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - ''\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\nroleRef:\n  kind: ClusterRole\n  name: weave-net\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n  - kind: ServiceAccount\n    name: weave-net\n    namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\nrules:\n  - apiGroups:\n      - ''\n    resourceNames:\n      - weave-net\n    resources:\n      - configmaps\n    verbs:\n      - get\n      - update\n  - apiGroups:\n      - ''\n    resources:\n      - configmaps\n    verbs:\n      - create\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: RoleBinding\nmetadata:\n  name: weave-net\n  labels:\n    name: weave-net\n  namespace: kube-system\nroleRef:\n  kind: Role\n  name: weave-net\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n  - kind: ServiceAccount\n    name: weave-net\n    namespace: kube-system\n{{- end}}\n"
  },
  "weave": {
   "\u003e=1.16.0-alpha": "weave-v1.16",
   "\u003e=1.8.0-rancher0 \u003c1.16.0-alpha": "weave-v1.8"
  }
 },
 "K8sVersionInfo": {
  "v1.10": {
   "maxRKEVersion": "0.2.2",
   "maxRancherVersion": "2.2"
  },
  "v1.10.1-rancher1": {
   "deprecateRKEVersion": "0.2.2",
   "deprecateRancherVersion": "2.2"
  },
  "v1.11": {
   "maxRKEVersion": "0.2.2",
   "maxRancherVersion": "2.2"
  },
  "v1.12": {
   "maxRKEVersion": "0.2.2",
   "maxRancherVersion": "2.2"
  },
  "v1.13": {
   "maxRKEVersion": "0.3.1",
   "maxRancherVersion": "2.3.1"
  },
  "v1.14": {
   "maxRKEVersion": "1.0.0",
   "maxRancherVersion": "2.3.3"
  },
  "v1.15": {
   "maxRKEVersion": "1.1.99",
   "maxRancherVersion": "2.4.99"
  },
  "v1.15.11-rancher1-1": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.15.11-rancher1-2": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.15.11-rancher1-3": {
   "minRKEVersion": "1.0.7",
   "minRancherVersion": "2.3.7"
  },
  "v1.15.12-rancher1-1": {
   "maxRKEVersion": "0.2.99",
   "maxRancherVersion": "2.2.99"
  },
  "v1.15.12-rancher2-2": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.15.12-rancher2-3": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.15.12-rancher2-4": {
   "minRKEVersion": "1.1.4-rc0",
   "minRancherVersion": "2.4.6-rc0"
  },
  "v1.15.12-rancher2-5": {
   "minRKEVersion": "1.1.5-rc0",
   "minRancherVersion": "2.4.6-rc0"
  },
  "v1.15.5-rancher1-1": {
   "maxRKEVersion": "0.2.8",
   "maxRancherVersion": "2.2.9"
  },
  "v1.16.10-rancher2-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.10-rancher2-2": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.13-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.13-rancher1-2": {
   "minRKEVersion": "1.1.5-rc0",
   "minRancherVersion": "2.4.6-rc0"
  },
  "v1.16.14-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.15-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.15-rancher1-2": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.16.8-rancher1-1": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.16.8-rancher1-2": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.16.9-rancher1-1": {
   "minRKEVersion": "1.0.7",
   "minRancherVersion": "2.3.7-rc0"
  },
  "v1.17.11-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.17.12-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.17.4-rancher1-1": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.17.4-rancher1-2": {
   "minRKEVersion": "1.0.0",
   "minRancherVersion": "2.3.3"
  },
  "v1.17.5-rancher1-1": {
   "minRKEVersion": "1.0.7",
   "minRancherVersion": "2.3.7-rc0"
  },
  "v1.17.6-rancher2-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.17.6-rancher2-2": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.17.9-rancher1-1": {
   "minRKEVersion": "1.1.2-rc0",
   "minRancherVersion": "2.4.4-rc0"
  },
  "v1.17.9-rancher1-2": {
   "minRKEVersion": "1.1.5-rc0",
   "minRancherVersion": "2.4.6-rc0"
  },
  "v1.18.3-rancher2-1": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.18.3-rancher2-2": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.18.6-rancher1-1": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.18.6-rancher1-2": {
   "minRKEVersion": "1.1.5-rc0",
   "minRancherVersion": "2.4.6-rc0"
  },
  "v1.18.8-rancher1-1": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.18.9-rancher1-1": {
   "minRKEVersion": "1.1.3-rc0",
   "minRancherVersion": "2.4.5-rc0"
  },
  "v1.8": {
   "maxRKEVersion": "0.2.2",
   "maxRancherVersion": "2.2"
  },
  "v1.8.10-rancher1-1": {
   "deprecateRKEVersion": "0.2.2",
   "deprecateRancherVersion": "2.2"
  },
  "v1.8.11-rancher1": {
   "deprecateRKEVersion": "0.2.2",
   "deprecateRancherVersion": "2.2"
  },
  "v1.9": {
   "maxRKEVersion": "0.2.2",
   "maxRancherVersion": "2.2"
  },
  "v1.9.7-rancher1": {
   "deprecateRKEVersion": "0.2.2",
   "deprecateRancherVersion": "2.2"
  }
 },
 "RancherDefaultK8sVersions": {
  "2.3": "v1.17.x",
  "2.3.0": "v1.15.x",
  "2.3.1": "v1.15.x",
  "2.3.2": "v1.15.x",
  "2.3.3": "v1.16.x",
  "2.4": "v1.17.x",
  "2.4.0": "v1.17.x",
  "2.4.1": "v1.17.x",
  "2.4.2": "v1.17.x",
  "2.4.3": "v1.17.x",
  "2.4.4": "v1.17.x",
  "2.4.5": "v1.18.x",
  "default": "v1.19.x"
 },
 "RKEDefaultK8sVersions": {
  "0.3": "v1.16.3-rancher1-1",
  "default": "v1.19.2-rancher1-1"
 },
 "K8sVersionDockerInfo": {
  "1.10": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.11": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.12": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.13": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.14": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.15": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.16": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.17": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.18": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.19": [
   "1.13.x",
   "17.03.x",
   "17.06.x",
   "17.09.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ],
  "1.8": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x"
  ],
  "1.9": [
   "1.11.x",
   "1.12.x",
   "1.13.x",
   "17.03.x",
   "18.06.x",
   "18.09.x",
   "19.03.x"
  ]
 },
 "K8sVersionWindowsServiceOptions": {
  "v1.15": {
   "etcd": null,
   "kubeapi": null,
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cert-dir": "[PREFIX_PATH]/var/lib/kubelet/pki",
    "cgroups-per-qos": "false",
    "cni-bin-dir": "[PREFIX_PATH]/opt/cni/bin",
    "cni-conf-dir": "[PREFIX_PATH]/etc/cni/net.d",
    "enforce-node-allocatable": "''",
    "event-qps": "0",
    "feature-gates": "HyperVContainer=true,WindowsGMSA=true",
    "image-pull-progress-deadline": "30m",
    "kube-reserved": "cpu=500m,memory=500Mi,ephemeral-storage=1Gi",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "''",
    "streaming-connection-idle-timeout": "30m",
    "system-reserved": "cpu=1000m,memory=2Gi,ephemeral-storage=2Gi",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "[PREFIX_PATH]/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "enable-dsr": "false",
    "feature-gates": "WinOverlay=true",
    "healthz-bind-address": "127.0.0.1",
    "proxy-mode": "kernelspace",
    "v": "2"
   },
   "kubeController": null,
   "scheduler": null
  },
  "v1.16": {
   "etcd": null,
   "kubeapi": null,
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cert-dir": "[PREFIX_PATH]/var/lib/kubelet/pki",
    "cgroups-per-qos": "false",
    "cni-bin-dir": "[PREFIX_PATH]/opt/cni/bin",
    "cni-conf-dir": "[PREFIX_PATH]/etc/cni/net.d",
    "enforce-node-allocatable": "''",
    "event-qps": "0",
    "feature-gates": "HyperVContainer=true,WindowsGMSA=true",
    "image-pull-progress-deadline": "30m",
    "kube-reserved": "cpu=500m,memory=500Mi,ephemeral-storage=1Gi",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "''",
    "streaming-connection-idle-timeout": "30m",
    "system-reserved": "cpu=1000m,memory=2Gi,ephemeral-storage=2Gi",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "[PREFIX_PATH]/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "enable-dsr": "false",
    "feature-gates": "WinOverlay=true",
    "healthz-bind-address": "127.0.0.1",
    "proxy-mode": "kernelspace",
    "v": "2"
   },
   "kubeController": null,
   "scheduler": null
  },
  "v1.17": {
   "etcd": null,
   "kubeapi": null,
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cert-dir": "[PREFIX_PATH]/var/lib/kubelet/pki",
    "cgroups-per-qos": "false",
    "cni-bin-dir": "[PREFIX_PATH]/opt/cni/bin",
    "cni-conf-dir": "[PREFIX_PATH]/etc/cni/net.d",
    "enforce-node-allocatable": "''",
    "event-qps": "0",
    "feature-gates": "HyperVContainer=true,WindowsGMSA=true",
    "image-pull-progress-deadline": "30m",
    "kube-reserved": "cpu=500m,memory=500Mi,ephemeral-storage=1Gi",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "''",
    "streaming-connection-idle-timeout": "30m",
    "system-reserved": "cpu=1000m,memory=2Gi,ephemeral-storage=2Gi",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "[PREFIX_PATH]/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "enable-dsr": "false",
    "feature-gates": "WinOverlay=true",
    "healthz-bind-address": "127.0.0.1",
    "proxy-mode": "kernelspace",
    "v": "2"
   },
   "kubeController": null,
   "scheduler": null
  },
  "v1.18": {
   "etcd": null,
   "kubeapi": null,
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cert-dir": "[PREFIX_PATH]/var/lib/kubelet/pki",
    "cgroups-per-qos": "false",
    "cni-bin-dir": "[PREFIX_PATH]/opt/cni/bin",
    "cni-conf-dir": "[PREFIX_PATH]/etc/cni/net.d",
    "enforce-node-allocatable": "''",
    "event-qps": "0",
    "feature-gates": "HyperVContainer=true,WindowsGMSA=true",
    "image-pull-progress-deadline": "30m",
    "kube-reserved": "cpu=500m,memory=500Mi,ephemeral-storage=1Gi",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "''",
    "streaming-connection-idle-timeout": "30m",
    "system-reserved": "cpu=1000m,memory=2Gi,ephemeral-storage=2Gi",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "[PREFIX_PATH]/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "enable-dsr": "false",
    "feature-gates": "WinOverlay=true",
    "healthz-bind-address": "127.0.0.1",
    "proxy-mode": "kernelspace",
    "v": "2"
   },
   "kubeController": null,
   "scheduler": null
  },
  "v1.19": {
   "etcd": null,
   "kubeapi": null,
   "kubelet": {
    "address": "0.0.0.0",
    "anonymous-auth": "false",
    "authentication-token-webhook": "true",
    "authorization-mode": "Webhook",
    "cert-dir": "[PREFIX_PATH]/var/lib/kubelet/pki",
    "cgroups-per-qos": "false",
    "cni-bin-dir": "[PREFIX_PATH]/opt/cni/bin",
    "cni-conf-dir": "[PREFIX_PATH]/etc/cni/net.d",
    "enforce-node-allocatable": "''",
    "event-qps": "0",
    "feature-gates": "HyperVContainer=true,WindowsGMSA=true",
    "image-pull-progress-deadline": "30m",
    "kube-reserved": "cpu=500m,memory=500Mi,ephemeral-storage=1Gi",
    "make-iptables-util-chains": "true",
    "network-plugin": "cni",
    "read-only-port": "0",
    "resolv-conf": "''",
    "streaming-connection-idle-timeout": "30m",
    "system-reserved": "cpu=1000m,memory=2Gi,ephemeral-storage=2Gi",
    "tls-cipher-suites": "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "v": "2",
    "volume-plugin-dir": "[PREFIX_PATH]/var/lib/kubelet/volumeplugins"
   },
   "kubeproxy": {
    "enable-dsr": "false",
    "feature-gates": "WinOverlay=true",
    "healthz-bind-address": "127.0.0.1",
    "proxy-mode": "kernelspace",
    "v": "2"
   },
   "kubeController": null,
   "scheduler": null
  }
 },
 "CisConfigParams": {
  "default": {
   "benchmarkVersion": "rke-cis-1.5"
  },
  "v1.15": {
   "benchmarkVersion": "rke-cis-1.5"
  },
  "v1.16": {
   "benchmarkVersion": "rke-cis-1.5"
  },
  "v1.17": {
   "benchmarkVersion": "rke-cis-1.5"
  },
  "v1.18": {
   "benchmarkVersion": "rke-cis-1.5"
  },
  "v1.19": {
   "benchmarkVersion": "rke-cis-1.5"
  }
 },
 "CisBenchmarkVersionInfo": {
  "cis-1.4": {
   "managed": false,
   "minKubernetesVersion": "1.13",
   "skippedChecks": null,
   "notApplicableChecks": null
  },
  "cis-1.5": {
   "managed": false,
   "minKubernetesVersion": "1.15",
   "skippedChecks": null,
   "notApplicableChecks": null
  },
  "rke-cis-1.4": {
   "managed": true,
   "minKubernetesVersion": "1.15",
   "skippedChecks": {
    "1.1.11": "Enabling AlwaysPullImages can use significant bandwidth.",
    "1.1.21": "When generating serving certificates, functionality could break in conjunction with hostname overrides which are required for certain cloud providers.",
    "1.1.24": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "1.1.34": "Enabling encryption changes how data can be recovered as data is encrypted.",
    "1.1.35": "Enabling encryption changes how data can be recovered as data is encrypted.",
    "1.1.36": "EventRateLimit needs to be tuned depending on the cluster.",
    "1.2.2": "Adding this argument prevents Rancher's monitoring tool to collect metrics on the scheduler.",
    "1.3.7": "Adding this argument prevents Rancher's monitoring tool to collect metrics on the controller manager.",
    "1.4.12": "A system service account is required for etcd data directory ownership. Refer to Rancher's hardening guide for more details on how to configure this ownership.",
    "1.7.2": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "1.7.3": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "1.7.4": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "1.7.5": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "2.1.10": "When generating serving certificates, functionality could break in conjunction with hostname overrides which are required for certain cloud providers.",
    "2.1.6": "System level configurations are required prior to provisioning the cluster in order for this argument to be set to true."
   },
   "notApplicableChecks": {
    "1.1.9": "The argument --repair-malformed-updates has been removed as of Kubernetes version 1.14.",
    "1.3.6": "Clusters provisioned by RKE handles certificate rotation directly through RKE.",
    "1.4.1": "Clusters provisioned by RKE doesn't require or maintain a configuration file for kube-apiserver.\nAll configuration is passed in as arguments at container run time.",
    "1.4.13": "Clusters provisioned by RKE does not store the kubernetes default kubeconfig credentials file on the nodes.",
    "1.4.14": "Clusters provisioned by RKE does not store the kubernetes default kubeconfig credentials file on the nodes.",
    "1.4.2": "Clusters provisioned by RKE doesn't require or maintain a configuration file for kube-apiserver.\nAll configuration is passed in as arguments at container run time.",
    "1.4.3": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.4.4": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.4.5": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.4.6": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.4.7": "Clusters provisioned by RKE doesn't require or maintain a configuration file for etcd.\nAll configuration is passed in as arguments at container run time.",
    "1.4.8": "Clusters provisioned by RKE doesn't require or maintain a configuration file for etcd.\nAll configuration is passed in as arguments at container run time.",
    "2.1.12": "Clusters provisioned by RKE handles certificate rotation directly through RKE.",
    "2.1.13": "Clusters provisioned by RKE handles certificate rotation directly through RKE.",
    "2.1.8": "Clusters provisioned by RKE clusters and most cloud providers require hostnames.",
    "2.2.10": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet.\nAll configuration is passed in as arguments at container run time.",
    "2.2.3": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet service.\nAll configuration is passed in as arguments at container run time.",
    "2.2.4": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet service.\nAll configuration is passed in as arguments at container run time.",
    "2.2.9": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet.\nAll configuration is passed in as arguments at container run time."
   }
  },
  "rke-cis-1.5": {
   "managed": true,
   "minKubernetesVersion": "1.15",
   "skippedChecks": {
    "1.1.12": "A system service account is required for etcd data directory ownership. Refer to Rancher's hardening guide for more details on how to configure this ownership.",
    "1.2.16": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "1.2.33": "Enabling encryption changes how data can be recovered as data is encrypted.",
    "1.2.34": "Enabling encryption changes how data can be recovered as data is encrypted.",
    "1.2.6": "When generating serving certificates, functionality could break in conjunction with hostname overrides which are required for certain cloud providers.",
    "4.2.10": "When generating serving certificates, functionality could break in conjunction with hostname overrides which are required for certain cloud providers.",
    "4.2.6": "System level configurations are required prior to provisioning the cluster in order for this argument to be set to true.",
    "5.1.5": "Kubernetes provides default service accounts to be used.",
    "5.2.2": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "5.2.3": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "5.2.4": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "5.2.5": "Enabling Pod Security Policy can cause applications to unexpectedly fail.",
    "5.3.2": "Enabling Network Policies can prevent certain applications from communicating with each other.",
    "5.6.4": "Kubernetes provides a default namespace."
   },
   "notApplicableChecks": {
    "1.1.1": "Clusters provisioned by RKE doesn't require or maintain a configuration file for kube-apiserver.\nAll configuration is passed in as arguments at container run time.",
    "1.1.13": "Clusters provisioned by RKE does not store the kubernetes default kubeconfig credentials file on the nodes.",
    "1.1.14": "Clusters provisioned by RKE does not store the kubernetes default kubeconfig credentials file on the nodes.",
    "1.1.15": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.1.16": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.1.17": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.1.18": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.1.2": "Clusters provisioned by RKE doesn't require or maintain a configuration file for kube-apiserver.\nAll configuration is passed in as arguments at container run time.",
    "1.1.3": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.1.4": "Clusters provisioned by RKE doesn't require or maintain a configuration file for controller-manager.\nAll configuration is passed in as arguments at container run time.",
    "1.1.5": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.1.6": "Clusters provisioned by RKE doesn't require or maintain a configuration file for scheduler.\nAll configuration is passed in as arguments at container run time.",
    "1.1.7": "Clusters provisioned by RKE doesn't require or maintain a configuration file for etcd.\nAll configuration is passed in as arguments at container run time.",
    "1.1.8": "Clusters provisioned by RKE doesn't require or maintain a configuration file for etcd.\nAll configuration is passed in as arguments at container run time.",
    "1.3.6": "Clusters provisioned by RKE handles certificate rotation directly through RKE.",
    "4.1.1": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet service.\nAll configuration is passed in as arguments at container run time.",
    "4.1.10": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet.\nAll configuration is passed in as arguments at container run time.",
    "4.1.2": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet service.\nAll configuration is passed in as arguments at container run time.",
    "4.1.9": "Clusters provisioned by RKE doesnt require or maintain a configuration file for the kubelet.\nAll configuration is passed in as arguments at container run time.",
    "4.2.12": "Clusters provisioned by RKE handles certificate rotation directly through RKE."
   }
  }
 },
 "k3s": {
  "releases": [
   {
    "maxChannelServerVersion": "v2.4.99",
    "minChannelServerVersion": "v2.4.0-rc1",
    "version": "v1.17.11+k3s1"
   },
   {
    "maxChannelServerVersion": "v2.4.99",
    "minChannelServerVersion": "v2.4.5-rc1",
    "version": "v1.18.8+k3s1"
   }
  ]
 },
 "rke2": {
  "releases": [
   {
    "maxChannelServerVersion": "v2.5.99",
    "minChannelServerVersion": "v2.5.0-rc1",
    "version": "v1.18.8-beta19+rke2"
   }
  ]
 }
}